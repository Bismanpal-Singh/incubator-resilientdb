#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 06_chatcompletioncontext</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('dd/dc3/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_206__chatcompletioncontext.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">06_chatcompletioncontext</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md770"></a>
autotoc_md770</h2>
<p>layout: default title: "ChatCompletionContext" parent: "AutoGen Core" </p>
<h2><a class="anchor" id="autotoc_md771"></a>
nav_order: 6</h2>
<h1><a class="anchor" id="autotoc_md772"></a>
Chapter 6: ChatCompletionContext - Remembering the Conversation</h1>
<p>In <a class="el" href="../../d8/df2/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_205__chatcompletionclient.html">Chapter 5: ChatCompletionClient</a>, we learned how agents talk to Large Language Models (LLMs) using a <code>ChatCompletionClient</code>. We saw that we need to send a list of <code>messages</code> (the conversation history) to the LLM so it knows the context.</p>
<p>But conversations can get very long! Imagine talking on the phone for an hour. Can you remember <em>every single word</em> that was said? Probably not. You remember the main points, the beginning, and what was said most recently. LLMs have a similar limitation â€“ they can only pay attention to a certain amount of text at once (called the "context window").</p>
<p>If we send the <em>entire</em> history of a very long chat, it might be too much for the LLM, lead to errors, be slow, or cost more money (since many LLMs charge based on the amount of text).</p>
<p>So, how do we smartly choose <em>which</em> parts of the conversation history to send? This is the problem that <b><code>ChatCompletionContext</code></b> solves.</p>
<h2><a class="anchor" id="autotoc_md773"></a>
Motivation: Keeping LLM Conversations Focused</h2>
<p>Let's say we have a helpful assistant agent chatting with a user:</p>
<ol type="1">
<li><b>User:</b> "Hi! Can you tell me about AutoGen?"</li>
<li><b>Assistant:</b> "Sure! AutoGen is a framework..." (provides details)</li>
<li><b>User:</b> "Thanks! Now, can you draft an email to my team about our upcoming meeting?"</li>
<li><b>Assistant:</b> "Okay, what's the meeting about?"</li>
<li><b>User:</b> "It's about the project planning for Q3."</li>
<li><b>Assistant:</b> (Needs to draft the email)</li>
</ol>
<p>When the Assistant needs to draft the email (step 6), does it need the <em>exact</em> text from step 2 about what AutoGen is? Probably not. It definitely needs the instructions from step 3 and the topic from step 5. Maybe the initial greeting isn't super important either.</p>
<p><code>ChatCompletionContext</code> acts like a <b>smart transcript editor</b>. Before sending the history to the LLM via the <code>ChatCompletionClient</code>, it reviews the full conversation log and prepares a shorter, focused version containing only the messages it thinks are most relevant for the LLM's next response.</p>
<h2><a class="anchor" id="autotoc_md774"></a>
Key Concepts: Managing the Chat History</h2>
<ol type="1">
<li><b>The Full Transcript Holder:</b> A <code>ChatCompletionContext</code> object holds the <em>complete</em> list of messages (<code>LLMMessage</code> objects like <code>SystemMessage</code>, <code>UserMessage</code>, <code>AssistantMessage</code> from Chapter 5) that have occurred in a specific conversation thread. You add new messages using its <code>add_message</code> method.</li>
<li><b>The Smart View Generator (<code>get_messages</code>):</b> The core job of <code>ChatCompletionContext</code> is done by its <code>get_messages</code> method. When called, it looks at the <em>full</em> transcript it holds, but returns only a <em>subset</em> of those messages based on its specific strategy. This subset is what you'll actually send to the <code>ChatCompletionClient</code>.</li>
<li><b>Different Strategies for Remembering:</b> Because different situations require different focus, AutoGen Core provides several <code>ChatCompletionContext</code> implementations (strategies):<ul>
<li><b><code>UnboundedChatCompletionContext</code>:</b> The simplest (and sometimes riskiest!). It doesn't edit anything; <code>get_messages</code> just returns the <em>entire</em> history. Good for short chats, but can break with long ones.</li>
<li><b><code>BufferedChatCompletionContext</code>:</b> Like remembering only the last few things someone said. It keeps the most recent <code>N</code> messages (where <code>N</code> is the <code>buffer_size</code> you set). Good for focusing on recent interactions.</li>
<li><b><code>HeadAndTailChatCompletionContext</code>:</b> Tries to get the best of both worlds. It keeps the first few messages (the "head", maybe containing initial instructions) and the last few messages (the "tail", the recent context). It skips the messages in the middle.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" id="autotoc_md775"></a>
Use Case Example: Chatting with Different Memory Strategies</h2>
<p>Let's simulate adding messages to different context managers and see what <code>get_messages</code> returns.</p>
<p><b>Step 1: Define some messages</b></p>
<div class="fragment"><div class="line"><span class="comment"># File: define_chat_messages.py</span></div>
<div class="line"><span class="keyword">from</span> autogen_core.models <span class="keyword">import</span> (</div>
<div class="line">    SystemMessage, UserMessage, AssistantMessage, LLMMessage</div>
<div class="line">)</div>
<div class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</div>
<div class="line"> </div>
<div class="line"><span class="comment"># The initial instruction for the assistant</span></div>
<div class="line">system_msg = SystemMessage(content=<span class="stringliteral">&quot;You are a helpful assistant.&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># A sequence of user/assistant turns</span></div>
<div class="line">chat_sequence: List[LLMMessage] = [</div>
<div class="line">    UserMessage(content=<span class="stringliteral">&quot;What is AutoGen?&quot;</span>, source=<span class="stringliteral">&quot;User&quot;</span>),</div>
<div class="line">    AssistantMessage(content=<span class="stringliteral">&quot;AutoGen is a multi-agent framework...&quot;</span>, source=<span class="stringliteral">&quot;Agent&quot;</span>),</div>
<div class="line">    UserMessage(content=<span class="stringliteral">&quot;What can it do?&quot;</span>, source=<span class="stringliteral">&quot;User&quot;</span>),</div>
<div class="line">    AssistantMessage(content=<span class="stringliteral">&quot;It can build complex LLM apps.&quot;</span>, source=<span class="stringliteral">&quot;Agent&quot;</span>),</div>
<div class="line">    UserMessage(content=<span class="stringliteral">&quot;Thanks!&quot;</span>, source=<span class="stringliteral">&quot;User&quot;</span>)</div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Combine system message and the chat sequence</span></div>
<div class="line">full_history: List[LLMMessage] = [system_msg] + chat_sequence</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Total messages in full history: {len(full_history)}&quot;</span>)</div>
<div class="line"><span class="comment"># Output: Total messages in full history: 6</span></div>
</div><!-- fragment --><p> We have a full history of 6 messages (1 system + 5 chat turns).</p>
<p><b>Step 2: Use <code>UnboundedChatCompletionContext</code></b></p>
<p>This context keeps everything.</p>
<div class="fragment"><div class="line"><span class="comment"># File: use_unbounded_context.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> define_chat_messages <span class="keyword">import</span> full_history</div>
<div class="line"><span class="keyword">from</span> autogen_core.model_context <span class="keyword">import</span> UnboundedChatCompletionContext</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Create context and add all messages</span></div>
<div class="line">    context = UnboundedChatCompletionContext()</div>
<div class="line">    <span class="keywordflow">for</span> msg <span class="keywordflow">in</span> full_history:</div>
<div class="line">        await context.add_message(msg)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Get the messages to send to the LLM</span></div>
<div class="line">    messages_for_llm = await context.get_messages()</div>
<div class="line"> </div>
<div class="line">    print(f<span class="stringliteral">&quot;--- Unbounded Context ({len(messages_for_llm)} messages) ---&quot;</span>)</div>
<div class="line">    <span class="keywordflow">for</span> i, msg <span class="keywordflow">in</span> enumerate(messages_for_llm):</div>
<div class="line">        print(f<span class="stringliteral">&quot;{i+1}. [{msg.type}]: {msg.content[:30]}...&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># asyncio.run(main()) # If run</span></div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Expected Output (Unbounded):</b> </p><div class="fragment"><div class="line">--- Unbounded Context (6 messages) ---</div>
<div class="line">1. [SystemMessage]: You are a helpful assistant....</div>
<div class="line">2. [UserMessage]: What is AutoGen?...</div>
<div class="line">3. [AssistantMessage]: AutoGen is a multi-agent fram...</div>
<div class="line">4. [UserMessage]: What can it do?...</div>
<div class="line">5. [AssistantMessage]: It can build complex LLM apps...</div>
<div class="line">6. [UserMessage]: Thanks!...</div>
</div><!-- fragment --><p> It returns all 6 messages, exactly as added.</p>
<p><b>Step 3: Use <code>BufferedChatCompletionContext</code></b></p>
<p>Let's keep only the last 3 messages.</p>
<div class="fragment"><div class="line"><span class="comment"># File: use_buffered_context.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> define_chat_messages <span class="keyword">import</span> full_history</div>
<div class="line"><span class="keyword">from</span> autogen_core.model_context <span class="keyword">import</span> BufferedChatCompletionContext</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Keep only the last 3 messages</span></div>
<div class="line">    context = BufferedChatCompletionContext(buffer_size=3)</div>
<div class="line">    <span class="keywordflow">for</span> msg <span class="keywordflow">in</span> full_history:</div>
<div class="line">        await context.add_message(msg)</div>
<div class="line"> </div>
<div class="line">    messages_for_llm = await context.get_messages()</div>
<div class="line"> </div>
<div class="line">    print(f<span class="stringliteral">&quot;--- Buffered Context (buffer=3, {len(messages_for_llm)} messages) ---&quot;</span>)</div>
<div class="line">    <span class="keywordflow">for</span> i, msg <span class="keywordflow">in</span> enumerate(messages_for_llm):</div>
<div class="line">        print(f<span class="stringliteral">&quot;{i+1}. [{msg.type}]: {msg.content[:30]}...&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># asyncio.run(main()) # If run</span></div>
</div><!-- fragment --><p><b>Expected Output (Buffered):</b> </p><div class="fragment"><div class="line">--- Buffered Context (buffer=3, 3 messages) ---</div>
<div class="line">1. [UserMessage]: What can it do?...</div>
<div class="line">2. [AssistantMessage]: It can build complex LLM apps...</div>
<div class="line">3. [UserMessage]: Thanks!...</div>
</div><!-- fragment --><p> It only returns the last 3 messages from the full history. The system message and the first chat turn are omitted.</p>
<p><b>Step 4: Use <code>HeadAndTailChatCompletionContext</code></b></p>
<p>Let's keep the first message (head=1) and the last two messages (tail=2).</p>
<div class="fragment"><div class="line"><span class="comment"># File: use_head_tail_context.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> define_chat_messages <span class="keyword">import</span> full_history</div>
<div class="line"><span class="keyword">from</span> autogen_core.model_context <span class="keyword">import</span> HeadAndTailChatCompletionContext</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Keep first 1 and last 2 messages</span></div>
<div class="line">    context = HeadAndTailChatCompletionContext(head_size=1, tail_size=2)</div>
<div class="line">    <span class="keywordflow">for</span> msg <span class="keywordflow">in</span> full_history:</div>
<div class="line">        await context.add_message(msg)</div>
<div class="line"> </div>
<div class="line">    messages_for_llm = await context.get_messages()</div>
<div class="line"> </div>
<div class="line">    print(f<span class="stringliteral">&quot;--- Head &amp; Tail Context (h=1, t=2, {len(messages_for_llm)} messages) ---&quot;</span>)</div>
<div class="line">    <span class="keywordflow">for</span> i, msg <span class="keywordflow">in</span> enumerate(messages_for_llm):</div>
<div class="line">        print(f<span class="stringliteral">&quot;{i+1}. [{msg.type}]: {msg.content[:30]}...&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># asyncio.run(main()) # If run</span></div>
</div><!-- fragment --><p><b>Expected Output (Head &amp; Tail):</b> </p><div class="fragment"><div class="line">--- Head &amp; Tail Context (h=1, t=2, 4 messages) ---</div>
<div class="line">1. [SystemMessage]: You are a helpful assistant....</div>
<div class="line">2. [UserMessage]: Skipped 3 messages....</div>
<div class="line">3. [AssistantMessage]: It can build complex LLM apps...</div>
<div class="line">4. [UserMessage]: Thanks!...</div>
</div><!-- fragment --><p> It keeps the very first message (<code>SystemMessage</code>), then inserts a placeholder telling the LLM that some messages were skipped, and finally includes the last two messages. This preserves the initial instruction and the most recent context.</p>
<p><b>Which one to choose?</b> It depends on your agent's task!</p><ul>
<li>Simple Q&amp;A? <code>Buffered</code> might be fine.</li>
<li>Following complex initial instructions? <code>HeadAndTail</code> or even <code>Unbounded</code> (if short) might be better.</li>
</ul>
<h2><a class="anchor" id="autotoc_md776"></a>
Under the Hood: How Context is Managed</h2>
<p>The core idea is defined by the <code>ChatCompletionContext</code> abstract base class.</p>
<p><b>Conceptual Flow:</b></p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant Agent as Agent Logic</div>
<div class="line">    participant Context as ChatCompletionContext</div>
<div class="line">    participant FullHistory as Internal Message List</div>
<div class="line"> </div>
<div class="line">    Agent-&gt;&gt;+Context: add_message(newMessage)</div>
<div class="line">    Context-&gt;&gt;+FullHistory: Append newMessage to list</div>
<div class="line">    FullHistory--&gt;&gt;-Context: List updated</div>
<div class="line">    Context--&gt;&gt;-Agent: Done</div>
<div class="line"> </div>
<div class="line">    Agent-&gt;&gt;+Context: get_messages()</div>
<div class="line">    Context-&gt;&gt;+FullHistory: Read the full list</div>
<div class="line">    FullHistory--&gt;&gt;-Context: Return full list</div>
<div class="line">    Context-&gt;&gt;Context: Apply Strategy (e.g., slice list for Buffered/HeadTail)</div>
<div class="line">    Context--&gt;&gt;-Agent: Return selected list of messages</div>
</div><!-- fragment --><ol type="1">
<li><b>Adding:</b> When <code>add_message(message)</code> is called, the context simply appends the <code>message</code> to its internal list (<code>self._messages</code>).</li>
<li><b>Getting:</b> When <code>get_messages()</code> is called:<ul>
<li>The context accesses its internal <code>self._messages</code> list.</li>
<li>The specific implementation (<code>Unbounded</code>, <code>Buffered</code>, <code>HeadAndTail</code>) applies its logic to select which messages to return.</li>
<li>It returns the selected list.</li>
</ul>
</li>
</ol>
<p><b>Code Glimpse:</b></p>
<ul>
<li><p class="startli"><b>Base Class (<code>_chat_completion_context.py</code>):</b> Defines the structure and common methods.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md777"></a>
From: model_context/_chat_completion_context.py (Simplified)</h1>
<p>from abc import ABC, abstractmethod from typing import List from ..models import LLMMessage</p>
<p>class ChatCompletionContext(ABC): component_type = "chat_completion_context" # Identifies this as a component type</p>
<p>def <b>init</b>(self, initial_messages: List[LLMMessage] | None = None) -&gt; None: </p>
<h1><a class="anchor" id="autotoc_md778"></a>
Holds the COMPLETE history</h1>
<p>self._messages: List[LLMMessage] = initial_messages or []</p>
<p>async def add_message(self, message: LLMMessage) -&gt; None: """Add a message to the full context.""" self._messages.append(message)</p>
<p>@abstractmethod async def get_messages(self) -&gt; List[LLMMessage]: """Get the subset of messages based on the strategy.""" </p>
<h1><a class="anchor" id="autotoc_md779"></a>
Each subclass MUST implement this logic</h1>
<p>...</p>
<h1><a class="anchor" id="autotoc_md780"></a>
Other methods like clear(), save_state(), load_state() exist too</h1>
<p>``` The base class handles storing messages; subclasses define <em>how</em> to retrieve them.</p>
<ul>
<li><p class="startli"><b>Unbounded (<code>_unbounded_chat_completion_context.py</code>):</b> The simplest implementation.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md781"></a>
From: model_context/_unbounded_chat_completion_context.py (Simplified)</h1>
<p>from typing import List from ._chat_completion_context import ChatCompletionContext from ..models import LLMMessage</p>
<p>class UnboundedChatCompletionContext(ChatCompletionContext): async def get_messages(self) -&gt; List[LLMMessage]: """Returns all messages.""" return self._messages # Just return the whole internal list ```</p>
<ul>
<li><p class="startli"><b>Buffered (<code>_buffered_chat_completion_context.py</code>):</b> Uses slicing to get the end of the list.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md782"></a>
From: model_context/_buffered_chat_completion_context.py (Simplified)</h1>
<p>from typing import List from ._chat_completion_context import ChatCompletionContext from ..models import LLMMessage, FunctionExecutionResultMessage</p>
<p>class BufferedChatCompletionContext(ChatCompletionContext): def <b>init</b>(self, buffer_size: int, ...): super().__init__(...) self._buffer_size = buffer_size</p>
<p>async def get_messages(self) -&gt; List[LLMMessage]: """Get at most `buffer_size` recent messages.""" </p>
<h1><a class="anchor" id="autotoc_md783"></a>
Slice the list to get the last 'buffer_size' items</h1>
<p>messages = self._messages[-self._buffer_size :] </p>
<h1><a class="anchor" id="autotoc_md784"></a>
Special case: Avoid starting with a function result message</h1>
<p>if messages and isinstance(messages[0], FunctionExecutionResultMessage): messages = messages[1:] return messages ```</p>
<ul>
<li><p class="startli"><b>Head and Tail (<code>_head_and_tail_chat_completion_context.py</code>):</b> Combines slices from the beginning and end.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md785"></a>
From: model_context/_head_and_tail_chat_completion_context.py (Simplified)</h1>
<p>from typing import List from ._chat_completion_context import ChatCompletionContext from ..models import LLMMessage, UserMessage</p>
<p>class HeadAndTailChatCompletionContext(ChatCompletionContext): def <b>init</b>(self, head_size: int, tail_size: int, ...): super().__init__(...) self._head_size = head_size self._tail_size = tail_size</p>
<p>async def get_messages(self) -&gt; List[LLMMessage]: head = self._messages[: self._head_size] # First 'head_size' items tail = self._messages[-self._tail_size :] # Last 'tail_size' items num_skipped = len(self._messages) - len(head) - len(tail)</p>
<p>if num_skipped &lt;= 0: # If no overlap or gap return self._messages else: # If messages were skipped placeholder = [UserMessage(content=f"Skipped {num_skipped} messages.", source="System")] </p>
<h1><a class="anchor" id="autotoc_md786"></a>
Combine head + placeholder + tail</h1>
<p>return head + placeholder + tail ``` These implementations provide different ways to manage the context window effectively.</p>
<h2><a class="anchor" id="autotoc_md787"></a>
Putting it Together with ChatCompletionClient</h2>
<p>How does an agent use <code>ChatCompletionContext</code> with the <code>ChatCompletionClient</code> from Chapter 5?</p>
<ol type="1">
<li>An agent has an instance of a <code>ChatCompletionContext</code> (e.g., <code>BufferedChatCompletionContext</code>) to store its conversation history.</li>
<li>When the agent receives a new message (e.g., a <code>UserMessage</code>), it calls <code>await context.add_message(new_user_message)</code>.</li>
<li>To prepare for calling the LLM, the agent calls <code>messages_to_send = await context.get_messages()</code>. This gets the strategically selected subset of the history.</li>
<li>The agent then passes this list to the <code>ChatCompletionClient</code>: <code>response = await llm_client.create(messages=messages_to_send, ...)</code>.</li>
<li>When the LLM replies (e.g., with an <code>AssistantMessage</code>), the agent adds it back to the context: <code>await context.add_message(llm_response_message)</code>.</li>
</ol>
<p>This loop ensures that the history is continuously updated and intelligently trimmed before each call to the LLM.</p>
<h2><a class="anchor" id="autotoc_md788"></a>
Next Steps</h2>
<p>You've learned how <code>ChatCompletionContext</code> helps manage the conversation history sent to LLMs, preventing context window overflows and keeping the interaction focused using different strategies (<code>Unbounded</code>, <code>Buffered</code>, <code>HeadAndTail</code>).</p>
<p>This context management is a specific form of <b>memory</b>. Agents might need to remember things beyond just the chat history. How do they store general information, state, or knowledge over time?</p>
<ul>
<li><a class="el" href="../../da/dd4/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_207__memory.html">Chapter 7: Memory</a>: Explore the broader concept of Memory in AutoGen Core, which provides more general ways for agents to store and retrieve information.</li>
<li><a class="el" href="../../dc/d8f/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_208__component.html">Chapter 8: Component</a>: Understand how <code>ChatCompletionContext</code> fits into the general <code>Component</code> model, allowing configuration and integration within the AutoGen system.</li>
</ul>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
