#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 08_deepcrawlstrategy</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d3/da3/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_208__deepcrawlstrategy.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">08_deepcrawlstrategy</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1435"></a>
autotoc_md1435</h2>
<p>layout: default title: "DeepCrawlStrategy" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1436"></a>
nav_order: 8</h2>
<h1><a class="anchor" id="autotoc_md1437"></a>
Chapter 8: Exploring Websites - DeepCrawlStrategy</h1>
<p>In <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">Chapter 7: Understanding the Results - CrawlResult</a>, we saw the final report (<code>CrawlResult</code>) that Crawl4AI gives us after processing a single URL. This report contains cleaned content, links, metadata, and maybe even extracted data.</p>
<p>But what if you want to explore a website <em>beyond</em> just the first page? Imagine you land on a blog's homepage. You don't just want the homepage content; you want to automatically discover and crawl all the individual blog posts linked from it. How can you tell Crawl4AI to act like an explorer, following links and venturing deeper into the website?</p>
<h2><a class="anchor" id="autotoc_md1438"></a>
What Problem Does <code>DeepCrawlStrategy</code> Solve?</h2>
<p>Think of the <code>AsyncWebCrawler.arun()</code> method we've used so far like visiting just the entrance hall of a vast library. You get information about that specific hall, but you don't automatically explore the adjoining rooms or different floors.</p>
<p>What if you want to systematically explore the library? You need a plan:</p>
<ul>
<li>Do you explore room by room on the current floor before going upstairs? (Level by level)</li>
<li>Do you pick one wing and explore all its rooms down to the very end before exploring another wing? (Go deep first)</li>
<li>Do you have a map highlighting potentially interesting sections and prioritize visiting those first? (Prioritize promising paths)</li>
</ul>
<p><code>DeepCrawlStrategy</code> provides this <b>exploration plan</b>. It defines the logic for how Crawl4AI should discover and crawl new URLs starting from the initial one(s) by following the links it finds on each page. It turns the crawler from a single-page visitor into a website explorer.</p>
<h2><a class="anchor" id="autotoc_md1439"></a>
What is <code>DeepCrawlStrategy</code>?</h2>
<p><code>DeepCrawlStrategy</code> is a concept (a blueprint) in Crawl4AI that represents the <b>method or logic used to navigate and crawl multiple pages by following links</b>. It tells the crawler <em>which links</em> to follow and in <em>what order</em> to visit them.</p>
<p>It essentially takes over the process when you call <code>arun()</code> if a deep crawl is requested, managing a queue or list of URLs to visit and coordinating the crawling of those URLs, potentially up to a certain depth or number of pages.</p>
<h2><a class="anchor" id="autotoc_md1440"></a>
Different Exploration Plans: The Strategies</h2>
<p>Crawl4AI provides several concrete exploration plans (implementations) for <code>DeepCrawlStrategy</code>:</p>
<ol type="1">
<li><b><code>BFSDeepCrawlStrategy</code> (Level-by-Level Explorer):</b><ul>
<li><b>Analogy:</b> Like ripples spreading in a pond.</li>
<li><b>How it works:</b> It first crawls the starting URL (Level 0). Then, it crawls all the valid links found on that page (Level 1). Then, it crawls all the valid links found on <em>those</em> pages (Level 2), and so on. It explores the website layer by layer.</li>
<li><b>Good for:</b> Finding the shortest path to all reachable pages, getting a broad overview quickly near the start page.</li>
</ul>
</li>
<li><b><code>DFSDeepCrawlStrategy</code> (Deep Path Explorer):</b><ul>
<li><b>Analogy:</b> Like exploring one specific corridor in a maze all the way to the end before backtracking and trying another corridor.</li>
<li><b>How it works:</b> It starts at the initial URL, follows one link, then follows a link from <em>that</em> page, and continues going deeper down one path as far as possible (or until a specified depth limit). Only when it hits a dead end or the limit does it backtrack and try another path.</li>
<li><b>Good for:</b> Exploring specific branches of a website thoroughly, potentially reaching deeper pages faster than BFS (if the target is down a specific path).</li>
</ul>
</li>
<li><b><code>BestFirstCrawlingStrategy</code> (Priority Explorer):</b><ul>
<li><b>Analogy:</b> Like using a treasure map where some paths are marked as more promising than others.</li>
<li><b>How it works:</b> This strategy uses a <b>scoring system</b>. It looks at all the discovered (but not yet visited) links and assigns a score to each one based on how "promising" it seems (e.g., does the URL contain relevant keywords? Is it from a trusted domain?). It then crawls the link with the <em>best</em> score first, regardless of its depth.</li>
<li><b>Good for:</b> Focusing the crawl on the most relevant or important pages first, especially useful when you can't crawl the entire site and need to prioritize.</li>
</ul>
</li>
</ol>
<p><b>Guiding the Explorer: Filters and Scorers</b></p>
<p>Deep crawl strategies often work together with:</p>
<ul>
<li><b>Filters:</b> Rules that decide <em>if</em> a discovered link should even be considered for crawling. Examples:<ul>
<li><code>DomainFilter</code>: Only follow links within the starting website's domain.</li>
<li><code>URLPatternFilter</code>: Only follow links matching a specific pattern (e.g., <code>/blog/posts/...</code>).</li>
<li><code>ContentTypeFilter</code>: Avoid following links to non-HTML content like PDFs or images.</li>
</ul>
</li>
<li><b>Scorers:</b> (Used mainly by <code>BestFirstCrawlingStrategy</code>) Rules that assign a score to a potential link to help prioritize it. Examples:<ul>
<li><code>KeywordRelevanceScorer</code>: Scores links higher if the URL contains certain keywords.</li>
<li><code>PathDepthScorer</code>: Might score links differently based on how deep they are.</li>
</ul>
</li>
</ul>
<p>These act like instructions for the explorer: "Only explore rooms on this floor (filter)," "Ignore corridors marked 'Staff Only' (filter)," or "Check rooms marked with a star first (scorer)."</p>
<h2><a class="anchor" id="autotoc_md1441"></a>
How to Use a <code>DeepCrawlStrategy</code></h2>
<p>You enable deep crawling by adding a <code>DeepCrawlStrategy</code> instance to your <code>CrawlerRunConfig</code>. Let's try exploring a website layer by layer using <code>BFSDeepCrawlStrategy</code>, going only one level deep from the start page.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter8_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> (</div>
<div class="line">    AsyncWebCrawler,</div>
<div class="line">    CrawlerRunConfig,</div>
<div class="line">    BFSDeepCrawlStrategy, <span class="comment"># 1. Import the desired strategy</span></div>
<div class="line">    DomainFilter          <span class="comment"># Import a filter to stay on the same site</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># 2. Create an instance of the strategy</span></div>
<div class="line">    <span class="comment">#    - max_depth=1: Crawl start URL (depth 0) + links found (depth 1)</span></div>
<div class="line">    <span class="comment">#    - filter_chain: Use DomainFilter to only follow links on the same website</span></div>
<div class="line">    bfs_explorer = BFSDeepCrawlStrategy(</div>
<div class="line">        max_depth=1,</div>
<div class="line">        filter_chain=[DomainFilter()] <span class="comment"># Stay within the initial domain</span></div>
<div class="line">    )</div>
<div class="line">    print(f<span class="stringliteral">&quot;Strategy: BFS, Max Depth: {bfs_explorer.max_depth}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 3. Create CrawlerRunConfig and set the deep_crawl_strategy</span></div>
<div class="line">    <span class="comment">#    Also set stream=True to get results as they come in.</span></div>
<div class="line">    run_config = CrawlerRunConfig(</div>
<div class="line">        deep_crawl_strategy=bfs_explorer,</div>
<div class="line">        stream=<span class="keyword">True</span> <span class="comment"># Get results one by one using async for</span></div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 4. Run the crawl - arun now handles the deep crawl!</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        start_url = <span class="stringliteral">&quot;https://httpbin.org/links/10/0&quot;</span> <span class="comment"># A page with 10 internal links</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;\nStarting deep crawl from: {start_url}...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        crawl_results_generator = await crawler.arun(url=start_url, config=run_config)</div>
<div class="line"> </div>
<div class="line">        crawled_count = 0</div>
<div class="line">        <span class="comment"># Iterate over the results as they are yielded</span></div>
<div class="line">        <span class="keyword">async</span> <span class="keywordflow">for</span> result <span class="keywordflow">in</span> crawl_results_generator:</div>
<div class="line">            crawled_count += 1</div>
<div class="line">            status = <span class="stringliteral">&quot;✅&quot;</span> <span class="keywordflow">if</span> result.success <span class="keywordflow">else</span> <span class="stringliteral">&quot;❌&quot;</span></div>
<div class="line">            depth = result.metadata.get(<span class="stringliteral">&quot;depth&quot;</span>, <span class="stringliteral">&quot;N/A&quot;</span>)</div>
<div class="line">            parent = result.metadata.get(<span class="stringliteral">&quot;parent_url&quot;</span>, <span class="stringliteral">&quot;Start&quot;</span>)</div>
<div class="line">            url_short = result.url.split(<span class="stringliteral">&#39;/&#39;</span>)[-1] <span class="comment"># Show last part of URL</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;  {status} Crawled: {url_short:&lt;6} (Depth: {depth})&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;\nFinished deep crawl. Total pages processed: {crawled_count}&quot;</span>)</div>
<div class="line">        <span class="comment"># Expecting 1 (start URL) + 10 (links) = 11 results</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Import:</b> We import <code>AsyncWebCrawler</code>, <code>CrawlerRunConfig</code>, <code>BFSDeepCrawlStrategy</code>, and <code>DomainFilter</code>.</li>
<li><b>Instantiate Strategy:</b> We create <code>BFSDeepCrawlStrategy</code>.<ul>
<li><code>max_depth=1</code>: We tell it to crawl the starting URL (depth 0) and any valid links it finds on that page (depth 1), but not to go any further.</li>
<li><code>filter_chain=[DomainFilter()]</code>: We provide a list containing <code>DomainFilter</code>. This tells the strategy to only consider following links that point to the same domain as the <code>start_url</code>. Links to external sites will be ignored.</li>
</ul>
</li>
<li><b>Configure Run:</b> We create a <code>CrawlerRunConfig</code> and pass our <code>bfs_explorer</code> instance to the <code>deep_crawl_strategy</code> parameter. We also set <code>stream=True</code> so we can process results as soon as they are ready, rather than waiting for the entire crawl to finish.</li>
<li><b>Crawl:</b> We call <code>await crawler.arun(url=start_url, config=run_config)</code>. Because the config contains a <code>deep_crawl_strategy</code>, <code>arun</code> doesn't just crawl the single <code>start_url</code>. Instead, it activates the deep crawl logic defined by <code>BFSDeepCrawlStrategy</code>.</li>
<li><b>Process Results:</b> Since we used <code>stream=True</code>, the return value is an asynchronous generator. We use <code>async for result in crawl_results_generator:</code> to loop through the <code>CrawlResult</code> objects as they are produced by the deep crawl. For each result, we print its status and depth.</li>
</ol>
<p>You'll see the output showing the crawl starting, then processing the initial page (<code>links/10/0</code> at depth 0), followed by the 10 linked pages (e.g., <code>9</code>, <code>8</code>, ... <code>0</code> at depth 1).</p>
<h2><a class="anchor" id="autotoc_md1442"></a>
How It Works (Under the Hood)</h2>
<p>How does simply putting a strategy in the config change <code>arun</code>'s behavior? It involves a bit of Python magic called a <b>decorator</b>.</p>
<ol type="1">
<li><b>Decorator:</b> When you create an <code>AsyncWebCrawler</code>, its <code>arun</code> method is automatically wrapped by a <code>DeepCrawlDecorator</code>.</li>
<li><b>Check Config:</b> When you call <code>await crawler.arun(url=..., config=...)</code>, this decorator checks if <code>config.deep_crawl_strategy</code> is set.</li>
<li><b>Delegate or Run Original:</b><ul>
<li>If a strategy <b>is set</b>, the decorator <em>doesn't</em> run the original single-page crawl logic. Instead, it calls the <code>arun</code> method of your chosen <code>DeepCrawlStrategy</code> instance (e.g., <code>bfs_explorer.arun(...)</code>), passing it the <code>crawler</code> itself, the <code>start_url</code>, and the <code>config</code>.</li>
<li>If no strategy is set, the decorator simply calls the original <code>arun</code> logic to crawl the single page.</li>
</ul>
</li>
<li><b>Strategy Takes Over:</b> The <code>DeepCrawlStrategy</code>'s <code>arun</code> method now manages the crawl.<ul>
<li>It maintains a list or queue of URLs to visit (e.g., <code>current_level</code> in BFS, a stack in DFS, a priority queue in BestFirst).</li>
<li>It repeatedly takes batches of URLs from its list/queue.</li>
<li>For each batch, it calls <code>crawler.arun_many(urls=batch_urls, config=batch_config)</code> (with deep crawling disabled in <code>batch_config</code> to avoid infinite loops!).</li>
<li>As results come back from <code>arun_many</code>, the strategy processes them:<ul>
<li>It yields the <code>CrawlResult</code> if running in stream mode.</li>
<li>It extracts links using its <code>link_discovery</code> method.</li>
<li><code>link_discovery</code> uses <code>can_process_url</code> (which applies filters) to validate links.</li>
<li>Valid new links are added to the list/queue for future crawling.</li>
</ul>
</li>
<li>This continues until the list/queue is empty, the max depth/pages limit is reached, or it's cancelled.</li>
</ul>
</li>
</ol>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant Decorator as DeepCrawlDecorator</div>
<div class="line">    participant Strategy as DeepCrawlStrategy (e.g., BFS)</div>
<div class="line">    participant AWC as AsyncWebCrawler</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;Decorator: arun(start_url, config_with_strategy)</div>
<div class="line">    Decorator-&gt;&gt;Strategy: arun(start_url, crawler=AWC, config)</div>
<div class="line">    Note over Strategy: Initialize queue/level with start_url</div>
<div class="line">    loop Until Queue Empty or Limits Reached</div>
<div class="line">        Strategy-&gt;&gt;Strategy: Get next batch of URLs from queue</div>
<div class="line">        Note over Strategy: Create batch_config (deep_crawl=None)</div>
<div class="line">        Strategy-&gt;&gt;AWC: arun_many(batch_urls, config=batch_config)</div>
<div class="line">        AWC--&gt;&gt;Strategy: batch_results (List/Stream of CrawlResult)</div>
<div class="line">        loop For each result in batch_results</div>
<div class="line">            Strategy-&gt;&gt;Strategy: Process result (yield if streaming)</div>
<div class="line">            Strategy-&gt;&gt;Strategy: Discover links (apply filters)</div>
<div class="line">            Strategy-&gt;&gt;Strategy: Add valid new links to queue</div>
<div class="line">        end</div>
<div class="line">    end</div>
<div class="line">    Strategy--&gt;&gt;Decorator: Final result (List or Generator)</div>
<div class="line">    Decorator--&gt;&gt;User: Final result</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md1443"></a>
Code Glimpse</h2>
<p>Let's peek at the simplified structure:</p>
<p><b>1. The Decorator (<code>deep_crawling/base_strategy.py</code>)</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from deep_crawling/base_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> contextvars <span class="keyword">import</span> ContextVar</div>
<div class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</div>
<div class="line"><span class="comment"># ... other imports</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>DeepCrawlDecorator:</div>
<div class="line">    deep_crawl_active = ContextVar(<span class="stringliteral">&quot;deep_crawl_active&quot;</span>, default=<span class="keyword">False</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>__init__(self, crawler: AsyncWebCrawler):</div>
<div class="line">        self.crawler = crawler</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>__call__(self, original_arun):</div>
<div class="line">        <span class="preprocessor">@wraps(original_arun)</span></div>
<div class="line">        <span class="keyword">async def </span>wrapped_arun(url: str, config: CrawlerRunConfig = <span class="keywordtype">None</span>, **kwargs):</div>
<div class="line">            <span class="comment"># Is a strategy present AND not already inside a deep crawl?</span></div>
<div class="line">            <span class="keywordflow">if</span> config <span class="keywordflow">and</span> config.deep_crawl_strategy <span class="keywordflow">and</span> <span class="keywordflow">not</span> self.deep_crawl_active.<a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#abe6524afb3a69dc9a4c314e11f96f29f">get</a>():</div>
<div class="line">                <span class="comment"># Mark that we are starting a deep crawl</span></div>
<div class="line">                token = self.deep_crawl_active.<a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#a26ae8807a2b3217bb2339bd18aaaa4e6">set</a>(<span class="keyword">True</span>)</div>
<div class="line">                <span class="keywordflow">try</span>:</div>
<div class="line">                    <span class="comment"># Call the STRATEGY&#39;s arun method instead of the original</span></div>
<div class="line">                    strategy_result = await config.deep_crawl_strategy.arun(</div>
<div class="line">                        crawler=self.crawler,</div>
<div class="line">                        start_url=url,</div>
<div class="line">                        config=config</div>
<div class="line">                    )</div>
<div class="line">                    <span class="comment"># Handle streaming if needed</span></div>
<div class="line">                    <span class="keywordflow">if</span> config.stream:</div>
<div class="line">                        <span class="comment"># Return an async generator that resets the context var on exit</span></div>
<div class="line">                        <span class="keyword">async def </span>result_wrapper():</div>
<div class="line">                            <span class="keywordflow">try</span>:</div>
<div class="line">                                <span class="keyword">async</span> <span class="keywordflow">for</span> result <span class="keywordflow">in</span> strategy_result: <span class="keywordflow">yield</span> result</div>
<div class="line">                            <span class="keywordflow">finally</span>: self.deep_crawl_active.reset(token)</div>
<div class="line">                        <span class="keywordflow">return</span> result_wrapper()</div>
<div class="line">                    <span class="keywordflow">else</span>:</div>
<div class="line">                        <span class="keywordflow">return</span> strategy_result <span class="comment"># Return the list of results directly</span></div>
<div class="line">                <span class="keywordflow">finally</span>:</div>
<div class="line">                    <span class="comment"># Reset the context var if not streaming (or handled in wrapper)</span></div>
<div class="line">                    <span class="keywordflow">if</span> <span class="keywordflow">not</span> config.stream: self.deep_crawl_active.reset(token)</div>
<div class="line">            <span class="keywordflow">else</span>:</div>
<div class="line">                <span class="comment"># No strategy or already deep crawling, call the original single-page arun</span></div>
<div class="line">                <span class="keywordflow">return</span> await original_arun(url, config=config, **kwargs)</div>
<div class="line">        <span class="keywordflow">return</span> wrapped_arun</div>
<div class="ttc" id="apybind__kv__service_8cpp_html_a26ae8807a2b3217bb2339bd18aaaa4e6"><div class="ttname"><a href="../../d7/da6/pybind__kv__service_8cpp.html#a26ae8807a2b3217bb2339bd18aaaa4e6">set</a></div><div class="ttdeci">bool set(std::string key, std::string value, std::string config_path)</div><div class="ttdef"><b>Definition</b> <a href="../../d7/da6/pybind__kv__service_8cpp_source.html#l00051">pybind_kv_service.cpp:51</a></div></div>
<div class="ttc" id="apybind__kv__service_8cpp_html_abe6524afb3a69dc9a4c314e11f96f29f"><div class="ttname"><a href="../../d7/da6/pybind__kv__service_8cpp.html#abe6524afb3a69dc9a4c314e11f96f29f">get</a></div><div class="ttdeci">std::string get(std::string key, std::string config_path)</div><div class="ttdef"><b>Definition</b> <a href="../../d7/da6/pybind__kv__service_8cpp_source.html#l00039">pybind_kv_service.cpp:39</a></div></div>
</div><!-- fragment --><p><b>2. The Strategy Blueprint (<code>deep_crawling/base_strategy.py</code>)</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from deep_crawling/base_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</div>
<div class="line"><span class="comment"># ... other imports</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>DeepCrawlStrategy(ABC):</div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>_arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]:</div>
<div class="line">        <span class="comment"># Implementation for non-streaming mode</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>_arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]:</div>
<div class="line">        <span class="comment"># Implementation for streaming mode</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>arun(self, start_url, crawler, config) -&gt; RunManyReturn:</div>
<div class="line">        <span class="comment"># Decides whether to call _arun_batch or _arun_stream</span></div>
<div class="line">        <span class="keywordflow">if</span> config.stream:</div>
<div class="line">            <span class="keywordflow">return</span> self._arun_stream(start_url, crawler, config)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            <span class="keywordflow">return</span> await self._arun_batch(start_url, crawler, config)</div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>can_process_url(self, url: str, depth: int) -&gt; bool:</div>
<div class="line">        <span class="comment"># Applies filters to decide if a URL is valid to crawl</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>link_discovery(self, result, source_url, current_depth, visited, next_level, depths):</div>
<div class="line">        <span class="comment"># Extracts, validates, and prepares links for the next step</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>shutdown(self):</div>
<div class="line">        <span class="comment"># Cleanup logic</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
</div><!-- fragment --><p><b>3. Example: BFS Implementation (<code>deep_crawling/bfs_strategy.py</code>)</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from deep_crawling/bfs_strategy.py</span></div>
<div class="line"><span class="comment"># ... imports ...</span></div>
<div class="line"><span class="keyword">from</span> .base_strategy <span class="keyword">import</span> DeepCrawlStrategy <span class="comment"># Import the base class</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>BFSDeepCrawlStrategy(DeepCrawlStrategy):</div>
<div class="line">    <span class="keyword">def </span>__init__(self, max_depth, filter_chain=None, url_scorer=None, ...):</div>
<div class="line">        self.max_depth = max_depth</div>
<div class="line">        self.filter_chain = filter_chain <span class="keywordflow">or</span> FilterChain() <span class="comment"># Use default if none</span></div>
<div class="line">        self.url_scorer = url_scorer</div>
<div class="line">        <span class="comment"># ... other init ...</span></div>
<div class="line">        self._pages_crawled = 0</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>can_process_url(self, url: str, depth: int) -&gt; bool:</div>
<div class="line">        <span class="comment"># ... (validation logic using self.filter_chain) ...</span></div>
<div class="line">        is_valid = <span class="keyword">True</span> <span class="comment"># Placeholder</span></div>
<div class="line">        <span class="keywordflow">if</span> depth != 0 <span class="keywordflow">and</span> <span class="keywordflow">not</span> await self.filter_chain.apply(url):</div>
<div class="line">            is_valid = <span class="keyword">False</span></div>
<div class="line">        <span class="keywordflow">return</span> is_valid</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>link_discovery(self, result, source_url, current_depth, visited, next_level, depths):</div>
<div class="line">        <span class="comment"># ... (logic to get links from result.links) ...</span></div>
<div class="line">        links = result.links.get(<span class="stringliteral">&quot;internal&quot;</span>, []) <span class="comment"># Example: only internal</span></div>
<div class="line">        <span class="keywordflow">for</span> link_data <span class="keywordflow">in</span> links:</div>
<div class="line">            url = link_data.get(<span class="stringliteral">&quot;href&quot;</span>)</div>
<div class="line">            <span class="keywordflow">if</span> url <span class="keywordflow">and</span> url <span class="keywordflow">not</span> <span class="keywordflow">in</span> visited:</div>
<div class="line">                <span class="keywordflow">if</span> await self.can_process_url(url, current_depth + 1):</div>
<div class="line">                    <span class="comment"># Check scoring, max_pages limit etc.</span></div>
<div class="line">                    depths[url] = current_depth + 1</div>
<div class="line">                    next_level.append((url, source_url)) <span class="comment"># Add (url, parent) tuple</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>_arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]:</div>
<div class="line">        visited = <a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#a26ae8807a2b3217bb2339bd18aaaa4e6">set</a>()</div>
<div class="line">        current_level = [(start_url, <span class="keywordtype">None</span>)] <span class="comment"># List of (url, parent_url)</span></div>
<div class="line">        depths = {start_url: 0}</div>
<div class="line">        all_results = []</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">while</span> current_level: <span class="comment"># While there are pages in the current level</span></div>
<div class="line">            next_level = []</div>
<div class="line">            urls_in_level = [url <span class="keywordflow">for</span> url, parent <span class="keywordflow">in</span> current_level]</div>
<div class="line">            visited.update(urls_in_level)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Create config for this batch (no deep crawl recursion)</span></div>
<div class="line">            batch_config = config.clone(deep_crawl_strategy=<span class="keywordtype">None</span>, stream=<span class="keyword">False</span>)</div>
<div class="line">            <span class="comment"># Crawl all URLs in the current level</span></div>
<div class="line">            batch_results = await crawler.arun_many(urls=urls_in_level, config=batch_config)</div>
<div class="line"> </div>
<div class="line">            <span class="keywordflow">for</span> result <span class="keywordflow">in</span> batch_results:</div>
<div class="line">                <span class="comment"># Add metadata (depth, parent)</span></div>
<div class="line">                depth = depths.get(result.url, 0)</div>
<div class="line">                result.metadata = result.metadata <span class="keywordflow">or</span> {}</div>
<div class="line">                result.metadata[<span class="stringliteral">&quot;depth&quot;</span>] = depth</div>
<div class="line">                <span class="comment"># ... find parent ...</span></div>
<div class="line">                all_results.append(result)</div>
<div class="line">                <span class="comment"># Discover links for the *next* level</span></div>
<div class="line">                <span class="keywordflow">if</span> result.success:</div>
<div class="line">                     await self.link_discovery(result, result.url, depth, visited, next_level, depths)</div>
<div class="line"> </div>
<div class="line">            current_level = next_level <span class="comment"># Move to the next level</span></div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">return</span> all_results</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>_arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]:</div>
<div class="line">        <span class="comment"># Similar logic to _arun_batch, but uses &#39;yield result&#39;</span></div>
<div class="line">        <span class="comment"># and processes results as they come from arun_many stream</span></div>
<div class="line">        visited = <a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#a26ae8807a2b3217bb2339bd18aaaa4e6">set</a>()</div>
<div class="line">        current_level = [(start_url, <span class="keywordtype">None</span>)] <span class="comment"># List of (url, parent_url)</span></div>
<div class="line">        depths = {start_url: 0}</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">while</span> current_level:</div>
<div class="line">             next_level = []</div>
<div class="line">             urls_in_level = [url <span class="keywordflow">for</span> url, parent <span class="keywordflow">in</span> current_level]</div>
<div class="line">             visited.update(urls_in_level)</div>
<div class="line"> </div>
<div class="line">             <span class="comment"># Use stream=True for arun_many</span></div>
<div class="line">             batch_config = config.clone(deep_crawl_strategy=<span class="keywordtype">None</span>, stream=<span class="keyword">True</span>)</div>
<div class="line">             batch_results_gen = await crawler.arun_many(urls=urls_in_level, config=batch_config)</div>
<div class="line"> </div>
<div class="line">             <span class="keyword">async</span> <span class="keywordflow">for</span> result <span class="keywordflow">in</span> batch_results_gen:</div>
<div class="line">                  <span class="comment"># Add metadata</span></div>
<div class="line">                  depth = depths.get(result.url, 0)</div>
<div class="line">                  result.metadata = result.metadata <span class="keywordflow">or</span> {}</div>
<div class="line">                  result.metadata[<span class="stringliteral">&quot;depth&quot;</span>] = depth</div>
<div class="line">                  <span class="comment"># ... find parent ...</span></div>
<div class="line">                  <span class="keywordflow">yield</span> result <span class="comment"># Yield result immediately</span></div>
<div class="line">                  <span class="comment"># Discover links for the next level</span></div>
<div class="line">                  <span class="keywordflow">if</span> result.success:</div>
<div class="line">                      await self.link_discovery(result, result.url, depth, visited, next_level, depths)</div>
<div class="line"> </div>
<div class="line">             current_level = next_level</div>
<div class="line">    <span class="comment"># ... shutdown method ...</span></div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md1444"></a>
Conclusion</h2>
<p>You've learned about <code>DeepCrawlStrategy</code>, the component that turns Crawl4AI into a website explorer!</p>
<ul>
<li>It solves the problem of crawling beyond a single starting page by following links.</li>
<li>It defines the <b>exploration plan</b>:<ul>
<li><code>BFSDeepCrawlStrategy</code>: Level by level.</li>
<li><code>DFSDeepCrawlStrategy</code>: Deep paths first.</li>
<li><code>BestFirstCrawlingStrategy</code>: Prioritized by score.</li>
</ul>
</li>
<li><b>Filters</b> and <b>Scorers</b> help guide the exploration.</li>
<li>You enable it by setting <code>deep_crawl_strategy</code> in the <code>CrawlerRunConfig</code>.</li>
<li>A decorator mechanism intercepts <code>arun</code> calls to activate the strategy.</li>
<li>The strategy manages the queue of URLs and uses <code>crawler.arun_many</code> to crawl them in batches.</li>
</ul>
<p>Deep crawling allows you to gather information from multiple related pages automatically. But how does Crawl4AI avoid re-fetching the same page over and over again, especially during these deeper crawls? The answer lies in caching.</p>
<p><b>Next:</b> Let's explore how Crawl4AI smartly caches results with <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
