#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 04_contentscrapingstrategy</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">04_contentscrapingstrategy</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1386"></a>
autotoc_md1386</h2>
<p>layout: default title: "ContentScrapingStrategy" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1387"></a>
nav_order: 4</h2>
<h1><a class="anchor" id="autotoc_md1388"></a>
Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy</h1>
<p>In <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">Chapter 3: Giving Instructions - CrawlerRunConfig</a>, we learned how to give specific instructions to our <code>AsyncWebCrawler</code> using <code>CrawlerRunConfig</code>. This included telling it <em>how</em> to fetch the page and potentially take screenshots or PDFs.</p>
<p>Now, imagine the crawler has successfully fetched the raw HTML content of a webpage. What's next? Raw HTML is often messy! It contains not just the main article or product description you might care about, but also:</p>
<ul>
<li>Navigation menus</li>
<li>Advertisements</li>
<li>Headers and footers</li>
<li>Hidden code like JavaScript (<code>&lt;script&gt;</code>) and styling information (<code>&lt;style&gt;</code>)</li>
<li>Comments left by developers</li>
</ul>
<p>Before we can really understand the <em>meaning</em> of the page or extract specific important information, we need to clean up this mess and get a basic understanding of its structure.</p>
<h2><a class="anchor" id="autotoc_md1389"></a>
What Problem Does <code>ContentScrapingStrategy</code> Solve?</h2>
<p>Think of the raw HTML fetched by the crawler as a very rough first draft of a book manuscript. It has the core story, but it's full of editor's notes, coffee stains, layout instructions for the printer, and maybe even doodles in the margins.</p>
<p>Before the <em>main</em> editor (who focuses on plot and character) can work on it, someone needs to do an initial cleanup. This "First Pass Editor" would:</p>
<ol type="1">
<li>Remove the coffee stains and doodles (irrelevant stuff like ads, scripts, styles).</li>
<li>Identify the basic structure: chapter headings (like the page title), paragraph text, image captions (image alt text), and maybe a list of illustrations (links).</li>
<li>Produce a tidier version of the manuscript, ready for more detailed analysis.</li>
</ol>
<p>In Crawl4AI, the <code>ContentScrapingStrategy</code> acts as this <b>First Pass Editor</b>. It takes the raw HTML and performs an initial cleanup and structure extraction. Its job is to transform the messy HTML into a more manageable format, identifying key elements like text content, links, images, and basic page metadata (like the title).</p>
<h2><a class="anchor" id="autotoc_md1390"></a>
What is <code>ContentScrapingStrategy</code>?</h2>
<p><code>ContentScrapingStrategy</code> is an abstract concept (like a job description) in Crawl4AI that defines <em>how</em> the initial processing of raw HTML should happen. It specifies <em>that</em> we need a method to clean HTML and extract basic structure, but the specific tools and techniques used can vary.</p>
<p>This allows Crawl4AI to be flexible. Different strategies might use different underlying libraries or have different performance characteristics.</p>
<h2><a class="anchor" id="autotoc_md1391"></a>
The Implementations: Meet the Editors</h2>
<p>Crawl4AI provides concrete implementations (the actual editors doing the work) of this strategy:</p>
<ol type="1">
<li><b><code>WebScrapingStrategy</code> (The Default Editor):</b><ul>
<li>This is the strategy used by default if you don't specify otherwise.</li>
<li>It uses a popular Python library called <code>BeautifulSoup</code> behind the scenes to parse and manipulate the HTML.</li>
<li>It's generally robust and good at handling imperfect HTML.</li>
<li>Think of it as a reliable, experienced editor who does a thorough job.</li>
</ul>
</li>
<li><b><code>LXMLWebScrapingStrategy</code> (The Speedy Editor):</b><ul>
<li>This strategy uses another powerful library called <code>lxml</code>.</li>
<li><code>lxml</code> is often faster than <code>BeautifulSoup</code>, especially on large or complex pages.</li>
<li>Think of it as a very fast editor who might be slightly stricter about the manuscript's format but gets the job done quickly.</li>
</ul>
</li>
</ol>
<p>For most beginners, the default <code>WebScrapingStrategy</code> works perfectly fine! You usually don't need to worry about switching unless you encounter performance issues on very large-scale crawls (which is a more advanced topic).</p>
<h2><a class="anchor" id="autotoc_md1392"></a>
How It Works Conceptually</h2>
<p>Here's the flow:</p>
<ol type="1">
<li>The <a class="el" href="../../d8/dc9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_202__asyncwebcrawler.html">AsyncWebCrawler</a> receives the raw HTML from the <a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a> (the fetcher).</li>
<li>It looks at the <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">CrawlerRunConfig</a> to see which <code>ContentScrapingStrategy</code> to use (defaulting to <code>WebScrapingStrategy</code> if none is specified).</li>
<li>It hands the raw HTML over to the chosen strategy's <code>scrap</code> method.</li>
<li>The strategy parses the HTML, removes unwanted tags (like <code>&lt;script&gt;</code>, <code>&lt;style&gt;</code>, <code>&lt;nav&gt;</code>, <code>&lt;aside&gt;</code>, etc., based on its internal rules), extracts all links (<code>&lt;a&gt;</code> tags), images (<code>&lt;img&gt;</code> tags with their <code>alt</code> text), and metadata (like the <code>&lt;title&gt;</code> tag).</li>
<li>It returns the results packaged in a <code>ScrapingResult</code> object, containing the cleaned HTML, lists of links and media items, and extracted metadata.</li>
<li>The <code>AsyncWebCrawler</code> then takes this <code>ScrapingResult</code> and uses its contents (along with other info) to build the final <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">CrawlResult</a>.</li>
</ol>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant AWC as AsyncWebCrawler (Manager)</div>
<div class="line">    participant Fetcher as AsyncCrawlerStrategy</div>
<div class="line">    participant HTML as Raw HTML</div>
<div class="line">    participant CSS as ContentScrapingStrategy (Editor)</div>
<div class="line">    participant SR as ScrapingResult (Cleaned Draft)</div>
<div class="line">    participant CR as CrawlResult (Final Report)</div>
<div class="line"> </div>
<div class="line">    AWC-&gt;&gt;Fetcher: Fetch(&quot;https://example.com&quot;)</div>
<div class="line">    Fetcher--&gt;&gt;AWC: Here&#39;s the Raw HTML</div>
<div class="line">    AWC-&gt;&gt;CSS: Please scrap this Raw HTML (using config)</div>
<div class="line">    Note over CSS: Parsing HTML... Removing scripts, styles, ads... Extracting links, images, title...</div>
<div class="line">    CSS--&gt;&gt;AWC: Here&#39;s the ScrapingResult (Cleaned HTML, Links, Media, Metadata)</div>
<div class="line">    AWC-&gt;&gt;CR: Combine ScrapingResult with other info</div>
<div class="line">    AWC--&gt;&gt;User: Return final CrawlResult</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md1393"></a>
Using the Default Strategy (<code>WebScrapingStrategy</code>)</h2>
<p>You're likely already using it without realizing it! When you run a basic crawl, <code>AsyncWebCrawler</code> automatically employs <code>WebScrapingStrategy</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter4_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Uses the default AsyncPlaywrightCrawlerStrategy (fetching)</span></div>
<div class="line">    <span class="comment"># AND the default WebScrapingStrategy (scraping/cleaning)</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span> <span class="comment"># A very simple HTML page</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># We don&#39;t specify a scraping_strategy in the config, so it uses the default</span></div>
<div class="line">        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) <span class="comment"># Fetch fresh</span></div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {url_to_crawl} using default scraping strategy...&quot;</span>)</div>
<div class="line">        result = await crawler.arun(url=url_to_crawl, config=config)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Content fetched and scraped.&quot;</span>)</div>
<div class="line">            <span class="comment"># The &#39;result&#39; object now contains info processed by WebScrapingStrategy</span></div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 1. Metadata extracted (e.g., page title)</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Page Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 2. Links extracted</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Found {len(result.links.internal)} internal links and {len(result.links.external)} external links.&quot;</span>)</div>
<div class="line">            <span class="comment"># Example: print first external link if exists</span></div>
<div class="line">            <span class="keywordflow">if</span> result.links.external:</div>
<div class="line">                print(f<span class="stringliteral">&quot;  Example external link: {result.links.external[0].href}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 3. Media extracted (images, videos, etc.)</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Found {len(result.media.images)} images.&quot;</span>)</div>
<div class="line">             <span class="comment"># Example: print first image alt text if exists</span></div>
<div class="line">            <span class="keywordflow">if</span> result.media.images:</div>
<div class="line">                print(f<span class="stringliteral">&quot;  Example image alt text: &#39;{result.media.images[0].alt}&#39;&quot;</span>)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 4. Cleaned HTML (scripts, styles etc. removed) - might still be complex</span></div>
<div class="line">            <span class="comment"># print(f&quot;\nCleaned HTML snippet:\n---\n{result.cleaned_html[:200]}...\n---&quot;)</span></div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 5. Markdown representation (generated AFTER scraping)</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;\nMarkdown snippet:\n---\n{result.markdown.raw_markdown[:200]}...\n---&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li>We create <code>AsyncWebCrawler</code> and <code>CrawlerRunConfig</code> as usual.</li>
<li>We <b>don't</b> set the <code>scraping_strategy</code> parameter in <code>CrawlerRunConfig</code>. Crawl4AI automatically picks <code>WebScrapingStrategy</code>.</li>
<li>When <code>crawler.arun</code> executes, after fetching the HTML, it internally calls <code>WebScrapingStrategy.scrap()</code>.</li>
<li>The <code>result</code> (a <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">CrawlResult</a> object) contains fields populated by the scraping strategy:<ul>
<li><code>result.metadata</code>: Contains things like the page title found in <code>&lt;title&gt;</code> tags.</li>
<li><code>result.links</code>: Contains lists of internal and external links found (<code>&lt;a&gt;</code> tags).</li>
<li><code>result.media</code>: Contains lists of images (<code>&lt;img&gt;</code>), videos (<code>&lt;video&gt;</code>), etc.</li>
<li><code>result.cleaned_html</code>: The HTML after the strategy removed unwanted tags and attributes (this is then used to generate the Markdown).</li>
<li><code>result.markdown</code>: While not <em>directly</em> created by the scraping strategy, the cleaned HTML it produces is the input for generating the Markdown representation.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" id="autotoc_md1394"></a>
Explicitly Choosing a Strategy (e.g., <code>LXMLWebScrapingStrategy</code>)</h2>
<p>What if you want to try the potentially faster <code>LXMLWebScrapingStrategy</code>? You can specify it in the <code>CrawlerRunConfig</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter4_example_2.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode</div>
<div class="line"><span class="comment"># 1. Import the specific strategy you want to use</span></div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> LXMLWebScrapingStrategy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># 2. Create an instance of the desired scraping strategy</span></div>
<div class="line">    lxml_editor = LXMLWebScrapingStrategy()</div>
<div class="line">    print(f<span class="stringliteral">&quot;Using scraper: {lxml_editor.__class__.__name__}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Create a CrawlerRunConfig and pass the strategy instance</span></div>
<div class="line">        config = CrawlerRunConfig(</div>
<div class="line">            cache_mode=CacheMode.BYPASS,</div>
<div class="line">            scraping_strategy=lxml_editor <span class="comment"># Tell the config which strategy to use</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {url_to_crawl} with explicit LXML scraping strategy...&quot;</span>)</div>
<div class="line">        result = await crawler.arun(url=url_to_crawl, config=config)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Content fetched and scraped using LXML.&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;Page Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;Found {len(result.links.external)} external links.&quot;</span>)</div>
<div class="line">            <span class="comment"># Output should be largely the same as the default strategy for simple pages</span></div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Import:</b> We import <code>LXMLWebScrapingStrategy</code> alongside the other classes.</li>
<li><b>Instantiate:</b> We create an instance: <code>lxml_editor = LXMLWebScrapingStrategy()</code>.</li>
<li><b>Configure:</b> We create <code>CrawlerRunConfig</code> and pass our instance to the <code>scraping_strategy</code> parameter: <code>CrawlerRunConfig(..., scraping_strategy=lxml_editor)</code>.</li>
<li><b>Run:</b> Now, when <code>crawler.arun</code> is called with this config, it will use <code>LXMLWebScrapingStrategy</code> instead of the default <code>WebScrapingStrategy</code> for the initial HTML processing step.</li>
</ol>
<p>For simple pages, the results from both strategies will often be very similar. The choice typically comes down to performance considerations in more advanced scenarios.</p>
<h2><a class="anchor" id="autotoc_md1395"></a>
A Glimpse Under the Hood</h2>
<p>Inside the <code>crawl4ai</code> library, the file <code>content_scraping_strategy.py</code> defines the blueprint and the implementations.</p>
<p><b>The Blueprint (Abstract Base Class):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/content_scraping_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</div>
<div class="line"><span class="keyword">from</span> .models <span class="keyword">import</span> ScrapingResult <span class="comment"># Defines the structure of the result</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ContentScrapingStrategy(ABC):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Abstract base class for content scraping strategies.&quot;&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">def </span>scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral">        Synchronous method to scrape content.</span></div>
<div class="line"><span class="stringliteral">        Takes raw HTML, returns structured ScrapingResult.</span></div>
<div class="line"><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral">        Asynchronous method to scrape content.</span></div>
<div class="line"><span class="stringliteral">        Takes raw HTML, returns structured ScrapingResult.</span></div>
<div class="line"><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
</div><!-- fragment --><p><b>The Implementations:</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/content_scraping_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment"># Library used by WebScrapingStrategy</span></div>
<div class="line"><span class="comment"># ... other imports like models ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>WebScrapingStrategy(ContentScrapingStrategy):</div>
<div class="line">    <span class="keyword">def </span>__init__(self, logger=None):</div>
<div class="line">        self.logger = logger</div>
<div class="line">        <span class="comment"># ... potentially other setup ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult:</div>
<div class="line">        <span class="comment"># 1. Parse HTML using BeautifulSoup</span></div>
<div class="line">        soup = BeautifulSoup(html, <span class="stringliteral">&#39;lxml&#39;</span>) <span class="comment"># Or another parser</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 2. Find the main content area (maybe using kwargs[&#39;css_selector&#39;])</span></div>
<div class="line">        <span class="comment"># 3. Remove unwanted tags (scripts, styles, nav, footer, ads...)</span></div>
<div class="line">        <span class="comment"># 4. Extract metadata (title, description...)</span></div>
<div class="line">        <span class="comment"># 5. Extract all links (&lt;a&gt; tags)</span></div>
<div class="line">        <span class="comment"># 6. Extract all images (&lt;img&gt; tags) and other media</span></div>
<div class="line">        <span class="comment"># 7. Get the remaining cleaned HTML text content</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># ... complex cleaning and extraction logic using BeautifulSoup methods ...</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 8. Package results into a ScrapingResult object</span></div>
<div class="line">        cleaned_html_content = <span class="stringliteral">&quot;&lt;html&gt;&lt;body&gt;Cleaned content...&lt;/body&gt;&lt;/html&gt;&quot;</span> <span class="comment"># Placeholder</span></div>
<div class="line">        links_data = Links(...)</div>
<div class="line">        media_data = Media(...)</div>
<div class="line">        metadata_dict = {<span class="stringliteral">&quot;title&quot;</span>: <span class="stringliteral">&quot;Page Title&quot;</span>}</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">return</span> ScrapingResult(</div>
<div class="line">            cleaned_html=cleaned_html_content,</div>
<div class="line">            links=links_data,</div>
<div class="line">            media=media_data,</div>
<div class="line">            metadata=metadata_dict,</div>
<div class="line">            success=<span class="keyword">True</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult:</div>
<div class="line">        <span class="comment"># Often delegates to the synchronous version for CPU-bound tasks</span></div>
<div class="line">        <span class="keywordflow">return</span> await asyncio.to_thread(self.scrap, url, html, **kwargs)</div>
</div><!-- fragment --><div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/content_scraping_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> html <span class="keyword">as</span> lhtml <span class="comment"># Library used by LXMLWebScrapingStrategy</span></div>
<div class="line"><span class="comment"># ... other imports like models ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>LXMLWebScrapingStrategy(WebScrapingStrategy): <span class="comment"># Often inherits for shared logic</span></div>
<div class="line">    <span class="keyword">def </span>__init__(self, logger=None):</div>
<div class="line">        super().__init__(logger)</div>
<div class="line">        <span class="comment"># ... potentially LXML specific setup ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult:</div>
<div class="line">        <span class="comment"># 1. Parse HTML using lxml</span></div>
<div class="line">        doc = lhtml.document_fromstring(html)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 2. Find main content, remove unwanted tags, extract info</span></div>
<div class="line">        <span class="comment"># ... complex cleaning and extraction logic using lxml&#39;s XPath or CSS selectors ...</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Package results into a ScrapingResult object</span></div>
<div class="line">        cleaned_html_content = <span class="stringliteral">&quot;&lt;html&gt;&lt;body&gt;Cleaned LXML content...&lt;/body&gt;&lt;/html&gt;&quot;</span> <span class="comment"># Placeholder</span></div>
<div class="line">        links_data = Links(...)</div>
<div class="line">        media_data = Media(...)</div>
<div class="line">        metadata_dict = {<span class="stringliteral">&quot;title&quot;</span>: <span class="stringliteral">&quot;Page Title LXML&quot;</span>}</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">return</span> ScrapingResult(</div>
<div class="line">            cleaned_html=cleaned_html_content,</div>
<div class="line">            links=links_data,</div>
<div class="line">            media=media_data,</div>
<div class="line">            metadata=metadata_dict,</div>
<div class="line">            success=<span class="keyword">True</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># ascrap might also delegate or have specific async optimizations</span></div>
</div><!-- fragment --><p>The key takeaway is that both strategies implement the <code>scrap</code> (and <code>ascrap</code>) method, taking raw HTML and returning a structured <code>ScrapingResult</code>. The <code>AsyncWebCrawler</code> can use either one thanks to this common interface.</p>
<h2><a class="anchor" id="autotoc_md1396"></a>
Conclusion</h2>
<p>You've learned about <code>ContentScrapingStrategy</code>, Crawl4AI's "First Pass Editor" for raw HTML.</p>
<ul>
<li>It tackles the problem of messy HTML by cleaning it and extracting basic structure.</li>
<li>It acts as a blueprint, with <code>WebScrapingStrategy</code> (default, using BeautifulSoup) and <code>LXMLWebScrapingStrategy</code> (using lxml) as concrete implementations.</li>
<li>It's used automatically by <code>AsyncWebCrawler</code> after fetching content.</li>
<li>You can specify which strategy to use via <code>CrawlerRunConfig</code>.</li>
<li>Its output (cleaned HTML, links, media, metadata) is packaged into a <code>ScrapingResult</code> and contributes significantly to the final <code>CrawlResult</code>.</li>
</ul>
<p>Now that we have this initially cleaned and structured content, we might want to further filter it. What if we only care about the parts of the page that are <em>relevant</em> to a specific topic?</p>
<p><b>Next:</b> Let's explore how to filter content for relevance with <a class="el" href="../../d2/d40/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_205__relevantcontentfilter.html">Chapter 5: Focusing on What Matters - RelevantContentFilter</a>.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
