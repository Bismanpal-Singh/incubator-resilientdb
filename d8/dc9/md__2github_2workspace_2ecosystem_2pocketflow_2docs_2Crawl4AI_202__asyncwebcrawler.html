#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 02_asyncwebcrawler</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d8/dc9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_202__asyncwebcrawler.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">02_asyncwebcrawler</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1388"></a>
autotoc_md1388</h2>
<p>layout: default title: "AsyncWebCrawler" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1389"></a>
nav_order: 2</h2>
<h1><a class="anchor" id="autotoc_md1390"></a>
Chapter 2: Meet the General Manager - AsyncWebCrawler</h1>
<p>In <a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy</a>, we learned about the different ways Crawl4AI can fetch the raw content of a webpage, like choosing between a fast drone (<code>AsyncHTTPCrawlerStrategy</code>) or a versatile delivery truck (<code>AsyncPlaywrightCrawlerStrategy</code>).</p>
<p>But who decides <em>which</em> delivery vehicle to use? Who tells it <em>which</em> address (URL) to go to? And who takes the delivered package (the raw HTML) and turns it into something useful?</p>
<p>That's where the <code>AsyncWebCrawler</code> comes in. Think of it as the <b>General Manager</b> of the entire crawling operation.</p>
<h2><a class="anchor" id="autotoc_md1391"></a>
What Problem Does <code>AsyncWebCrawler</code> Solve?</h2>
<p>Imagine you want to get information from a website. You need to:</p>
<ol type="1">
<li>Decide <em>how</em> to fetch the page (like choosing the drone or truck from Chapter 1).</li>
<li>Actually <em>fetch</em> the page content.</li>
<li>Maybe <em>clean up</em> the messy HTML.</li>
<li>Perhaps <em>extract</em> specific pieces of information (like product prices or article titles).</li>
<li>Maybe <em>save</em> the results so you don't have to fetch them again immediately (caching).</li>
<li>Finally, give you the <em>final, processed result</em>.</li>
</ol>
<p>Doing all these steps manually for every URL would be tedious and complex. <code>AsyncWebCrawler</code> acts as the central coordinator, managing all these steps for you. You just tell it what URL to crawl and maybe some preferences, and it handles the rest.</p>
<h2><a class="anchor" id="autotoc_md1392"></a>
What is <code>AsyncWebCrawler</code>?</h2>
<p><code>AsyncWebCrawler</code> is the main class you'll interact with when using Crawl4AI. It's the primary entry point for starting any crawling task.</p>
<p><b>Key Responsibilities:</b></p>
<ul>
<li><b>Initialization:</b> Sets up the necessary components, like the browser (if needed).</li>
<li><b>Coordination:</b> Takes your request (a URL and configuration) and orchestrates the different parts:<ul>
<li>Delegates fetching to an <a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a>.</li>
<li>Manages caching using <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">CacheContext / CacheMode</a>.</li>
<li>Uses a <a class="el" href="../../df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html">ContentScrapingStrategy</a> to clean and parse HTML.</li>
<li>Applies a <a class="el" href="../../d2/d40/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_205__relevantcontentfilter.html">RelevantContentFilter</a> if configured.</li>
<li>Uses an <a class="el" href="../../d1/d07/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_206__extractionstrategy.html">ExtractionStrategy</a> to pull out specific data if needed.</li>
</ul>
</li>
<li><b>Result Packaging:</b> Bundles everything up into a neat <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">CrawlResult</a> object.</li>
<li><b>Resource Management:</b> Handles starting and stopping resources (like browsers) cleanly.</li>
</ul>
<p>It's the "conductor" making sure all the different instruments play together harmoniously.</p>
<h2><a class="anchor" id="autotoc_md1393"></a>
Your First Crawl: Using <code>arun</code></h2>
<p>Let's see the <code>AsyncWebCrawler</code> in action. The most common way to use it is with an <code>async with</code> block, which automatically handles setup and cleanup. The main method to crawl a single URL is <code>arun</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter2_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler <span class="comment"># Import the General Manager</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Create the General Manager instance using &#39;async with&#39;</span></div>
<div class="line">    <span class="comment"># This handles setup (like starting a browser if needed)</span></div>
<div class="line">    <span class="comment"># and cleanup (closing the browser).</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        print(<span class="stringliteral">&quot;Crawler is ready!&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Tell the manager to crawl a specific URL</span></div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span> <span class="comment"># A simple example page</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;Asking the crawler to fetch: {url_to_crawl}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        result = await crawler.arun(url=url_to_crawl)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Check if the crawl was successful</span></div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Crawler got the content.&quot;</span>)</div>
<div class="line">            <span class="comment"># The result object contains the processed data</span></div>
<div class="line">            <span class="comment"># We&#39;ll learn more about CrawlResult in Chapter 7</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Page Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;First 100 chars of Markdown: {result.markdown.raw_markdown[:100]}...&quot;</span>)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed to crawl: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b><code>import AsyncWebCrawler</code></b>: We import the main class.</li>
<li><b><code>async def <a class="el" href="../../d2/dc1/namespacemain.html">main()</a>:</code></b>: Crawl4AI uses Python's <code>asyncio</code> for efficiency, so our code needs to be in an <code>async</code> function.</li>
<li><b><code>async with AsyncWebCrawler() as crawler:</code></b>: This is the standard way to create and manage the crawler. The <code>async with</code> statement ensures that resources (like the underlying browser used by the default <code>AsyncPlaywrightCrawlerStrategy</code>) are properly started and stopped, even if errors occur.</li>
<li><b><code>crawler.arun(url=url_to_crawl)</code></b>: This is the core command. We tell our <code>crawler</code> instance (the General Manager) to run (<code>arun</code>) the crawling process for the specified <code>url</code>. <code>await</code> is used because fetching webpages takes time, and <code>asyncio</code> allows other tasks to run while waiting.</li>
<li><b><code>result</code></b>: The <code>arun</code> method returns a <code>CrawlResult</code> object. This object contains all the information gathered during the crawl (HTML, cleaned text, metadata, etc.). We'll explore this object in detail in <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">Chapter 7: Understanding the Results - CrawlResult</a>.</li>
<li><b><code>result.success</code></b>: We check this boolean flag to see if the crawl completed without critical errors.</li>
<li><b>Accessing Data:</b> If successful, we can access processed information like the page title (&lsquo;result.metadata['title&rsquo;]<code>) or the content formatted as Markdown (</code>result.markdown.raw_markdown`).</li>
</ol>
<h2><a class="anchor" id="autotoc_md1394"></a>
Configuring the Crawl</h2>
<p>Sometimes, the default behavior isn't quite what you need. Maybe you want to use the faster "drone" strategy from Chapter 1, or perhaps you want to ensure you <em>always</em> fetch a fresh copy of the page, ignoring any saved cache.</p>
<p>You can customize the behavior of a specific <code>arun</code> call by passing a <code>CrawlerRunConfig</code> object. Think of this as giving specific instructions to the General Manager for <em>this particular job</em>.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter2_example_2.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> CrawlerRunConfig <span class="comment"># Import configuration class</span></div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> CacheMode <span class="comment"># Import cache options</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        print(<span class="stringliteral">&quot;Crawler is ready!&quot;</span>)</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Create a specific configuration for this run</span></div>
<div class="line">        <span class="comment"># Tell the crawler to BYPASS the cache (fetch fresh)</span></div>
<div class="line">        run_config = CrawlerRunConfig(</div>
<div class="line">            cache_mode=CacheMode.BYPASS</div>
<div class="line">        )</div>
<div class="line">        print(<span class="stringliteral">&quot;Configuration: Bypass cache for this run.&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Pass the config object to the arun method</span></div>
<div class="line">        result = await crawler.arun(</div>
<div class="line">            url=url_to_crawl,</div>
<div class="line">            config=run_config <span class="comment"># Pass the specific instructions</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Crawler got fresh content (cache bypassed).&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;Page Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed to crawl: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b><code>from crawl4ai import CrawlerRunConfig, CacheMode</code></b>: We import the necessary classes for configuration.</li>
<li><b><code>run_config = CrawlerRunConfig(...)</code></b>: We create an instance of <code>CrawlerRunConfig</code>. This object holds various settings for a specific crawl job.</li>
<li><b><code>cache_mode=CacheMode.BYPASS</code></b>: We set the <code>cache_mode</code>. <code>CacheMode.BYPASS</code> tells the crawler to ignore any previously saved results for this URL and fetch it directly from the web server. We'll learn all about caching options in <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>.</li>
<li><b><code>crawler.arun(..., config=run_config)</code></b>: We pass our custom <code>run_config</code> object to the <code>arun</code> method using the <code>config</code> parameter.</li>
</ol>
<p>The <code>CrawlerRunConfig</code> is very powerful and lets you control many aspects of the crawl, including which scraping or extraction methods to use. We'll dive deep into it in the next chapter: <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">Chapter 3: Giving Instructions - CrawlerRunConfig</a>.</p>
<h2><a class="anchor" id="autotoc_md1395"></a>
What Happens When You Call <code>arun</code>? (The Flow)</h2>
<p>When you call <code>crawler.arun(url="...")</code>, the <code>AsyncWebCrawler</code> (our General Manager) springs into action and coordinates several steps behind the scenes:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant U as User</div>
<div class="line">    participant AWC as AsyncWebCrawler (Manager)</div>
<div class="line">    participant CC as Cache Check</div>
<div class="line">    participant CS as AsyncCrawlerStrategy (Fetcher)</div>
<div class="line">    participant SP as Scraping/Processing</div>
<div class="line">    participant CR as CrawlResult (Final Report)</div>
<div class="line"> </div>
<div class="line">    U-&gt;&gt;AWC: arun(&quot;https://example.com&quot;, config)</div>
<div class="line">    AWC-&gt;&gt;CC: Need content for &quot;https://example.com&quot;? (Respect CacheMode in config)</div>
<div class="line">    alt Cache Hit &amp; Cache Mode allows reading</div>
<div class="line">        CC--&gt;&gt;AWC: Yes, here&#39;s the cached result.</div>
<div class="line">        AWC--&gt;&gt;CR: Package cached result.</div>
<div class="line">        AWC--&gt;&gt;U: Here is the CrawlResult</div>
<div class="line">    else Cache Miss or Cache Mode prevents reading</div>
<div class="line">        CC--&gt;&gt;AWC: No cached result / Cannot read cache.</div>
<div class="line">        AWC-&gt;&gt;CS: Please fetch &quot;https://example.com&quot; (using configured strategy)</div>
<div class="line">        CS--&gt;&gt;AWC: Here&#39;s the raw response (HTML, etc.)</div>
<div class="line">        AWC-&gt;&gt;SP: Process this raw content (Scrape, Filter, Extract based on config)</div>
<div class="line">        SP--&gt;&gt;AWC: Here&#39;s the processed data (Markdown, Metadata, etc.)</div>
<div class="line">        AWC-&gt;&gt;CC: Cache this result? (Respect CacheMode in config)</div>
<div class="line">        CC--&gt;&gt;AWC: OK, cached.</div>
<div class="line">        AWC--&gt;&gt;CR: Package new result.</div>
<div class="line">        AWC--&gt;&gt;U: Here is the CrawlResult</div>
<div class="line">    end</div>
</div><!-- fragment --><p><b>Simplified Steps:</b></p>
<ol type="1">
<li><b>Receive Request:</b> The <code>AsyncWebCrawler</code> gets the URL and configuration from your <code>arun</code> call.</li>
<li><b>Check Cache:</b> It checks if a valid result for this URL is already saved (cached) and if the <code>CacheMode</code> allows using it. (See <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">Chapter 9</a>).</li>
<li><b>Fetch (if needed):</b> If no valid cached result exists or caching is bypassed, it asks the configured <a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a> (e.g., Playwright or HTTP) to fetch the raw page content.</li>
<li><b>Process Content:</b> It takes the raw HTML and passes it through various processing steps based on the configuration:<ul>
<li><b>Scraping:</b> Cleaning up HTML, extracting basic structure using a <a class="el" href="../../df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html">ContentScrapingStrategy</a>.</li>
<li><b>Filtering:</b> Optionally filtering content for relevance using a <a class="el" href="../../d2/d40/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_205__relevantcontentfilter.html">RelevantContentFilter</a>.</li>
<li><b>Extraction:</b> Optionally extracting specific structured data using an <a class="el" href="../../d1/d07/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_206__extractionstrategy.html">ExtractionStrategy</a>.</li>
</ul>
</li>
<li><b>Cache Result (if needed):</b> If caching is enabled for writing, it saves the final processed result.</li>
<li><b>Return Result:</b> It bundles everything into a <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">CrawlResult</a> object and returns it to you.</li>
</ol>
<h2><a class="anchor" id="autotoc_md1396"></a>
Crawling Many Pages: <code>arun_many</code></h2>
<p>What if you have a whole list of URLs to crawl? Calling <code>arun</code> in a loop works, but it might not be the most efficient way. <code>AsyncWebCrawler</code> provides the <code>arun_many</code> method designed for this.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter2_example_3.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler, CrawlerRunConfig, CacheMode</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        urls_to_crawl = [</div>
<div class="line">            <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;https://httpbin.org/links/10/0&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;https://httpbin.org/robots.txt&quot;</span></div>
<div class="line">        ]</div>
<div class="line">        print(f<span class="stringliteral">&quot;Asking crawler to fetch {len(urls_to_crawl)} URLs.&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Use arun_many for multiple URLs</span></div>
<div class="line">        <span class="comment"># We can still pass a config that applies to all URLs in the batch</span></div>
<div class="line">        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)</div>
<div class="line">        results = await crawler.arun_many(urls=urls_to_crawl, config=config)</div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;\nFinished crawling! Got {len(results)} results.&quot;</span>)</div>
<div class="line">        <span class="keywordflow">for</span> result <span class="keywordflow">in</span> results:</div>
<div class="line">            status = <span class="stringliteral">&quot;Success&quot;</span> <span class="keywordflow">if</span> result.success <span class="keywordflow">else</span> <span class="stringliteral">&quot;Failed&quot;</span></div>
<div class="line">            url_short = result.url.split(<span class="stringliteral">&#39;/&#39;</span>)[-1] <span class="comment"># Get last part of URL</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;- URL: {url_short:&lt;10} | Status: {status:&lt;7} | Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b><code>urls_to_crawl = [...]</code></b>: We define a list of URLs.</li>
<li><b><code>await crawler.arun_many(urls=urls_to_crawl, config=config)</code></b>: We call <code>arun_many</code>, passing the list of URLs. It handles crawling them concurrently (like dispatching multiple delivery trucks or drones efficiently).</li>
<li><b><code>results</code></b>: <code>arun_many</code> returns a list where each item is a <code>CrawlResult</code> object corresponding to one of the input URLs.</li>
</ol>
<p><code>arun_many</code> is much more efficient for batch processing as it leverages <code>asyncio</code> to handle multiple fetches and processing tasks concurrently. It uses a <a class="el" href="../../d6/d6d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_210__basedispatcher.html">BaseDispatcher</a> internally to manage this concurrency.</p>
<h2><a class="anchor" id="autotoc_md1397"></a>
Under the Hood (A Peek at the Code)</h2>
<p>You don't need to know the internal details to use <code>AsyncWebCrawler</code>, but seeing the structure can help. Inside the <code>crawl4ai</code> library, the file <code>async_webcrawler.py</code> defines this class.</p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from async_webcrawler.py</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># ... imports ...</span></div>
<div class="line"><span class="keyword">from</span> .async_crawler_strategy <span class="keyword">import</span> AsyncCrawlerStrategy, AsyncPlaywrightCrawlerStrategy</div>
<div class="line"><span class="keyword">from</span> .async_configs <span class="keyword">import</span> BrowserConfig, CrawlerRunConfig</div>
<div class="line"><span class="keyword">from</span> .models <span class="keyword">import</span> CrawlResult</div>
<div class="line"><span class="keyword">from</span> .cache_context <span class="keyword">import</span> CacheContext, CacheMode</div>
<div class="line"><span class="comment"># ... other strategy imports ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>AsyncWebCrawler:</div>
<div class="line">    <span class="keyword">def </span>__init__(</div>
<div class="line">        self,</div>
<div class="line">        crawler_strategy: AsyncCrawlerStrategy = <span class="keywordtype">None</span>, <span class="comment"># You can provide a strategy...</span></div>
<div class="line">        config: BrowserConfig = <span class="keywordtype">None</span>, <span class="comment"># Configuration for the browser</span></div>
<div class="line">        <span class="comment"># ... other parameters like logger, base_directory ...</span></div>
<div class="line">    ):</div>
<div class="line">        <span class="comment"># If no strategy is given, it defaults to Playwright (the &#39;truck&#39;)</span></div>
<div class="line">        self.crawler_strategy = crawler_strategy <span class="keywordflow">or</span> AsyncPlaywrightCrawlerStrategy(...)</div>
<div class="line">        self.browser_config = config <span class="keywordflow">or</span> BrowserConfig()</div>
<div class="line">        <span class="comment"># ... setup logger, directories, etc. ...</span></div>
<div class="line">        self.ready = <span class="keyword">False</span> <span class="comment"># Flag to track if setup is complete</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>__aenter__(self):</div>
<div class="line">        <span class="comment"># This is called when you use &#39;async with&#39;. It starts the strategy.</span></div>
<div class="line">        await self.crawler_strategy.__aenter__()</div>
<div class="line">        await self.awarmup() <span class="comment"># Perform internal setup</span></div>
<div class="line">        self.ready = <span class="keyword">True</span></div>
<div class="line">        <span class="keywordflow">return</span> self</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>__aexit__(self, exc_type, exc_val, exc_tb):</div>
<div class="line">        <span class="comment"># This is called when exiting &#39;async with&#39;. It cleans up.</span></div>
<div class="line">        await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb)</div>
<div class="line">        self.ready = <span class="keyword">False</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>arun(self, url: str, config: CrawlerRunConfig = <span class="keywordtype">None</span>) -&gt; CrawlResult:</div>
<div class="line">        <span class="comment"># 1. Ensure config exists, set defaults (like CacheMode.ENABLED)</span></div>
<div class="line">        crawler_config = config <span class="keywordflow">or</span> CrawlerRunConfig()</div>
<div class="line">        <span class="keywordflow">if</span> crawler_config.cache_mode <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line">            crawler_config.cache_mode = CacheMode.ENABLED</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 2. Create CacheContext to manage caching logic</span></div>
<div class="line">        cache_context = CacheContext(url, crawler_config.cache_mode)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Try reading from cache if allowed</span></div>
<div class="line">        cached_result = <span class="keywordtype">None</span></div>
<div class="line">        <span class="keywordflow">if</span> cache_context.should_read():</div>
<div class="line">            cached_result = await async_db_manager.aget_cached_url(url)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 4. If cache hit and valid, return cached result</span></div>
<div class="line">        <span class="keywordflow">if</span> cached_result <span class="keywordflow">and</span> self._is_cache_valid(cached_result, crawler_config):</div>
<div class="line">             <span class="comment"># ... log cache hit ...</span></div>
<div class="line">             <span class="keywordflow">return</span> cached_result</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 5. If no cache hit or cache invalid/bypassed: Fetch fresh content</span></div>
<div class="line">        <span class="comment">#    Delegate to the configured AsyncCrawlerStrategy</span></div>
<div class="line">        async_response = await self.crawler_strategy.crawl(url, config=crawler_config)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 6. Process the HTML (scrape, filter, extract)</span></div>
<div class="line">        <span class="comment">#    This involves calling other strategies based on config</span></div>
<div class="line">        crawl_result = await self.aprocess_html(</div>
<div class="line">            url=url,</div>
<div class="line">            html=async_response.html,</div>
<div class="line">            config=crawler_config,</div>
<div class="line">            <span class="comment"># ... other details from async_response ...</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 7. Write to cache if allowed</span></div>
<div class="line">        <span class="keywordflow">if</span> cache_context.should_write():</div>
<div class="line">            await async_db_manager.acache_url(crawl_result)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 8. Return the final CrawlResult</span></div>
<div class="line">        <span class="keywordflow">return</span> crawl_result</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>aprocess_html(self, url: str, html: str, config: CrawlerRunConfig, ...) -&gt; CrawlResult:</div>
<div class="line">        <span class="comment"># This internal method handles:</span></div>
<div class="line">        <span class="comment"># - Getting the configured ContentScrapingStrategy</span></div>
<div class="line">        <span class="comment"># - Calling its &#39;scrap&#39; method</span></div>
<div class="line">        <span class="comment"># - Getting the configured MarkdownGenerationStrategy</span></div>
<div class="line">        <span class="comment"># - Calling its &#39;generate_markdown&#39; method</span></div>
<div class="line">        <span class="comment"># - Getting the configured ExtractionStrategy (if any)</span></div>
<div class="line">        <span class="comment"># - Calling its &#39;run&#39; method</span></div>
<div class="line">        <span class="comment"># - Packaging everything into a CrawlResult</span></div>
<div class="line">        <span class="comment"># ... implementation details ...</span></div>
<div class="line">        <span class="keywordflow">pass</span> <span class="comment"># Simplified</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>arun_many(self, urls: List[str], config: Optional[CrawlerRunConfig] = <span class="keywordtype">None</span>, ...) -&gt; List[CrawlResult]:</div>
<div class="line">        <span class="comment"># Uses a Dispatcher (like MemoryAdaptiveDispatcher)</span></div>
<div class="line">        <span class="comment"># to run self.arun for each URL concurrently.</span></div>
<div class="line">        <span class="comment"># ... implementation details using a dispatcher ...</span></div>
<div class="line">        <span class="keywordflow">pass</span> <span class="comment"># Simplified</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># ... other methods like awarmup, close, caching helpers ...</span></div>
</div><!-- fragment --><p>The key takeaway is that <code>AsyncWebCrawler</code> doesn't do the fetching or detailed processing <em>itself</em>. It acts as the central hub, coordinating calls to the various specialized <code>Strategy</code> classes based on the provided configuration.</p>
<h2><a class="anchor" id="autotoc_md1398"></a>
Conclusion</h2>
<p>You've met the General Manager: <code>AsyncWebCrawler</code>!</p>
<ul>
<li>It's the <b>main entry point</b> for using Crawl4AI.</li>
<li>It <b>coordinates</b> all the steps: fetching, caching, scraping, extracting.</li>
<li>You primarily interact with it using <code>async with</code> and the <code>arun()</code> (single URL) or <code>arun_many()</code> (multiple URLs) methods.</li>
<li>It takes a URL and an optional <code>CrawlerRunConfig</code> object to customize the crawl.</li>
<li>It returns a comprehensive <code>CrawlResult</code> object.</li>
</ul>
<p>Now that you understand the central role of <code>AsyncWebCrawler</code>, let's explore how to give it detailed instructions for each crawling job.</p>
<p><b>Next:</b> Let's dive into the specifics of configuration with <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">Chapter 3: Giving Instructions - CrawlerRunConfig</a>.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
