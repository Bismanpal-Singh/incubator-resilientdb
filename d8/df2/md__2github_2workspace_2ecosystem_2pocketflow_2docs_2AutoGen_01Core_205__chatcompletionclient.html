#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 05_chatcompletionclient</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d8/df2/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_205__chatcompletionclient.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">05_chatcompletionclient</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md729"></a>
autotoc_md729</h2>
<p>layout: default title: "ChatCompletionClient" parent: "AutoGen Core" </p>
<h2><a class="anchor" id="autotoc_md730"></a>
nav_order: 5</h2>
<h1><a class="anchor" id="autotoc_md731"></a>
Chapter 5: ChatCompletionClient - Talking to the Brains</h1>
<p>So far, we've learned about:</p><ul>
<li><a class="el" href="../../d6/d55/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_201__agent.html">Agents</a>: The workers in our system.</li>
<li><a class="el" href="../../d8/ddc/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_202__messaging__system____topic______subscription__.html">Messaging</a>: How agents communicate broadly.</li>
<li><a class="el" href="../../db/ded/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_203__agentruntime.html">AgentRuntime</a>: The manager that runs the show.</li>
<li><a class="el" href="../../d8/d8c/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_204__tool.html">Tools</a>: How agents get specific skills.</li>
</ul>
<p>But how does an agent actually <em>think</em> or <em>generate text</em>? Many powerful agents rely on Large Language Models (LLMs) – think of models like GPT-4, Claude, or Gemini – as their "brains". How does an agent in AutoGen Core communicate with these external LLM services?</p>
<p>This is where the <b><code>ChatCompletionClient</code></b> comes in. It's the dedicated component for talking to LLMs.</p>
<h2><a class="anchor" id="autotoc_md732"></a>
Motivation: Bridging the Gap to LLMs</h2>
<p>Imagine you want to build an agent that can summarize long articles.</p><ol type="1">
<li>You give the agent an article (as a message).</li>
<li>The agent needs to send this article to an LLM (like GPT-4).</li>
<li>It also needs to tell the LLM: "Please summarize this."</li>
<li>The LLM processes the request and generates a summary.</li>
<li>The agent needs to receive this summary back from the LLM.</li>
</ol>
<p>How does the agent handle the technical details of connecting to the LLM's specific API, formatting the request correctly, sending it over the internet, and understanding the response?</p>
<p>The <code>ChatCompletionClient</code> solves this! Think of it as the <b>standard phone line and translator</b> connecting your agent to the LLM service. You tell the client <em>what</em> to say (the conversation history and instructions), and it handles <em>how</em> to say it to the specific LLM and translates the LLM's reply back into a standard format.</p>
<h2><a class="anchor" id="autotoc_md733"></a>
Key Concepts: Understanding the LLM Communicator</h2>
<p>Let's break down the <code>ChatCompletionClient</code>:</p>
<ol type="1">
<li><b>LLM Communication Bridge:</b> It's the primary way AutoGen agents interact with external LLM APIs (like OpenAI, Anthropic, Google Gemini, etc.). It hides the complexity of specific API calls.</li>
<li><b>Standard Interface (<code>create</code> method):</b> It defines a common way to send requests and receive responses, regardless of the underlying LLM. The core method is <code>create</code>. You give it:<ul>
<li><code>messages</code>: A list of messages representing the conversation history so far.</li>
<li>Optional <code>tools</code>: A list of tools (<a class="el" href="../../d8/d8c/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_204__tool.html">Chapter 4</a>) the LLM might be able to use.</li>
<li>Other parameters (like <code>json_output</code> hints, <code>cancellation_token</code>).</li>
</ul>
</li>
<li><b>Messages (<code>LLMMessage</code>):</b> The conversation history is passed as a sequence of specific message types defined in <code>autogen_core.models</code>:<ul>
<li><code>SystemMessage</code>: Instructions for the LLM (e.g., "You are a helpful assistant.").</li>
<li><code>UserMessage</code>: Input from the user or another agent (e.g., the article text).</li>
<li><code>AssistantMessage</code>: Previous responses from the LLM (can include text or requests to call functions/tools).</li>
<li><code>FunctionExecutionResultMessage</code>: The results of executing a tool/function call.</li>
</ul>
</li>
<li><b>Tools (<code>ToolSchema</code>):</b> You can provide the schemas of available tools (<a class="el" href="../../d8/d8c/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_204__tool.html">Chapter 4</a>). The LLM might then respond not with text, but with a request to call one of these tools (<code>FunctionCall</code> inside an <code>AssistantMessage</code>).</li>
<li><b>Response (<code>CreateResult</code>):</b> The <code>create</code> method returns a standard <code>CreateResult</code> object containing:<ul>
<li><code>content</code>: The LLM's generated text or a list of <code>FunctionCall</code> requests.</li>
<li><code>finish_reason</code>: Why the LLM stopped generating (e.g., "stop", "length", "function_calls").</li>
<li><code>usage</code>: How many input (<code>prompt_tokens</code>) and output (<code>completion_tokens</code>) tokens were used.</li>
<li><code>cached</code>: Whether the response came from a cache.</li>
</ul>
</li>
<li><b>Token Tracking:</b> The client automatically tracks token usage (<code>prompt_tokens</code>, <code>completion_tokens</code>) for each call. You can query the total usage via methods like <code>total_usage()</code>. This is vital for monitoring costs, as most LLM APIs charge based on tokens.</li>
</ol>
<h2><a class="anchor" id="autotoc_md734"></a>
Use Case Example: Summarizing Text with an LLM</h2>
<p>Let's build a simplified scenario where we use a <code>ChatCompletionClient</code> to ask an LLM to summarize text.</p>
<p><b>Goal:</b> Send text to an LLM via a client and get a summary back.</p>
<p><b>Step 1: Prepare the Input Messages</b></p>
<p>We need to structure our request as a list of <code>LLMMessage</code> objects.</p>
<div class="fragment"><div class="line"><span class="comment"># File: prepare_messages.py</span></div>
<div class="line"><span class="keyword">from</span> autogen_core.models <span class="keyword">import</span> SystemMessage, UserMessage</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Instructions for the LLM</span></div>
<div class="line">system_prompt = SystemMessage(</div>
<div class="line">    content=<span class="stringliteral">&quot;You are a helpful assistant designed to summarize text concisely.&quot;</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># The text we want to summarize</span></div>
<div class="line">article_text = <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral">AutoGen is a framework that enables the development of LLM applications using multiple agents</span></div>
<div class="line"><span class="stringliteral">that can converse with each other to solve tasks. AutoGen agents are customizable,</span></div>
<div class="line"><span class="stringliteral">conversable, and can seamlessly allow human participation. They can operate in various modes</span></div>
<div class="line"><span class="stringliteral">that employ combinations of LLMs, human inputs, and tools.</span></div>
<div class="line"><span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line">user_request = UserMessage(</div>
<div class="line">    content=f<span class="stringliteral">&quot;Please summarize the following text in one sentence:\n\n{article_text}&quot;</span>,</div>
<div class="line">    source=<span class="stringliteral">&quot;User&quot;</span> <span class="comment"># Indicate who provided this input</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Combine into a list for the client</span></div>
<div class="line">messages_to_send = [system_prompt, user_request]</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;Messages prepared:&quot;</span>)</div>
<div class="line"><span class="keywordflow">for</span> msg <span class="keywordflow">in</span> messages_to_send:</div>
<div class="line">    print(f<span class="stringliteral">&quot;- {msg.type}: {msg.content[:50]}...&quot;</span>) <span class="comment"># Print first 50 chars</span></div>
</div><!-- fragment --><p> This code defines the instructions (<code>SystemMessage</code>) and the user's request (<code>UserMessage</code>) and puts them in a list, ready to be sent.</p>
<p><b>Step 2: Use the ChatCompletionClient (Conceptual)</b></p>
<p>Now, we need an instance of a <code>ChatCompletionClient</code>. In a real application, you'd configure a specific client (like <code>OpenAIChatCompletionClient</code> with your API key). For this example, let's imagine we have a pre-configured client called <code>llm_client</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># File: call_llm_client.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> autogen_core.models <span class="keyword">import</span> CreateResult, RequestUsage</div>
<div class="line"><span class="comment"># Assume &#39;messages_to_send&#39; is from the previous step</span></div>
<div class="line"><span class="comment"># Assume &#39;llm_client&#39; is a pre-configured ChatCompletionClient instance</span></div>
<div class="line"><span class="comment"># (e.g., llm_client = OpenAIChatCompletionClient(config=...))</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span>get_summary(client, messages):</div>
<div class="line">    print(<span class="stringliteral">&quot;\nSending messages to LLM via ChatCompletionClient...&quot;</span>)</div>
<div class="line">    <span class="keywordflow">try</span>:</div>
<div class="line">        <span class="comment"># The core call: send messages, get structured result</span></div>
<div class="line">        response: CreateResult = await client.create(</div>
<div class="line">            messages=messages,</div>
<div class="line">            <span class="comment"># We aren&#39;t providing tools in this simple example</span></div>
<div class="line">            tools=[]</div>
<div class="line">        )</div>
<div class="line">        print(<span class="stringliteral">&quot;Received response:&quot;</span>)</div>
<div class="line">        print(f<span class="stringliteral">&quot;- Finish Reason: {response.finish_reason}&quot;</span>)</div>
<div class="line">        print(f<span class="stringliteral">&quot;- Content: {response.content}&quot;</span>) <span class="comment"># This should be the summary</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;- Usage (Tokens): Prompt={response.usage.prompt_tokens}, Completion={response.usage.completion_tokens}&quot;</span>)</div>
<div class="line">        print(f<span class="stringliteral">&quot;- Cached: {response.cached}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Also, check total usage tracked by the client</span></div>
<div class="line">        total_usage = client.total_usage()</div>
<div class="line">        print(f<span class="stringliteral">&quot;\nClient Total Usage: Prompt={total_usage.prompt_tokens}, Completion={total_usage.completion_tokens}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">        print(f<span class="stringliteral">&quot;An error occurred: {e}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># --- Placeholder for actual client ---</span></div>
<div class="line"><span class="keyword">class </span>MockChatCompletionClient: <span class="comment"># Simulate a real client</span></div>
<div class="line">    _total_usage = RequestUsage(prompt_tokens=0, completion_tokens=0)</div>
<div class="line">    <span class="keyword">async def </span>create(self, messages, tools=[], **kwargs) -&gt; CreateResult:</div>
<div class="line">        <span class="comment"># Simulate API call and response</span></div>
<div class="line">        prompt_len = sum(len(str(m.content)) <span class="keywordflow">for</span> m <span class="keywordflow">in</span> messages) // 4 <span class="comment"># Rough token estimate</span></div>
<div class="line">        summary = <span class="stringliteral">&quot;AutoGen is a multi-agent framework for developing LLM applications.&quot;</span></div>
<div class="line">        completion_len = len(summary) // 4 <span class="comment"># Rough token estimate</span></div>
<div class="line">        usage = RequestUsage(prompt_tokens=prompt_len, completion_tokens=completion_len)</div>
<div class="line">        self._total_usage.prompt_tokens += usage.prompt_tokens</div>
<div class="line">        self._total_usage.completion_tokens += usage.completion_tokens</div>
<div class="line">        <span class="keywordflow">return</span> CreateResult(</div>
<div class="line">            finish_reason=<span class="stringliteral">&quot;stop&quot;</span>, content=summary, usage=usage, cached=<span class="keyword">False</span></div>
<div class="line">        )</div>
<div class="line">    <span class="keyword">def </span>total_usage(self) -&gt; RequestUsage: <span class="keywordflow">return</span> self._total_usage</div>
<div class="line">    <span class="comment"># Other required methods (count_tokens, model_info etc.) omitted for brevity</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="keyword">from</span> prepare_messages <span class="keyword">import</span> messages_to_send <span class="comment"># Get messages from previous step</span></div>
<div class="line">    mock_client = MockChatCompletionClient()</div>
<div class="line">    await get_summary(mock_client, messages_to_send)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># asyncio.run(main()) # If you run this, it uses the mock client</span></div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p> This code shows the essential <code>client.create(...)</code> call. We pass our <code>messages_to_send</code> and receive a <code>CreateResult</code>. We then print the summary (<code>response.content</code>) and the token usage reported for that specific call (<code>response.usage</code>) and the total tracked by the client (<code>client.total_usage()</code>).</p>
<p><b>How an Agent Uses It:</b> Typically, an agent's logic (e.g., inside its <code>on_message</code> handler) would:</p><ol type="1">
<li>Receive an incoming message (like the article to summarize).</li>
<li>Prepare the list of <code>LLMMessage</code> objects (including system prompts, history, and the new request).</li>
<li>Access a <code>ChatCompletionClient</code> instance (often provided during agent setup or accessed via its context).</li>
<li>Call <code>await client.create(...)</code>.</li>
<li>Process the <code>CreateResult</code> (e.g., extract the summary text, check for function calls if tools were provided).</li>
<li>Potentially send the result as a new message to another agent or return it.</li>
</ol>
<h2><a class="anchor" id="autotoc_md735"></a>
Under the Hood: How the Client Talks to the LLM</h2>
<p>What happens when you call <code>await client.create(...)</code>?</p>
<p><b>Conceptual Flow:</b></p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant Agent as Agent Logic</div>
<div class="line">    participant Client as ChatCompletionClient</div>
<div class="line">    participant Formatter as API Formatter</div>
<div class="line">    participant HTTP as HTTP Client</div>
<div class="line">    participant LLM_API as External LLM API</div>
<div class="line"> </div>
<div class="line">    Agent-&gt;&gt;+Client: create(messages, tools)</div>
<div class="line">    Client-&gt;&gt;+Formatter: Format messages &amp; tools for specific API (e.g., OpenAI JSON format)</div>
<div class="line">    Formatter--&gt;&gt;-Client: Return formatted request body</div>
<div class="line">    Client-&gt;&gt;+HTTP: Send POST request to LLM API endpoint with formatted body &amp; API Key</div>
<div class="line">    HTTP-&gt;&gt;+LLM_API: Transmit request over network</div>
<div class="line">    LLM_API-&gt;&gt;LLM_API: Process request, generate completion/function call</div>
<div class="line">    LLM_API--&gt;&gt;-HTTP: Return API response (e.g., JSON)</div>
<div class="line">    HTTP--&gt;&gt;-Client: Receive HTTP response</div>
<div class="line">    Client-&gt;&gt;+Formatter: Parse API response (extract content, usage, finish_reason)</div>
<div class="line">    Formatter--&gt;&gt;-Client: Return parsed data</div>
<div class="line">    Client-&gt;&gt;Client: Create standard CreateResult object</div>
<div class="line">    Client--&gt;&gt;-Agent: Return CreateResult</div>
</div><!-- fragment --><ol type="1">
<li><b>Prepare:</b> The <code>ChatCompletionClient</code> takes the standard <code>LLMMessage</code> list and <code>ToolSchema</code> list.</li>
<li><b>Format:</b> It translates these into the specific format required by the target LLM's API (e.g., the JSON structure expected by OpenAI's <code>/chat/completions</code> endpoint). This might involve renaming roles (like <code>SystemMessage</code> to <code>system</code>), formatting tool descriptions, etc.</li>
<li><b>Request:</b> It uses an underlying HTTP client to send a network request (usually a POST request) to the LLM service's API endpoint, including the formatted data and authentication (like an API key).</li>
<li><b>Wait &amp; Receive:</b> It waits for the LLM service to process the request and send back a response over the network.</li>
<li><b>Parse:</b> It receives the raw HTTP response (usually JSON) from the API.</li>
<li><b>Standardize:</b> It parses this specific API response, extracting the generated text or function calls, token usage figures, finish reason, etc.</li>
<li><b>Return:</b> It packages all this information into a standard <code>CreateResult</code> object and returns it to the calling agent code.</li>
</ol>
<p><b>Code Glimpse:</b></p>
<ul>
<li><p class="startli"><b><code>ChatCompletionClient</code> Protocol (<code>models/_model_client.py</code>):</b> This is the abstract base class (or protocol) defining the <em>contract</em> that all specific clients must follow.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md736"></a>
From: models/_model_client.py (Simplified ABC)</h1>
<p>from abc import ABC, abstractmethod from typing import Sequence, Optional, Mapping, Any, AsyncGenerator, Union from ._types import LLMMessage, CreateResult, RequestUsage from ..tools import Tool, ToolSchema from .. import CancellationToken</p>
<p>class ChatCompletionClient(ABC): @abstractmethod async def create( self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = [], json_output: Optional[bool] = None, # Hint for JSON mode extra_create_args: Mapping[str, Any] = {}, # API-specific args cancellation_token: Optional[CancellationToken] = None, ) -&gt; CreateResult: ... # The core method</p>
<p>@abstractmethod def create_stream( self, # Similar to create, but yields results incrementally </p>
<h1><a class="anchor" id="autotoc_md737"></a>
... parameters ...</h1>
<p>) -&gt; AsyncGenerator[Union[str, CreateResult], None]: ...</p>
<p>@abstractmethod def total_usage(self) -&gt; RequestUsage: ... # Get total tracked usage</p>
<p>@abstractmethod def count_tokens(self, messages: Sequence[LLMMessage], *, tools: Sequence[Tool | ToolSchema] = []) -&gt; int: ... # Estimate token count</p>
<h1><a class="anchor" id="autotoc_md738"></a>
Other methods like close(), actual_usage(), remaining_tokens(), model_info...</h1>
<p>``<code> Concrete classes like</code>OpenAIChatCompletionClient<code>,</code>AnthropicChatCompletionClient` etc., implement these methods using the specific libraries and API calls for each service.</p>
<ul>
<li><p class="startli"><b><code>LLMMessage</code> Types (<code>models/_types.py</code>):</b> These define the structure of messages passed <em>to</em> the client.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md739"></a>
From: models/_types.py (Simplified)</h1>
<p>from pydantic import BaseModel from typing import List, Union, Literal from .. import FunctionCall # From Chapter 4 context</p>
<p>class SystemMessage(BaseModel): content: str type: Literal["SystemMessage"] = "SystemMessage"</p>
<p>class UserMessage(BaseModel): content: Union[str, List[Union[str, Image]]] # Can include images! source: str type: Literal["UserMessage"] = "UserMessage"</p>
<p>class AssistantMessage(BaseModel): content: Union[str, List[FunctionCall]] # Can be text or function calls source: str type: Literal["AssistantMessage"] = "AssistantMessage"</p>
<h1><a class="anchor" id="autotoc_md740"></a>
FunctionExecutionResultMessage also exists here...</h1>
<p>```</p>
<ul>
<li><p class="startli"><b><code>CreateResult</code> (<code>models/_types.py</code>):</b> This defines the structure of the response <em>from</em> the client.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md741"></a>
From: models/_types.py (Simplified)</h1>
<p>from pydantic import BaseModel from dataclasses import dataclass from typing import Union, List, Optional from .. import FunctionCall</p>
<p>@dataclass class RequestUsage: prompt_tokens: int completion_tokens: int</p>
<p>FinishReasons = Literal["stop", "length", "function_calls", "content_filter", "unknown"]</p>
<p>class CreateResult(BaseModel): finish_reason: FinishReasons content: Union[str, List[FunctionCall]] # LLM output usage: RequestUsage # Token usage for this call cached: bool </p>
<h1><a class="anchor" id="autotoc_md742"></a>
Optional fields like logprobs, thought...</h1>
<p>``<code> Using these standard types ensures that agent logic can work consistently, even if you switch the underlying LLM service by using a different</code>ChatCompletionClient` implementation.</p>
<h2><a class="anchor" id="autotoc_md743"></a>
Next Steps</h2>
<p>You now understand the role of <code>ChatCompletionClient</code> as the crucial link between AutoGen agents and the powerful capabilities of Large Language Models. It provides a standard way to send conversational history and tool definitions, receive generated text or function call requests, and track token usage.</p>
<p>Managing the conversation history (<code>messages</code>) sent to the client is very important. How do you ensure the LLM has the right context, especially after tool calls have happened?</p>
<ul>
<li><a class="el" href="../../dd/dc3/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_206__chatcompletioncontext.html">Chapter 6: ChatCompletionContext</a>: Learn how AutoGen helps manage the conversation history, including adding tool call requests and their results, before sending it to the <code>ChatCompletionClient</code>.</li>
</ul>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
