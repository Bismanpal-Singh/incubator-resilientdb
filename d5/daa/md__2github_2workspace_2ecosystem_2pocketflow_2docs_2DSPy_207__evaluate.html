#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 07_evaluate</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d5/daa/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_207__evaluate.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">07_evaluate</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1679"></a>
autotoc_md1679</h2>
<p>layout: default title: "Evaluate" parent: "DSPy" </p>
<h2><a class="anchor" id="autotoc_md1680"></a>
nav_order: 7</h2>
<h1><a class="anchor" id="autotoc_md1681"></a>
Chapter 7: Evaluate - Grading Your Program</h1>
<p>In the previous chapter, <a class="el" href="../../d9/d67/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_206__rm____retrieval__model__client__.html">Chapter 6: RM (Retrieval Model Client)</a>, we learned how to connect our DSPy program to external knowledge sources using Retrieval Models (RMs). We saw how combining RMs with Language Models (LMs) allows us to build sophisticated programs like Retrieval-Augmented Generation (RAG) systems.</p>
<p>Now that we can build these powerful programs, a crucial question arises: <b>How good are they?</b> If we build a RAG system to answer questions, how often does it get the answer right? How do we measure its performance objectively?</p>
<p>This is where <b><code>dspy.Evaluate</code></b> comes in! It's DSPy's built-in tool for testing and grading your programs.</p>
<p>Think of <code>dspy.Evaluate</code> as:</p>
<ul>
<li><b>An Automated Grader:</b> Like a teacher grading a batch of homework assignments based on an answer key.</li>
<li><b>A Test Suite Runner:</b> Similar to how software developers use test suites to check if their code works correctly.</li>
<li><b>Your Program's Report Card:</b> It gives you a score that tells you how well your DSPy program is performing on a specific set of tasks.</li>
</ul>
<p>In this chapter, you'll learn:</p>
<ul>
<li>What you need to evaluate a DSPy program.</li>
<li>How to define a metric (a grading rule).</li>
<li>How to use <code>dspy.Evaluate</code> to run the evaluation and get a score.</li>
<li>How it works behind the scenes.</li>
</ul>
<p>Let's learn how to grade our DSPy creations!</p>
<h2><a class="anchor" id="autotoc_md1682"></a>
The Ingredients for Evaluation</h2>
<p>To grade your program using <code>dspy.Evaluate</code>, you need three main ingredients:</p>
<ol type="1">
<li><b>Your DSPy <code>Program</code>:</b> The program you want to test. This could be a simple <code>dspy.Predict</code> module or a complex multi-step program like the <code>SimpleRAG</code> we sketched out in the last chapter.</li>
<li><b>A Dataset (<code>devset</code>):</b> A list of <code>dspy.Example</code> objects (<a class="el" href="../../dc/db9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_203__example.html">Chapter 3: Example</a>). Crucially, these examples must contain not only the <b>inputs</b> your program expects but also the <b>gold standard outputs</b> (the correct answers or desired results) that you want to compare against. This dataset is often called a "development set" or "dev set".</li>
<li><b>A Metric Function (<code>metric</code>):</b> A Python function you define. This function takes one gold standard <code>Example</code> and the <code>Prediction</code> generated by your program for that example's inputs. It then compares them and returns a score indicating how well the prediction matched the gold standard. The score is often <code>1.0</code> for a perfect match and <code>0.0</code> for a mismatch, but it can also be a fractional score (e.g., for F1 score).</li>
</ol>
<p><code>dspy.Evaluate</code> takes these three ingredients, runs your program on all examples in the dataset, uses your metric function to score each prediction against the gold standard, and finally reports the average score across the entire dataset.</p>
<h2><a class="anchor" id="autotoc_md1683"></a>
Evaluating a Simple Question Answering Program</h2>
<p>Let's illustrate this with a simple example. Suppose we have a basic DSPy program that's supposed to answer simple questions.</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> dspy</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Assume we have configured an LM client (Chapter 5)</span></div>
<div class="line"><span class="comment"># gpt3_turbo = dspy.LM(model=&#39;openai/gpt-3.5-turbo&#39;)</span></div>
<div class="line"><span class="comment"># dspy.settings.configure(lm=gpt3_turbo)</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># A simple program using dspy.Predict (Chapter 4)</span></div>
<div class="line"><span class="keyword">class </span>BasicQA(dspy.Module):</div>
<div class="line">    <span class="keyword">def </span>__init__(self):</div>
<div class="line">        super().__init__()</div>
<div class="line">        <span class="comment"># Use a simple signature: question -&gt; answer</span></div>
<div class="line">        self.predictor = dspy.Predict(<span class="stringliteral">&#39;question -&gt; answer&#39;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>forward(self, question):</div>
<div class="line">        <span class="keywordflow">return</span> self.predictor(question=question)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create an instance of our program</span></div>
<div class="line">qa_program = BasicQA()</div>
</div><!-- fragment --><p>Now, let's prepare the other ingredients for evaluation.</p>
<p><b>1. Prepare the Dataset (<code>devset</code>)</b></p>
<p>We need a list of <code>dspy.Example</code> objects, each containing a <code>question</code> (input) and the correct <code>answer</code> (gold standard output).</p>
<div class="fragment"><div class="line"><span class="comment"># Create example data points with questions and gold answers</span></div>
<div class="line">dev_example1 = dspy.Example(question=<span class="stringliteral">&quot;What color is the sky?&quot;</span>, answer=<span class="stringliteral">&quot;blue&quot;</span>)</div>
<div class="line">dev_example2 = dspy.Example(question=<span class="stringliteral">&quot;What is 2 + 2?&quot;</span>, answer=<span class="stringliteral">&quot;4&quot;</span>)</div>
<div class="line">dev_example3 = dspy.Example(question=<span class="stringliteral">&quot;What is the capital of France?&quot;</span>, answer=<span class="stringliteral">&quot;Paris&quot;</span>)</div>
<div class="line">dev_example_wrong = dspy.Example(question=<span class="stringliteral">&quot;Who wrote Hamlet?&quot;</span>, answer=<span class="stringliteral">&quot;Shakespeare&quot;</span>) <span class="comment"># Let&#39;s assume our QA program might get this wrong</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create the development set (list of examples)</span></div>
<div class="line">devset = [dev_example1, dev_example2, dev_example3, dev_example_wrong]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># We need to tell DSPy which fields are inputs vs outputs for evaluation</span></div>
<div class="line"><span class="comment"># The .with_inputs() method marks the input keys.</span></div>
<div class="line"><span class="comment"># The remaining keys (&#39;answer&#39; in this case) are treated as labels.</span></div>
<div class="line">devset = [d.with_inputs(<span class="stringliteral">&#39;question&#39;</span>) <span class="keywordflow">for</span> d <span class="keywordflow">in</span> devset]</div>
</div><!-- fragment --><p> Here, we've created a small dataset <code>devset</code> with four question-answer pairs. We used &lsquo;.with_inputs('question&rsquo;)<code>to mark the</code>question<code>field as the input;</code>dspy.Evaluate<code>will automatically treat the remaining field (</code>answer`) as the gold label to compare against.</p>
<p><b>2. Define a Metric Function (<code>metric</code>)</b></p>
<p>We need a function that compares the program's predicted answer to the gold answer in an example. Let's create a simple "exact match" metric.</p>
<div class="fragment"><div class="line"><span class="keyword">def </span>simple_exact_match_metric(gold_example, prediction, trace=None):</div>
<div class="line">    <span class="comment"># Does the predicted &#39;answer&#39; EXACTLY match the gold &#39;answer&#39;?</span></div>
<div class="line">    <span class="comment"># &#39;.answer&#39; field comes from our Predict signature &#39;question -&gt; answer&#39;</span></div>
<div class="line">    <span class="comment"># &#39;gold_example.answer&#39; is the gold label from the devset example</span></div>
<div class="line">    <span class="keywordflow">return</span> prediction.answer == gold_example.answer</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Note: DSPy often provides common metrics too, like dspy.evaluate.answer_exact_match</span></div>
<div class="line"><span class="comment"># import dspy.evaluate</span></div>
<div class="line"><span class="comment"># metric = dspy.evaluate.answer_exact_match</span></div>
</div><!-- fragment --><p> Our <code>simple_exact_match_metric</code> function takes the gold <code>dspy.Example</code> (<code>gold_example</code>) and the program's output <code>dspy.Prediction</code> (<code>prediction</code>). It returns <code>True</code> (which Python treats as <code>1.0</code>) if the predicted <code>answer</code> matches the gold <code>answer</code>, and <code>False</code> (<code>0.0</code>) otherwise. The <code>trace</code> argument is optional and can be ignored for basic metrics; it sometimes contains information about the program's execution steps.</p>
<p><b>3. Create and Run <code>dspy.Evaluate</code></b></p>
<p>Now we have all the ingredients: <code>qa_program</code>, <code>devset</code>, and <code>simple_exact_match_metric</code>. Let's use <code>dspy.Evaluate</code>.</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> dspy.evaluate <span class="keyword">import</span> Evaluate</div>
<div class="line"> </div>
<div class="line"><span class="comment"># 1. Create the Evaluator instance</span></div>
<div class="line">evaluator = Evaluate(</div>
<div class="line">    devset=devset,            <span class="comment"># The dataset to evaluate on</span></div>
<div class="line">    metric=simple_exact_match_metric, <span class="comment"># The function to score predictions</span></div>
<div class="line">    num_threads=4,            <span class="comment"># Run 4 evaluations in parallel (optional)</span></div>
<div class="line">    display_progress=<span class="keyword">True</span>,    <span class="comment"># Show a progress bar (optional)</span></div>
<div class="line">    display_table=<span class="keyword">True</span>        <span class="comment"># Display results in a table (optional)</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># 2. Run the evaluation by calling the evaluator with the program</span></div>
<div class="line"><span class="comment"># This will run qa_program on each example in devset,</span></div>
<div class="line"><span class="comment"># score it using simple_exact_match_metric, and return the average score.</span></div>
<div class="line">average_score = evaluator(qa_program)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Average Score: {average_score}%&quot;</span>)</div>
</div><!-- fragment --><p><b>What happens here?</b></p>
<ol type="1">
<li>We create an <code>Evaluate</code> object, providing our dataset and metric. We also request parallel execution (<code>num_threads=4</code>) for speed and ask for progress/table display.</li>
<li>We call the <code>evaluator</code> instance with our <code>qa_program</code>.</li>
<li><code>Evaluate</code> iterates through <code>devset</code>:<ul>
<li>For <code>dev_example1</code>, it calls <code>qa_program(question="What color is the sky?")</code>. Let's assume the program predicts <code>answer="blue"</code>.</li>
<li>It calls <code>simple_exact_match_metric(dev_example1, predicted_output)</code>. Since <code>"blue" == "blue"</code>, the score is <code>1.0</code>.</li>
<li>It does the same for <code>dev_example2</code> (input: "What is 2 + 2?"). Assume prediction is <code>answer="4"</code>. Score: <code>1.0</code>.</li>
<li>It does the same for <code>dev_example3</code> (input: "What is the capital of France?"). Assume prediction is <code>answer="Paris"</code>. Score: <code>1.0</code>.</li>
<li>It does the same for <code>dev_example_wrong</code> (input: "Who wrote Hamlet?"). Maybe the simple LM messes up and predicts <code>answer="William Shakespeare"</code>. Since <code>"William Shakespeare" != "Shakespeare"</code>, the score is <code>0.0</code>.</li>
</ul>
</li>
<li><code>Evaluate</code> calculates the average score: <code>(1.0 + 1.0 + 1.0 + 0.0) / 4 = 0.75</code>.</li>
<li>It prints the average score as a percentage.</li>
</ol>
<p><b>Expected Output:</b></p>
<p>A progress bar will be shown (if <code>tqdm</code> is installed), followed by a table like this (requires <code>pandas</code>):</p>
<div class="fragment"><div class="line">Average Metric: 3 / 4  (75.0%)</div>
<div class="line">  question                           answer      simple_exact_match_metric</div>
<div class="line">0 What color is the sky?           blue        ✔️ [True]</div>
<div class="line">1 What is 2 + 2?                   4           ✔️ [True]</div>
<div class="line">2 What is the capital of France?   Paris       ✔️ [True]</div>
<div class="line">3 Who wrote Hamlet?                Shakespeare </div>
</div><!-- fragment --><p> <em>(Note: The table shows the predicted answer if different, otherwise just the metric outcome. The exact table format might vary slightly).</em></p>
<p>And finally: </p><div class="fragment"><div class="line">Average Score: 75.0%</div>
</div><!-- fragment --><p>This tells us our simple QA program achieved 75% accuracy on our small development set using the exact match criterion.</p>
<h2><a class="anchor" id="autotoc_md1684"></a>
Getting More Details (Optional Flags)</h2>
<p>Sometimes, just the average score isn't enough. You might want to see the score for each individual example or the actual predictions made by the program. <code>Evaluate</code> provides flags for this:</p>
<ul>
<li><code>return_all_scores=True</code>: Returns the average score <em>and</em> a list containing the individual score for each example.</li>
<li><code>return_outputs=True</code>: Returns the average score <em>and</em> a list of tuples, where each tuple contains <code>(example, prediction, score)</code>.</li>
</ul>
<div class="fragment"><div class="line"><span class="comment"># Re-run evaluation asking for more details</span></div>
<div class="line">evaluator_detailed = Evaluate(devset=devset, metric=simple_exact_match_metric)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Get individual scores</span></div>
<div class="line">avg_score, individual_scores = evaluator_detailed(qa_program, return_all_scores=<span class="keyword">True</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;Individual Scores: {individual_scores}&quot;</span>) <span class="comment"># Output: [True, True, True, False]</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Get full outputs</span></div>
<div class="line">avg_score, outputs_list = evaluator_detailed(qa_program, return_outputs=<span class="keyword">True</span>)</div>
<div class="line"><span class="comment"># outputs_list[0] would be roughly: (dev_example1, Prediction(answer=&#39;blue&#39;), True)</span></div>
<div class="line"><span class="comment"># outputs_list[3] would be roughly: (dev_example_wrong, Prediction(answer=&#39;William Shakespeare&#39;), False)</span></div>
<div class="line">print(f<span class="stringliteral">&quot;Number of outputs returned: {len(outputs_list)}&quot;</span>) <span class="comment"># Output: 4</span></div>
</div><!-- fragment --><p>These flags are useful for more detailed error analysis to understand <em>where</em> your program is failing.</p>
<h2><a class="anchor" id="autotoc_md1685"></a>
How It Works Under the Hood</h2>
<p>What happens internally when you call <code>evaluator(program)</code>?</p>
<ol type="1">
<li><b>Initialization:</b> The <code>Evaluate</code> instance stores the <code>devset</code>, <code>metric</code>, <code>num_threads</code>, and other settings.</li>
<li><b>Parallel Executor:</b> It creates a <code>ParallelExecutor</code> (if <code>num_threads &gt; 1</code>) to manage running the evaluations concurrently.</li>
<li><b>Iteration:</b> It iterates through each <code>example</code> in the <code>devset</code>.</li>
<li><b>Program Execution:</b> For each <code>example</code>, it calls <code>program(**example.inputs())</code> (e.g., <code>qa_program(question=example.question)</code>). This runs your DSPy program's <code>forward</code> method to get a <code>prediction</code>.</li>
<li><b>Metric Calculation:</b> It calls the provided <code>metric</code> function, passing it the original <code>example</code> (which contains the gold labels) and the <code>prediction</code> object returned by the program (e.g., <code>metric(example, prediction)</code>). This yields a <code>score</code>.</li>
<li><b>Error Handling:</b> If running the program or the metric causes an error for a specific example, <code>Evaluate</code> catches it (up to <code>max_errors</code>), records a default <code>failure_score</code> (usually 0.0), and continues with the rest of the dataset.</li>
<li><b>Aggregation:</b> It collects all the individual scores (including failure scores).</li>
<li><b>Calculate Average:</b> It computes the average score by summing all scores and dividing by the total number of examples in the <code>devset</code>.</li>
<li><b>Return Results:</b> It returns the average score (and optionally the individual scores or full output tuples based on the flags).</li>
</ol>
<p>Here's a simplified sequence diagram:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant Evaluator as dspy.Evaluate</div>
<div class="line">    participant Executor as ParallelExecutor</div>
<div class="line">    participant Program as Your DSPy Program</div>
<div class="line">    participant Metric as Your Metric Function</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;Evaluator: __call__(program)</div>
<div class="line">    Evaluator-&gt;&gt;Executor: Create (manages threads)</div>
<div class="line">    loop For each example in devset</div>
<div class="line">        Executor-&gt;&gt;Executor: Assign task to a thread</div>
<div class="line">        Note over Executor, Program: In parallel thread:</div>
<div class="line">        Executor-&gt;&gt;Program: Call program(**example.inputs())</div>
<div class="line">        Program--&gt;&gt;Executor: Return prediction</div>
<div class="line">        Executor-&gt;&gt;Metric: Call metric(example, prediction)</div>
<div class="line">        Metric--&gt;&gt;Executor: Return score</div>
<div class="line">    end</div>
<div class="line">    Executor-&gt;&gt;Evaluator: Collect all results (predictions, scores)</div>
<div class="line">    Evaluator-&gt;&gt;Evaluator: Calculate average score</div>
<div class="line">    Evaluator--&gt;&gt;User: Return average score (and other requested data)</div>
</div><!-- fragment --><p><b>Relevant Code Files:</b></p>
<ul>
<li><code>dspy/evaluate/evaluate.py</code>: Defines the <code>Evaluate</code> class.<ul>
<li>The <code>__init__</code> method stores the configuration.</li>
<li>The <code>__call__</code> method orchestrates the evaluation: sets up the <code>ParallelExecutor</code>, defines the <code>process_item</code> function (which runs the program and metric for one example), executes it over the <code>devset</code>, aggregates results, and handles display/return logic.</li>
</ul>
</li>
<li><code>dspy/utils/parallelizer.py</code>: Contains the <code>ParallelExecutor</code> class used for running tasks concurrently across multiple threads or processes.</li>
<li><code>dspy/evaluate/metrics.py</code>: Contains implementations of common metrics like <code>answer_exact_match</code>.</li>
</ul>
<div class="fragment"><div class="line"><span class="comment"># Simplified view from dspy/evaluate/evaluate.py</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># ... imports ...</span></div>
<div class="line"><span class="keyword">from</span> dspy.utils.parallelizer <span class="keyword">import</span> ParallelExecutor</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>Evaluate:</div>
<div class="line">    <span class="keyword">def </span>__init__(self, devset, metric, num_threads=1, ..., failure_score=0.0):</div>
<div class="line">        self.devset = devset</div>
<div class="line">        self.metric = metric</div>
<div class="line">        self.num_threads = num_threads</div>
<div class="line">        self.display_progress = ...</div>
<div class="line">        self.display_table = ...</div>
<div class="line">        <span class="comment"># ... store other flags ...</span></div>
<div class="line">        self.failure_score = failure_score</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># @with_callbacks # Decorator handles optional logging/callbacks</span></div>
<div class="line">    <span class="keyword">def </span>__call__(self, program, metric=None, devset=None, ...):</div>
<div class="line">        <span class="comment"># Use provided args or fall back to instance attributes</span></div>
<div class="line">        metric = metric <span class="keywordflow">if</span> metric <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> self.metric</div>
<div class="line">        devset = devset <span class="keywordflow">if</span> devset <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> self.devset</div>
<div class="line">        num_threads = ... <span class="comment"># Similar logic for other args</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Create executor for parallelism</span></div>
<div class="line">        executor = ParallelExecutor(num_threads=num_threads, ...)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Define the work to be done for each example</span></div>
<div class="line">        <span class="keyword">def </span>process_item(example):</div>
<div class="line">            <span class="keywordflow">try</span>:</div>
<div class="line">                <span class="comment"># Run the program with the example&#39;s inputs</span></div>
<div class="line">                prediction = program(**example.inputs())</div>
<div class="line">                <span class="comment"># Call the metric function with the gold example and prediction</span></div>
<div class="line">                score = metric(example, prediction)</div>
<div class="line">                <span class="keywordflow">return</span> prediction, score</div>
<div class="line">            <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">                <span class="comment"># Handle errors during program/metric execution</span></div>
<div class="line">                <span class="comment"># Log error, return None or failure score</span></div>
<div class="line">                print(f<span class="stringliteral">&quot;Error processing example: {e}&quot;</span>)</div>
<div class="line">                <span class="keywordflow">return</span> <span class="keywordtype">None</span> <span class="comment"># Executor will handle None later</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Execute process_item for all examples in devset using the executor</span></div>
<div class="line">        raw_results = executor.execute(process_item, devset)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Process results, handle failures (replace None with failure score)</span></div>
<div class="line">        results = []</div>
<div class="line">        <span class="keywordflow">for</span> i, r <span class="keywordflow">in</span> enumerate(raw_results):</div>
<div class="line">            example = devset[i]</div>
<div class="line">            <span class="keywordflow">if</span> r <span class="keywordflow">is</span> <span class="keywordtype">None</span>: <span class="comment"># Execution failed for this example</span></div>
<div class="line">                prediction, score = dspy.Prediction(), self.failure_score</div>
<div class="line">            <span class="keywordflow">else</span>:</div>
<div class="line">                prediction, score = r</div>
<div class="line">            results.append((example, prediction, score))</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Calculate the average score</span></div>
<div class="line">        total_score = sum(score <span class="keywordflow">for</span> *_, score <span class="keywordflow">in</span> results)</div>
<div class="line">        num_examples = len(devset)</div>
<div class="line">        average_score = round(100 * total_score / num_examples, 2) <span class="keywordflow">if</span> num_examples &gt; 0 <span class="keywordflow">else</span> 0</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Display table if requested</span></div>
<div class="line">        <span class="keywordflow">if</span> self.display_table:</div>
<div class="line">             self._display_result_table(...) <span class="comment"># Internal helper function</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Return results based on flags (return_all_scores, return_outputs)</span></div>
<div class="line">        <span class="comment"># ... logic to construct return tuple ...</span></div>
<div class="line">        <span class="keywordflow">return</span> average_score <span class="comment"># Base return value</span></div>
</div><!-- fragment --><p>The core logic involves running the program and the metric function for each data point, handling potential errors, and averaging the results, with parallel processing to speed things up.</p>
<h2><a class="anchor" id="autotoc_md1686"></a>
Conclusion</h2>
<p>You've now learned about <code>dspy.Evaluate</code>, the standard way to measure the performance of your DSPy programs!</p>
<ul>
<li><code>Evaluate</code> acts as an <b>automated grader</b> for your DSPy modules.</li>
<li>It requires three ingredients: your <b>program</b>, a <b>dataset (<code>devset</code>)</b> with gold labels, and a <b>metric function</b> to compare predictions against labels.</li>
<li>It runs the program on the dataset, applies the metric, and reports the <b>average score</b>.</li>
<li>It supports <b>parallel execution</b> for speed and offers options to display progress, show results tables, and return detailed outputs.</li>
</ul>
<p>Knowing how well your program performs is essential. But what if the score isn't good enough? How can we <em>improve</em> the program, perhaps by automatically finding better prompts or few-shot examples?</p>
<p>That's precisely what <b>Teleprompters</b> (Optimizers) are designed for! Let's dive into how DSPy can help automatically optimize your programs next.</p>
<p><b>Next:</b> <a class="el" href="../../dc/d0a/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_208__teleprompter______optimizer.html">Chapter 8: Teleprompter / Optimizer</a></p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
