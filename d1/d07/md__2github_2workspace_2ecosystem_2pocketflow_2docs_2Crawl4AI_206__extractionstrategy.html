#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 06_extractionstrategy</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d1/d07/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_206__extractionstrategy.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">06_extractionstrategy</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1411"></a>
autotoc_md1411</h2>
<p>layout: default title: "ExtractionStrategy" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1412"></a>
nav_order: 6</h2>
<h1><a class="anchor" id="autotoc_md1413"></a>
Chapter 6: Getting Specific Data - ExtractionStrategy</h1>
<p>In the previous chapter, <a class="el" href="../../d2/d40/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_205__relevantcontentfilter.html">Chapter 5: Focusing on What Matters - RelevantContentFilter</a>, we learned how to sift through the cleaned webpage content to keep only the parts relevant to our query or goal, producing a focused <code>fit_markdown</code>. This is great for tasks like summarization or getting the main gist of an article.</p>
<p>But sometimes, we need more than just relevant text. Imagine you're analyzing an e-commerce website listing products. You don't just want the <em>description</em>; you need the exact <b>product name</b>, the specific <b>price</b>, the <b>customer rating</b>, and maybe the <b>SKU number</b>, all neatly organized. How do we tell Crawl4AI to find these <em>specific</em> pieces of information and return them in a structured format, like a JSON object?</p>
<h2><a class="anchor" id="autotoc_md1414"></a>
What Problem Does <code>ExtractionStrategy</code> Solve?</h2>
<p>Think of the content we've processed so far (like the cleaned HTML or the generated Markdown) as a detailed report delivered by a researcher. <code>RelevantContentFilter</code> helped trim the report down to the most relevant pages.</p>
<p>Now, we need to give specific instructions to an <b>Analyst</b> to go through that focused report and pull out precise data points. We don't just want the report; we want a filled-in spreadsheet with columns for "Product Name," "Price," and "Rating."</p>
<p><code>ExtractionStrategy</code> is the set of instructions we give to this Analyst. It defines <em>how</em> to locate and extract specific, structured information (like fields in a database or keys in a JSON object) from the content.</p>
<h2><a class="anchor" id="autotoc_md1415"></a>
What is <code>ExtractionStrategy</code>?</h2>
<p><code>ExtractionStrategy</code> is a core concept (a blueprint) in Crawl4AI that represents the <b>method used to extract structured data</b> from the processed content (which could be HTML or Markdown). It specifies <em>that</em> we need a way to find specific fields, but the actual <em>technique</em> used to find them can vary.</p>
<p>This allows us to choose the best "Analyst" for the job, depending on the complexity of the website and the data we need.</p>
<h2><a class="anchor" id="autotoc_md1416"></a>
The Different Analysts: Ways to Extract Data</h2>
<p>Crawl4AI offers several concrete implementations (the different Analysts) for extracting structured data:</p>
<ol type="1">
<li><b>The Precise Locator (<code>JsonCssExtractionStrategy</code> &amp; <code>JsonXPathExtractionStrategy</code>)</b><ul>
<li><b>Analogy:</b> An analyst who uses very precise map coordinates (CSS Selectors or XPath expressions) to find information on a page. They need to be told exactly where to look. "The price is always in the HTML element with the ID `#product-price`."</li>
<li><b>How it works:</b> You define a <b>schema</b> (a Python dictionary) that maps the names of the fields you want (e.g., "product_name", "price") to the specific CSS selector (<code>JsonCssExtractionStrategy</code>) or XPath expression (<code>JsonXPathExtractionStrategy</code>) that locates that information within the HTML structure.</li>
<li><b>Pros:</b> Very fast and reliable if the website structure is consistent and predictable. Doesn't require external AI services.</li>
<li><b>Cons:</b> Can break easily if the website changes its layout (selectors become invalid). Requires you to inspect the HTML and figure out the correct selectors.</li>
<li><b>Input:</b> Typically works directly on the raw or cleaned HTML.</li>
</ul>
</li>
<li><b>The Smart Interpreter (<code>LLMExtractionStrategy</code>)</b><ul>
<li><b>Analogy:</b> A highly intelligent analyst who can <em>read and understand</em> the content. You give them a list of fields you need (a schema) or even just natural language instructions ("Find the product name, its price, and a short description"). They read the content (usually Markdown) and use their understanding of language and context to figure out the values, even if the layout isn't perfectly consistent.</li>
<li><b>How it works:</b> You provide a desired output schema (e.g., a Pydantic model or a dictionary structure) or a natural language instruction. The strategy sends the content (often the generated Markdown, possibly split into chunks) along with your schema/instruction to a configured Large Language Model (LLM) like GPT or Llama. The LLM reads the text and generates the structured data (usually JSON) according to your request.</li>
<li><b>Pros:</b> Much more resilient to website layout changes. Can understand context and handle variations. Can extract data based on meaning, not just location.</li>
<li><b>Cons:</b> Requires setting up access to an LLM (API keys, potentially costs). Can be significantly slower than selector-based methods. The quality of extraction depends on the LLM's capabilities and the clarity of your instructions/schema.</li>
<li><b>Input:</b> Often works best on the cleaned Markdown representation of the content, but can sometimes use HTML.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" id="autotoc_md1417"></a>
How to Use an <code>ExtractionStrategy</code></h2>
<p>You tell the <code>AsyncWebCrawler</code> which extraction strategy to use (if any) by setting the <code>extraction_strategy</code> parameter within the <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">CrawlerRunConfig</a> object you pass to <code>arun</code> or <code>arun_many</code>.</p>
<h3><a class="anchor" id="autotoc_md1418"></a>
Example 1: Extracting Data with <code>JsonCssExtractionStrategy</code></h3>
<p>Let's imagine we want to extract the title (from the <code>&lt;h1&gt;</code> tag) and the main heading (from the <code>&lt;h1&gt;</code> tag) of the simple <code>httpbin.org/html</code> page.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter6_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">import</span> json</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> (</div>
<div class="line">    AsyncWebCrawler,</div>
<div class="line">    CrawlerRunConfig,</div>
<div class="line">    JsonCssExtractionStrategy <span class="comment"># Import the CSS strategy</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># 1. Define the extraction schema (Field Name -&gt; CSS Selector)</span></div>
<div class="line">    extraction_schema = {</div>
<div class="line">        <span class="stringliteral">&quot;baseSelector&quot;</span>: <span class="stringliteral">&quot;body&quot;</span>, <span class="comment"># Operate within the body tag</span></div>
<div class="line">        <span class="stringliteral">&quot;fields&quot;</span>: [</div>
<div class="line">            {<span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;page_title&quot;</span>, <span class="stringliteral">&quot;selector&quot;</span>: <span class="stringliteral">&quot;title&quot;</span>, <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;text&quot;</span>},</div>
<div class="line">            {<span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;main_heading&quot;</span>, <span class="stringliteral">&quot;selector&quot;</span>: <span class="stringliteral">&quot;h1&quot;</span>, <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;text&quot;</span>}</div>
<div class="line">        ]</div>
<div class="line">    }</div>
<div class="line">    print(<span class="stringliteral">&quot;Extraction Schema defined using CSS selectors.&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 2. Create an instance of the strategy with the schema</span></div>
<div class="line">    css_extractor = JsonCssExtractionStrategy(schema=extraction_schema)</div>
<div class="line">    print(f<span class="stringliteral">&quot;Using strategy: {css_extractor.__class__.__name__}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 3. Create CrawlerRunConfig and set the extraction_strategy</span></div>
<div class="line">    run_config = CrawlerRunConfig(</div>
<div class="line">        extraction_strategy=css_extractor</div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 4. Run the crawl</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;\nCrawling {url_to_crawl} to extract structured data...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        result = await crawler.arun(url=url_to_crawl, config=run_config)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success <span class="keywordflow">and</span> result.extracted_content:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nExtraction successful!&quot;</span>)</div>
<div class="line">            <span class="comment"># The extracted data is stored as a JSON string in result.extracted_content</span></div>
<div class="line">            <span class="comment"># Parse the JSON string to work with the data as a Python object</span></div>
<div class="line">            extracted_data = json.loads(result.extracted_content)</div>
<div class="line">            print(<span class="stringliteral">&quot;Extracted Data:&quot;</span>)</div>
<div class="line">            <span class="comment"># Print the extracted data nicely formatted</span></div>
<div class="line">            print(json.dumps(extracted_data, indent=2))</div>
<div class="line">        <span class="keywordflow">elif</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nCrawl successful, but no structured data extracted.&quot;</span>)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nCrawl failed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Schema Definition:</b> We create a Python dictionary <code>extraction_schema</code>.<ul>
<li><code>baseSelector: "body"</code> tells the strategy to look for items within the <code>&lt;body&gt;</code> tag of the HTML.</li>
<li><code>fields</code> is a list of dictionaries, each defining a field to extract:<ul>
<li><code>name</code>: The key for this field in the output JSON (e.g., "page_title").</li>
<li><code>selector</code>: The CSS selector to find the element containing the data (e.g., "title" finds the <code>&lt;title&gt;</code> tag, "h1" finds the <code>&lt;h1&gt;</code> tag).</li>
<li><code>type</code>: How to get the data from the selected element (<code>"text"</code> means get the text content).</li>
</ul>
</li>
</ul>
</li>
<li><b>Instantiate Strategy:</b> We create an instance of <code>JsonCssExtractionStrategy</code>, passing our <code>extraction_schema</code>. This strategy knows its input format should be HTML.</li>
<li><b>Configure Run:</b> We create a <code>CrawlerRunConfig</code> and assign our <code>css_extractor</code> instance to the <code>extraction_strategy</code> parameter.</li>
<li><b>Crawl:</b> We run <code>crawler.arun</code>. After fetching and basic scraping, the <code>AsyncWebCrawler</code> will see the <code>extraction_strategy</code> in the config and call our <code>css_extractor</code>.</li>
<li><b>Result:</b> The <code>CrawlResult</code> object now contains a field called <code>extracted_content</code>. This field holds the structured data found by the strategy, formatted as a <b>JSON string</b>. We use <code>json.loads()</code> to convert this string back into a Python list/dictionary.</li>
</ol>
<p><b>Expected Output (Conceptual):</b></p>
<div class="fragment"><div class="line">Extraction Schema defined using CSS selectors.</div>
<div class="line">Using strategy: JsonCssExtractionStrategy</div>
<div class="line"> </div>
<div class="line">Crawling https://httpbin.org/html to extract structured data...</div>
<div class="line"> </div>
<div class="line">Extraction successful!</div>
<div class="line">Extracted Data:</div>
<div class="line">[</div>
<div class="line">  {</div>
<div class="line">    &quot;page_title&quot;: &quot;Herman Melville - Moby-Dick&quot;,</div>
<div class="line">    &quot;main_heading&quot;: &quot;Moby Dick&quot;</div>
<div class="line">  }</div>
<div class="line">]</div>
</div><!-- fragment --><p> <em>(Note: The actual output is a list containing one dictionary because <code>baseSelector: "body"</code> matches one element, and we extract fields relative to that.)</em></p>
<h3><a class="anchor" id="autotoc_md1419"></a>
Example 2: Extracting Data with <code>LLMExtractionStrategy</code> (Conceptual)</h3>
<p>Now, let's imagine we want the same information (title, heading) but using an AI. We'll provide a schema describing what we want. (Note: This requires setting up LLM access separately, e.g., API keys).</p>
<div class="fragment"><div class="line"><span class="comment"># chapter6_example_2.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">import</span> json</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> (</div>
<div class="line">    AsyncWebCrawler,</div>
<div class="line">    CrawlerRunConfig,</div>
<div class="line">    LLMExtractionStrategy, <span class="comment"># Import the LLM strategy</span></div>
<div class="line">    LlmConfig             <span class="comment"># Import LLM configuration helper</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Assume llm_config is properly configured with provider, API key, etc.</span></div>
<div class="line"><span class="comment"># This is just a placeholder - replace with your actual LLM setup</span></div>
<div class="line"><span class="comment"># E.g., llm_config = LlmConfig(provider=&quot;openai&quot;, api_token=&quot;env:OPENAI_API_KEY&quot;)</span></div>
<div class="line"><span class="keyword">class </span>MockLlmConfig: provider=<span class="stringliteral">&quot;mock&quot;</span>; api_token=<span class="stringliteral">&quot;mock&quot;</span>; base_url=<span class="keywordtype">None</span></div>
<div class="line">llm_config = MockLlmConfig()</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># 1. Define the desired output schema (what fields we want)</span></div>
<div class="line">    <span class="comment">#    This helps guide the LLM.</span></div>
<div class="line">    output_schema = {</div>
<div class="line">        <span class="stringliteral">&quot;page_title&quot;</span>: <span class="stringliteral">&quot;string&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;main_heading&quot;</span>: <span class="stringliteral">&quot;string&quot;</span></div>
<div class="line">    }</div>
<div class="line">    print(<span class="stringliteral">&quot;Extraction Schema defined for LLM.&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 2. Create an instance of the LLM strategy</span></div>
<div class="line">    <span class="comment">#    We pass the schema and the LLM configuration.</span></div>
<div class="line">    <span class="comment">#    We also specify input_format=&#39;markdown&#39; (common for LLMs).</span></div>
<div class="line">    llm_extractor = LLMExtractionStrategy(</div>
<div class="line">        schema=output_schema,</div>
<div class="line">        llmConfig=llm_config, <span class="comment"># Pass the LLM provider details</span></div>
<div class="line">        input_format=<span class="stringliteral">&quot;markdown&quot;</span> <span class="comment"># Tell it to read the Markdown content</span></div>
<div class="line">    )</div>
<div class="line">    print(f<span class="stringliteral">&quot;Using strategy: {llm_extractor.__class__.__name__}&quot;</span>)</div>
<div class="line">    print(f<span class="stringliteral">&quot;LLM Provider (mocked): {llm_config.provider}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 3. Create CrawlerRunConfig with the strategy</span></div>
<div class="line">    run_config = CrawlerRunConfig(</div>
<div class="line">        extraction_strategy=llm_extractor</div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 4. Run the crawl</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;\nCrawling {url_to_crawl} using LLM to extract...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># This would make calls to the configured LLM API</span></div>
<div class="line">        result = await crawler.arun(url=url_to_crawl, config=run_config)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success <span class="keywordflow">and</span> result.extracted_content:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nExtraction successful (using LLM)!&quot;</span>)</div>
<div class="line">            <span class="comment"># Extracted data is a JSON string</span></div>
<div class="line">            <span class="keywordflow">try</span>:</div>
<div class="line">                extracted_data = json.loads(result.extracted_content)</div>
<div class="line">                print(<span class="stringliteral">&quot;Extracted Data:&quot;</span>)</div>
<div class="line">                print(json.dumps(extracted_data, indent=2))</div>
<div class="line">            <span class="keywordflow">except</span> json.JSONDecodeError:</div>
<div class="line">                print(<span class="stringliteral">&quot;Could not parse LLM output as JSON:&quot;</span>)</div>
<div class="line">                print(result.extracted_content)</div>
<div class="line">        <span class="keywordflow">elif</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nCrawl successful, but no structured data extracted by LLM.&quot;</span>)</div>
<div class="line">            <span class="comment"># This might happen if the mock LLM doesn&#39;t return valid JSON</span></div>
<div class="line">            <span class="comment"># or if the content was too small/irrelevant for extraction.</span></div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nCrawl failed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Schema Definition:</b> We define a simple dictionary <code>output_schema</code> telling the LLM we want fields named "page_title" and "main_heading", both expected to be strings.</li>
<li><b>Instantiate Strategy:</b> We create <code>LLMExtractionStrategy</code>, passing:<ul>
<li><code>schema=output_schema</code>: Our desired output structure.</li>
<li><code>llmConfig=llm_config</code>: The configuration telling the strategy <em>which</em> LLM to use and how to authenticate (here, it's mocked).</li>
<li><code>input_format="markdown"</code>: Instructs the strategy to feed the generated Markdown content (from <code>result.markdown.raw_markdown</code>) to the LLM, which is often easier for LLMs to parse than raw HTML.</li>
</ul>
</li>
<li><b>Configure Run &amp; Crawl:</b> Same as before, we set the <code>extraction_strategy</code> in <code>CrawlerRunConfig</code> and run the crawl.</li>
<li><b>Result:</b> The <code>AsyncWebCrawler</code> calls the <code>llm_extractor</code>. The strategy sends the Markdown content and the schema instructions to the configured LLM. The LLM analyzes the text and (hopefully) returns a JSON object matching the schema. This JSON is stored as a string in <code>result.extracted_content</code>.</li>
</ol>
<p><b>Expected Output (Conceptual, with a real LLM):</b></p>
<div class="fragment"><div class="line">Extraction Schema defined for LLM.</div>
<div class="line">Using strategy: LLMExtractionStrategy</div>
<div class="line">LLM Provider (mocked): mock</div>
<div class="line"> </div>
<div class="line">Crawling https://httpbin.org/html using LLM to extract...</div>
<div class="line"> </div>
<div class="line">Extraction successful (using LLM)!</div>
<div class="line">Extracted Data:</div>
<div class="line">[</div>
<div class="line">  {</div>
<div class="line">    &quot;page_title&quot;: &quot;Herman Melville - Moby-Dick&quot;,</div>
<div class="line">    &quot;main_heading&quot;: &quot;Moby Dick&quot;</div>
<div class="line">  }</div>
<div class="line">]</div>
</div><!-- fragment --><p> <em>(Note: LLM output format might vary slightly, but it aims to match the requested schema based on the content it reads.)</em></p>
<h2><a class="anchor" id="autotoc_md1420"></a>
How It Works Inside (Under the Hood)</h2>
<p>When you provide an <code>extraction_strategy</code> in the <code>CrawlerRunConfig</code>, how does <code>AsyncWebCrawler</code> use it?</p>
<ol type="1">
<li><b>Fetch &amp; Scrape:</b> The crawler fetches the raw HTML (<a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a>) and performs initial cleaning/scraping (<a class="el" href="../../df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html">ContentScrapingStrategy</a>) to get <code>cleaned_html</code>, links, etc.</li>
<li><b>Markdown Generation:</b> It usually generates Markdown representation (<a href="../../05_relevantcontentfilter.md#how-relevantcontentfilter-is-used-via-markdown-generation">DefaultMarkdownGenerator</a>).</li>
<li><b>Check for Strategy:</b> The <code>AsyncWebCrawler</code> (specifically in its internal <code>aprocess_html</code> method) checks if <code>config.extraction_strategy</code> is set.</li>
<li><b>Execute Strategy:</b> If a strategy exists:<ul>
<li>It determines the required input format (e.g., "html" for <code>JsonCssExtractionStrategy</code>, "markdown" for <code>LLMExtractionStrategy</code> based on its <code>input_format</code> attribute).</li>
<li>It retrieves the corresponding content (e.g., <code>result.cleaned_html</code> or <code>result.markdown.raw_markdown</code>).</li>
<li>If the content is long and the strategy supports chunking (like <code>LLMExtractionStrategy</code>), it might first split the content into smaller chunks.</li>
<li>It calls the strategy's <code>run</code> method, passing the content chunk(s).</li>
<li>The strategy performs its logic (applying selectors, calling LLM API).</li>
<li>The strategy returns the extracted data (typically as a list of dictionaries).</li>
</ul>
</li>
<li><b>Store Result:</b> The <code>AsyncWebCrawler</code> converts the returned structured data into a JSON string and stores it in <code>CrawlResult.extracted_content</code>.</li>
</ol>
<p>Here's a simplified view:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant AWC as AsyncWebCrawler</div>
<div class="line">    participant Config as CrawlerRunConfig</div>
<div class="line">    participant Processor as HTML Processing</div>
<div class="line">    participant Extractor as ExtractionStrategy</div>
<div class="line">    participant Result as CrawlResult</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;AWC: arun(url, config=my_config)</div>
<div class="line">    Note over AWC: Config includes an Extraction Strategy</div>
<div class="line">    AWC-&gt;&gt;Processor: Process HTML (scrape, generate markdown)</div>
<div class="line">    Processor--&gt;&gt;AWC: Processed Content (HTML, Markdown)</div>
<div class="line">    AWC-&gt;&gt;Extractor: Run extraction on content (using Strategy&#39;s input format)</div>
<div class="line">    Note over Extractor: Applying logic (CSS, XPath, LLM...)</div>
<div class="line">    Extractor--&gt;&gt;AWC: Structured Data (List[Dict])</div>
<div class="line">    AWC-&gt;&gt;AWC: Convert data to JSON String</div>
<div class="line">    AWC-&gt;&gt;Result: Store JSON String in extracted_content</div>
<div class="line">    AWC--&gt;&gt;User: Return CrawlResult</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md1421"></a>
Code Glimpse (<code>extraction_strategy.py</code>)</h3>
<p>Inside the <code>crawl4ai</code> library, the file <code>extraction_strategy.py</code> defines the blueprint and the implementations.</p>
<p><b>The Blueprint (Abstract Base Class):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/extraction_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</div>
<div class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Dict, Any</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExtractionStrategy(ABC):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Abstract base class for all extraction strategies.&quot;&quot;&quot;</span></div>
<div class="line">    <span class="keyword">def </span>__init__(self, input_format: str = <span class="stringliteral">&quot;markdown&quot;</span>, **kwargs):</div>
<div class="line">        self.input_format = input_format <span class="comment"># e.g., &#39;html&#39;, &#39;markdown&#39;</span></div>
<div class="line">        <span class="comment"># ... other common init ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">def </span>extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;Extract structured data from a single chunk of content.&quot;&quot;&quot;</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;Process content sections (potentially chunked) and call extract.&quot;&quot;&quot;</span></div>
<div class="line">        <span class="comment"># Default implementation might process sections in parallel or sequentially</span></div>
<div class="line">        all_extracted_data = []</div>
<div class="line">        <span class="keywordflow">for</span> section <span class="keywordflow">in</span> sections:</div>
<div class="line">             all_extracted_data.extend(self.extract(url, section, **kwargs))</div>
<div class="line">        <span class="keywordflow">return</span> all_extracted_data</div>
</div><!-- fragment --><p><b>Example Implementation (<code>JsonCssExtractionStrategy</code>):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/extraction_strategy.py</span></div>
<div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="comment"># Uses BeautifulSoup for CSS selectors</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>JsonCssExtractionStrategy(ExtractionStrategy):</div>
<div class="line">    <span class="keyword">def </span>__init__(self, schema: Dict[str, Any], **kwargs):</div>
<div class="line">        <span class="comment"># Force input format to HTML for CSS selectors</span></div>
<div class="line">        super().__init__(input_format=<span class="stringliteral">&quot;html&quot;</span>, **kwargs)</div>
<div class="line">        self.schema = schema <span class="comment"># Store the user-defined schema</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>extract(self, url: str, html_content: str, *q, **kwargs) -&gt; List[Dict[str, Any]]:</div>
<div class="line">        <span class="comment"># Parse the HTML content chunk</span></div>
<div class="line">        soup = BeautifulSoup(html_content, <span class="stringliteral">&quot;html.parser&quot;</span>)</div>
<div class="line">        extracted_items = []</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Find base elements defined in the schema</span></div>
<div class="line">        base_elements = soup.select(self.schema.<a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#abe6524afb3a69dc9a4c314e11f96f29f">get</a>(<span class="stringliteral">&quot;baseSelector&quot;</span>, <span class="stringliteral">&quot;body&quot;</span>))</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">for</span> element <span class="keywordflow">in</span> base_elements:</div>
<div class="line">            item = {}</div>
<div class="line">            <span class="comment"># Extract fields based on schema selectors and types</span></div>
<div class="line">            fields_to_extract = self.schema.<a class="code hl_function" href="../../d7/da6/pybind__kv__service_8cpp.html#abe6524afb3a69dc9a4c314e11f96f29f">get</a>(<span class="stringliteral">&quot;fields&quot;</span>, [])</div>
<div class="line">            <span class="keywordflow">for</span> field_def <span class="keywordflow">in</span> fields_to_extract:</div>
<div class="line">                <span class="keywordflow">try</span>:</div>
<div class="line">                    <span class="comment"># Find the specific sub-element using CSS selector</span></div>
<div class="line">                    target_element = element.select_one(field_def[<span class="stringliteral">&quot;selector&quot;</span>])</div>
<div class="line">                    <span class="keywordflow">if</span> target_element:</div>
<div class="line">                        <span class="keywordflow">if</span> field_def[<span class="stringliteral">&quot;type&quot;</span>] == <span class="stringliteral">&quot;text&quot;</span>:</div>
<div class="line">                            item[field_def[<span class="stringliteral">&quot;name&quot;</span>]] = target_element.get_text(strip=<span class="keyword">True</span>)</div>
<div class="line">                        <span class="keywordflow">elif</span> field_def[<span class="stringliteral">&quot;type&quot;</span>] == <span class="stringliteral">&quot;attribute&quot;</span>:</div>
<div class="line">                            item[field_def[<span class="stringliteral">&quot;name&quot;</span>]] = target_element.get(field_def[<span class="stringliteral">&quot;attribute&quot;</span>])</div>
<div class="line">                        <span class="comment"># ... other types like &#39;html&#39;, &#39;list&#39;, &#39;nested&#39; ...</span></div>
<div class="line">                <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">                    <span class="comment"># Handle errors, maybe log them if verbose</span></div>
<div class="line">                    <span class="keywordflow">pass</span></div>
<div class="line">            <span class="keywordflow">if</span> item:</div>
<div class="line">                extracted_items.append(item)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">return</span> extracted_items</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># run() method likely uses the default implementation from base class</span></div>
<div class="ttc" id="apybind__kv__service_8cpp_html_abe6524afb3a69dc9a4c314e11f96f29f"><div class="ttname"><a href="../../d7/da6/pybind__kv__service_8cpp.html#abe6524afb3a69dc9a4c314e11f96f29f">get</a></div><div class="ttdeci">std::string get(std::string key, std::string config_path)</div><div class="ttdef"><b>Definition</b> <a href="../../d7/da6/pybind__kv__service_8cpp_source.html#l00039">pybind_kv_service.cpp:39</a></div></div>
</div><!-- fragment --><p><b>Example Implementation (<code>LLMExtractionStrategy</code>):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/extraction_strategy.py</span></div>
<div class="line"><span class="comment"># Needs imports for LLM interaction (e.g., perform_completion_with_backoff)</span></div>
<div class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> perform_completion_with_backoff, chunk_documents, escape_json_string</div>
<div class="line"><span class="keyword">from</span> .prompts <span class="keyword">import</span> PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION <span class="comment"># Example prompt</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>LLMExtractionStrategy(ExtractionStrategy):</div>
<div class="line">    <span class="keyword">def </span>__init__(self, schema: Dict = <span class="keywordtype">None</span>, instruction: str = <span class="keywordtype">None</span>, llmConfig=<span class="keywordtype">None</span>, input_format=<span class="stringliteral">&quot;markdown&quot;</span>, **kwargs):</div>
<div class="line">        super().__init__(input_format=input_format, **kwargs)</div>
<div class="line">        self.schema = schema</div>
<div class="line">        self.instruction = instruction</div>
<div class="line">        self.llmConfig = llmConfig <span class="comment"># Contains provider, API key, etc.</span></div>
<div class="line">        <span class="comment"># ... other LLM specific setup ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]:</div>
<div class="line">        <span class="comment"># Prepare the prompt for the LLM</span></div>
<div class="line">        prompt = self._build_llm_prompt(url, content_chunk)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Call the LLM API</span></div>
<div class="line">        response = perform_completion_with_backoff(</div>
<div class="line">            provider=self.llmConfig.provider,</div>
<div class="line">            prompt_with_variables=prompt,</div>
<div class="line">            api_token=self.llmConfig.api_token,</div>
<div class="line">            base_url=self.llmConfig.base_url,</div>
<div class="line">            json_response=<span class="keyword">True</span> <span class="comment"># Often expect JSON from LLM for extraction</span></div>
<div class="line">            <span class="comment"># ... pass other necessary args ...</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Parse the LLM&#39;s response (which should ideally be JSON)</span></div>
<div class="line">        <span class="keywordflow">try</span>:</div>
<div class="line">            extracted_data = json.loads(response.choices[0].message.content)</div>
<div class="line">            <span class="comment"># Ensure it&#39;s a list</span></div>
<div class="line">            <span class="keywordflow">if</span> isinstance(extracted_data, dict):</div>
<div class="line">                extracted_data = [extracted_data]</div>
<div class="line">            <span class="keywordflow">return</span> extracted_data</div>
<div class="line">        <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">            <span class="comment"># Handle LLM response parsing errors</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Error parsing LLM response: {e}&quot;</span>)</div>
<div class="line">            <span class="keywordflow">return</span> [{<span class="stringliteral">&quot;error&quot;</span>: <span class="stringliteral">&quot;Failed to parse LLM output&quot;</span>, <span class="stringliteral">&quot;raw_output&quot;</span>: response.choices[0].message.content}]</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>_build_llm_prompt(self, url: str, content_chunk: str) -&gt; str:</div>
<div class="line">        <span class="comment"># Logic to construct the prompt using self.schema or self.instruction</span></div>
<div class="line">        <span class="comment"># and the content_chunk. Example:</span></div>
<div class="line">        prompt_template = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION <span class="comment"># Choose appropriate prompt</span></div>
<div class="line">        variable_values = {</div>
<div class="line">            <span class="stringliteral">&quot;URL&quot;</span>: url,</div>
<div class="line">            <span class="stringliteral">&quot;CONTENT&quot;</span>: escape_json_string(content_chunk), <span class="comment"># Send Markdown or HTML chunk</span></div>
<div class="line">            <span class="stringliteral">&quot;SCHEMA&quot;</span>: json.dumps(self.schema) <span class="keywordflow">if</span> self.schema <span class="keywordflow">else</span> <span class="stringliteral">&quot;{}&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;REQUEST&quot;</span>: self.instruction <span class="keywordflow">if</span> self.instruction <span class="keywordflow">else</span> <span class="stringliteral">&quot;Extract relevant data based on the schema.&quot;</span></div>
<div class="line">        }</div>
<div class="line">        prompt = prompt_template</div>
<div class="line">        <span class="keywordflow">for</span> var, val <span class="keywordflow">in</span> variable_values.items():</div>
<div class="line">            prompt = prompt.replace(<span class="stringliteral">&quot;{&quot;</span> + var + <span class="stringliteral">&quot;}&quot;</span>, str(val))</div>
<div class="line">        <span class="keywordflow">return</span> prompt</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># run() method might override the base to handle chunking specifically for LLMs</span></div>
<div class="line">    <span class="keyword">def </span>run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]:</div>
<div class="line">        <span class="comment"># Potentially chunk sections based on token limits before calling extract</span></div>
<div class="line">        <span class="comment"># chunked_content = chunk_documents(sections, ...)</span></div>
<div class="line">        <span class="comment"># extracted_data = []</span></div>
<div class="line">        <span class="comment"># for chunk in chunked_content:</span></div>
<div class="line">        <span class="comment">#    extracted_data.extend(self.extract(url, chunk, **kwargs))</span></div>
<div class="line">        <span class="comment"># return extracted_data</span></div>
<div class="line">        <span class="comment"># Simplified for now:</span></div>
<div class="line">        <span class="keywordflow">return</span> super().run(url, sections, *q, **kwargs)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md1422"></a>
Conclusion</h2>
<p>You've learned about <code>ExtractionStrategy</code>, Crawl4AI's way of giving instructions to an "Analyst" to pull out specific, structured data from web content.</p>
<ul>
<li>It solves the problem of needing precise data points (like product names, prices) in an organized format, not just blocks of text.</li>
<li>You can choose your "Analyst":<ul>
<li><b>Precise Locators (<code>JsonCssExtractionStrategy</code>, <code>JsonXPathExtractionStrategy</code>):</b> Use exact CSS/XPath selectors defined in a schema. Fast but brittle.</li>
<li><b>Smart Interpreter (<code>LLMExtractionStrategy</code>):</b> Uses an AI (LLM) guided by a schema or instructions. More flexible but slower and needs setup.</li>
</ul>
</li>
<li>You configure the desired strategy within the <a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">CrawlerRunConfig</a>.</li>
<li>The extracted structured data is returned as a JSON string in the <code>CrawlResult.extracted_content</code> field.</li>
</ul>
<p>Now that we understand how to fetch, clean, filter, and extract data, let's put it all together and look at the final package that Crawl4AI delivers after a crawl.</p>
<p><b>Next:</b> Let's dive into the details of the output with <a class="el" href="../../dd/d0d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_207__crawlresult.html">Chapter 7: Understanding the Results - CrawlResult</a>.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
