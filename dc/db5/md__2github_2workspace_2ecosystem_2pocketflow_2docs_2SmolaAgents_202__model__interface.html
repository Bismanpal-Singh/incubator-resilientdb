#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 02_model_interface</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('dc/db5/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_202__model__interface.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">02_model_interface</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md3029"></a>
autotoc_md3029</h2>
<p>layout: default title: "Model Interface" parent: "SmolaAgents" </p>
<h2><a class="anchor" id="autotoc_md3030"></a>
nav_order: 2</h2>
<h1><a class="anchor" id="autotoc_md3031"></a>
Chapter 2: Model Interface - Your Agent's Universal Translator</h1>
<p>Welcome back! In <a class="el" href="../../dc/d92/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_201__multistepagent.html">Chapter 1: The MultiStepAgent - Your Task Orchestrator</a>, we met the <code>MultiStepAgent</code>, our AI project manager. We learned that it follows a "Think -&gt; Act -&gt; Observe" cycle to solve tasks. A crucial part of the "Think" phase is consulting its "brain" – a Large Language Model (LLM).</p>
<p>But wait... there are so many different LLMs out there! OpenAI's GPT-4, Anthropic's Claude, Google's Gemini, open-source models you can run locally like Llama or Mistral... How can our agent talk to all of them without needing completely different code for each one?</p>
<p>This is where the <b>Model Interface</b> comes in!</p>
<h2><a class="anchor" id="autotoc_md3032"></a>
The Problem: Too Many Remotes!</h2>
<p>Imagine you have several TVs at home, each from a different brand (Sony, Samsung, LG). Each TV comes with its own specific remote control. To watch TV, you need to find the <em>right</em> remote and know <em>its specific buttons</em>. It's a hassle!</p>
<p><img src="https://img.icons8.com/cotton/64/000000/remote-control.png" alt="Different TV Remotes" class="inline"/> <img src="https://img.icons8.com/fluency/48/000000/remote-control.png" alt="Different TV Remotes" class="inline"/> <img src="https://img.icons8.com/color/48/000000/remote-control.png" alt="Different TV Remotes" class="inline"/></p>
<p>Different LLMs are like those different TVs. Each has its own way of being "controlled" – its own API (Application Programming Interface) or library with specific functions, required inputs, and ways of giving back answers. If our <code>MultiStepAgent</code> had to learn the specific "remote control commands" for every possible LLM, our code would become very complicated very quickly!</p>
<h2><a class="anchor" id="autotoc_md3033"></a>
The Solution: The Universal Remote (Model Interface)</h2>
<p>Wouldn't it be great if you had <em>one</em> universal remote that could control <em>all</em> your TVs? You'd just press "Power", "Volume Up", or "Channel Down", and the universal remote would figure out how to send the correct signal to whichever TV you're using.</p>
<p><img src="https://img.icons8.com/office/80/000000/remote-control.png" alt="Universal Remote" class="inline"/> -&gt; Controls -&gt; <img src="https://img.icons8.com/color/48/000000/tv.png" alt="Sony TV" class="inline"/> <img src="https://img.icons8.com/color/48/000000/tv-on.png" alt="Samsung TV" class="inline"/> <img src="https://img.icons8.com/emoji/48/000000/television.png" alt="LG TV" class="inline"/></p>
<p>The <b>Model Interface</b> in <code>SmolaAgents</code> is exactly like that universal remote.</p>
<ul>
<li>It's an <b>abstraction layer</b>: a way to hide the complicated details.</li>
<li>It provides a <b>consistent way</b> for the <code>MultiStepAgent</code> to talk to <em>any</em> supported LLM.</li>
<li>It handles the "translation" behind the scenes:<ul>
<li>Taking the agent's request (like "What should I do next?").</li>
<li>Formatting it correctly for the specific LLM being used.</li>
<li>Sending the request (making the API call or running the local model).</li>
<li>Receiving the LLM's raw response.</li>
<li>Parsing that response back into a standard format the agent understands (including things like requests to use <a class="el" href="../../d4/d89/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_203__tool.html">Tools</a>).</li>
</ul>
</li>
</ul>
<p>So, the <code>MultiStepAgent</code> only needs to learn how to use the <em>one</em> universal remote (the Model Interface), not the specific commands for every LLM "TV".</p>
<h2><a class="anchor" id="autotoc_md3034"></a>
How It Works: The Standard <code>__call__</code></h2>
<p>The magic of the Model Interface lies in its simplicity from the agent's perspective. All Model Interfaces in <code>SmolaAgents</code> work the same way: you "call" them like a function, passing in the conversation history.</p>
<p>Think of it like pressing the main button on our universal remote.</p>
<ol type="1">
<li><b>Input:</b> The agent gives the Model Interface a list of messages representing the conversation so far. This usually includes the system prompt (instructions for the LLM), the user's task, and any previous "Think -&gt; Act -&gt; Observe" steps stored in <a class="el" href="../../d1/d59/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_204__agentmemory.html">AgentMemory</a>. Each message typically has a <code>role</code> (like <code>user</code>, <code>assistant</code>, or <code>system</code>) and <code>content</code>.</li>
<li><b>Processing (Behind the Scenes):</b> The <em>specific</em> Model Interface (e.g., one for OpenAI, one for local models) takes this standard list of messages and:<ul>
<li>Connects to the correct LLM (using API keys, loading a local model, etc.).</li>
<li>Formats the messages exactly how that LLM expects them.</li>
<li>Sends the request.</li>
<li>Waits for the LLM to generate a response.</li>
<li>Gets the response back.</li>
</ul>
</li>
<li><b>Output:</b> It translates the LLM's raw response back into a standard <code>ChatMessage</code> object. This object contains the LLM's text response and, importantly, might include structured information if the LLM decided the agent should use a <a class="el" href="../../d4/d89/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_203__tool.html">Tool</a>. The agent knows exactly how to read this <code>ChatMessage</code>.</li>
</ol>
<h2><a class="anchor" id="autotoc_md3035"></a>
Using a Model Interface</h2>
<p>Let's see how you'd actually <em>use</em> one. <code>SmolaAgents</code> comes with several built-in Model Interfaces. A very useful one is <code>LiteLLMModel</code>, which uses the <code>litellm</code> library to connect to hundreds of different LLM providers (OpenAI, Anthropic, Cohere, Azure, local models via Ollama, etc.) with minimal code changes!</p>
<p><b>Step 1: Choose and Initialize Your Model Interface</b></p>
<p>First, you decide which LLM you want your agent to use. Then, you create an instance of the corresponding Model Interface.</p>
<div class="fragment"><div class="line"><span class="comment"># --- File: choose_model.py ---</span></div>
<div class="line"><span class="comment"># Import the model interface you want to use</span></div>
<div class="line"><span class="keyword">from</span> smolagents.models <span class="keyword">import</span> LiteLLMModel</div>
<div class="line"><span class="comment"># (You might need to install litellm first: pip install smolagents[litellm])</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Choose the specific LLM model ID that litellm supports</span></div>
<div class="line"><span class="comment"># Example: OpenAI&#39;s GPT-3.5 Turbo</span></div>
<div class="line"><span class="comment"># Requires setting the OPENAI_API_KEY environment variable!</span></div>
<div class="line">model_id = <span class="stringliteral">&quot;gpt-3.5-turbo&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create an instance of the Model Interface</span></div>
<div class="line"><span class="comment"># This object is our &quot;universal remote&quot; configured for GPT-3.5</span></div>
<div class="line">llm = LiteLLMModel(model_id=model_id)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Model Interface created for: {model_id}&quot;</span>)</div>
<div class="line"><span class="comment"># Example Output: Model Interface created for: gpt-3.5-turbo</span></div>
</div><!-- fragment --><p><b>Explanation:</b></p><ul>
<li>We import <code>LiteLLMModel</code>.</li>
<li>We specify the <code>model_id</code> we want to use (here, <code>"gpt-3.5-turbo"</code>). <code>litellm</code> knows how to talk to this model if the necessary API key (<code>OPENAI_API_KEY</code>) is available in your environment.</li>
<li>We create the <code>llm</code> object. This object now knows how to communicate with GPT-3.5 Turbo via the <code>litellm</code> library, but it presents a standard interface to the rest of our code.</li>
</ul>
<p><b>Step 2: Give the Model to the Agent</b></p>
<p>Remember from Chapter 1 how we created the <code>MultiStepAgent</code>? We simply pass our <code>llm</code> object (the configured universal remote) to it.</p>
<div class="fragment"><div class="line"><span class="comment"># --- Continued from choose_model.py ---</span></div>
<div class="line"><span class="comment"># (Requires imports from Chapter 1: MultiStepAgent, SearchTool, etc.)</span></div>
<div class="line"><span class="keyword">from</span> smolagents <span class="keyword">import</span> MultiStepAgent</div>
<div class="line"><span class="keyword">from</span> smolagents.tools <span class="keyword">import</span> SearchTool <span class="comment"># Example Tool</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Define some tools (details in Chapter 3)</span></div>
<div class="line">search_tool = SearchTool()</div>
<div class="line">tools = [search_tool]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create the agent, giving it the model interface instance</span></div>
<div class="line">agent = MultiStepAgent(</div>
<div class="line">    model=llm,  <span class="comment"># &lt;= Here&#39;s where we plug in our &quot;universal remote&quot;!</span></div>
<div class="line">    tools=tools</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;MultiStepAgent created and configured with the model!&quot;</span>)</div>
<div class="line"><span class="comment"># Example Output: MultiStepAgent created and configured with the model!</span></div>
</div><!-- fragment --><p><b>Explanation:</b></p><ul>
<li>The <code>MultiStepAgent</code> doesn't need to know it's talking to GPT-3.5 Turbo specifically. It just knows it has a <code>model</code> object that it can call.</li>
</ul>
<p><b>Step 3: How the Agent Uses the Model (Simplified)</b></p>
<p>Inside its "Think" phase, the agent prepares the conversation history and calls the model:</p>
<div class="fragment"><div class="line"><span class="comment"># --- Simplified view of what happens inside the agent ---</span></div>
<div class="line"><span class="keyword">from</span> smolagents.models <span class="keyword">import</span> ChatMessage, MessageRole</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Agent prepares messages (example)</span></div>
<div class="line">messages_for_llm = [</div>
<div class="line">    {<span class="stringliteral">&quot;role&quot;</span>: MessageRole.SYSTEM, <span class="stringliteral">&quot;content&quot;</span>: <span class="stringliteral">&quot;You are a helpful agent. Decide the next step.&quot;</span>},</div>
<div class="line">    {<span class="stringliteral">&quot;role&quot;</span>: MessageRole.USER, <span class="stringliteral">&quot;content&quot;</span>: <span class="stringliteral">&quot;Task: What is the capital of France?&quot;</span>},</div>
<div class="line">    <span class="comment"># ... potentially previous steps ...</span></div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Agent calls the model using the standard interface</span></div>
<div class="line"><span class="comment"># This is like pressing the main button on the universal remote</span></div>
<div class="line">print(<span class="stringliteral">&quot;Agent asking model: What should I do next?&quot;</span>)</div>
<div class="line">response: ChatMessage = agent.model(messages_for_llm) <span class="comment"># agent.model refers to our &#39;llm&#39; instance</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Agent gets a standard response back</span></div>
<div class="line">print(f<span class="stringliteral">&quot;Model suggested action (simplified): {response.content}&quot;</span>)</div>
<div class="line"><span class="comment"># Example Output (will vary):</span></div>
<div class="line"><span class="comment"># Agent asking model: What should I do next?</span></div>
<div class="line"><span class="comment"># Model suggested action (simplified): Thought: I need to find the capital of France. I can use the search tool.</span></div>
<div class="line"><span class="comment"># Action:</span></div>
<div class="line"><span class="comment"># </span></div>
</div><!-- fragment --><p> json </p>
<h1><a class="anchor" id="autotoc_md3036"></a>
{</h1>
<h1><a class="anchor" id="autotoc_md3037"></a>
"action": "search",</h1>
<h1><a class="anchor" id="autotoc_md3038"></a>
"action_input": "Capital of France"</h1>
<h1><a class="anchor" id="autotoc_md3039"></a>
}</h1>
<h1><a class="anchor" id="autotoc_md3040"></a>
```</h1>
<div class="fragment"><div class="line">**Explanation:**</div>
<div class="line">*   The agent prepares a list of `messages_for_llm`.</div>
<div class="line">*   It simply calls `agent.model(...)` which executes `llm(messages_for_llm)`.</div>
<div class="line">*   The `LiteLLMModel` (`llm`) handles talking to the actual OpenAI API.</div>
<div class="line">*   The agent receives a `ChatMessage` object, which it knows how to parse to find the next action (like using the `search` tool, as suggested in the example output).</div>
<div class="line"> </div>
<div class="line">## Under the Hood: How the &quot;Universal Remote&quot; Works</div>
<div class="line"> </div>
<div class="line">Let&#39;s peek behind the curtain. What happens when the agent calls `model(messages)`?</div>
<div class="line"> </div>
<div class="line">**Conceptual Steps:**</div>
<div class="line"> </div>
<div class="line">1.  **Receive Request:** The specific Model Interface (e.g., `LiteLLMModel`) gets the standard list of messages from the agent.</div>
<div class="line">2.  **Prepare Backend Request:** It looks at its own configuration (e.g., `model_id=&quot;gpt-3.5-turbo&quot;`, API key) and translates the standard messages into the specific format the target LLM backend (e.g., the OpenAI API) requires. This might involve changing role names, structuring the data differently, etc.</div>
<div class="line">3.  **Send to Backend:** It makes the actual network call to the LLM&#39;s API endpoint or runs the command to invoke a local model.</div>
<div class="line">4.  **Receive Backend Response:** It gets the raw response back from the LLM (often as JSON or plain text).</div>
<div class="line">5.  **Parse Response:** It parses this raw response, extracting the generated text and any structured data (like tool calls).</div>
<div class="line">6.  **Return Standard Response:** It packages this information into a standard `ChatMessage` object and returns it to the agent.</div>
<div class="line"> </div>
<div class="line">**Diagram:**</div>
<div class="line"> </div>
<div class="line">Here&#39;s a simplified sequence diagram showing the flow:</div>
</div><!-- fragment --><p> mermaid sequenceDiagram participant Agent as MultiStepAgent participant ModelI as Model Interface (e.g., LiteLLMModel) participant Backend as Specific LLM API/Library (e.g., OpenAI)</p>
<p>Agent-&gt;&gt;ModelI: call(standard_messages) ModelI-&gt;&gt;ModelI: Translate messages to backend format ModelI-&gt;&gt;Backend: Send API Request (formatted messages, API key) Backend--&gt;&gt;ModelI: Receive API Response (raw JSON/text) ModelI-&gt;&gt;ModelI: Parse raw response into ChatMessage ModelI--&gt;&gt;Agent: Return ChatMessage object </p><div class="fragment"><div class="line">**Code Glimpse (Simplified):**</div>
<div class="line"> </div>
<div class="line">Let&#39;s look at `models.py` where these interfaces are defined.</div>
<div class="line"> </div>
<div class="line">*   **Base Class (`Model`):** Defines the common structure, including the `__call__` method that all specific interfaces must implement.</div>
</div><!-- fragment --><p> python </p>
<h1><a class="anchor" id="autotoc_md3041"></a>
&mdash; File: models.py (Simplified Model base class) &mdash;</h1>
<p>from typing import List, Dict, Optional from .tools import Tool # Reference to Tool concept</p>
<p>@dataclass class ChatMessage: # Simplified representation of the standard response role: str content: Optional[str] = None tool_calls: Optional[List[dict]] = None # For tool usage (Chapter 3) </p>
<h1><a class="anchor" id="autotoc_md3042"></a>
... other fields ...</h1>
<p>class Model: def <b>init</b>(self, **kwargs): self.kwargs = kwargs # Stores model-specific settings </p>
<h1><a class="anchor" id="autotoc_md3043"></a>
...</h1>
<h1><a class="anchor" id="autotoc_md3044"></a>
The standard "button" our agent presses!</h1>
<p>def <b>call</b>( self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, tools_to_call_from: Optional[List[Tool]] = None, **kwargs, ) -&gt; ChatMessage: </p>
<h1><a class="anchor" id="autotoc_md3045"></a>
Each specific model interface implements this method</h1>
<p>raise NotImplementedError("Subclasses must implement the __call__ method.")</p>
<p>def _prepare_completion_kwargs(self, messages, **kwargs) -&gt; Dict: </p>
<h1><a class="anchor" id="autotoc_md3046"></a>
Helper to format messages and parameters for the backend</h1>
<h1><a class="anchor" id="autotoc_md3047"></a>
... translation logic ...</h1>
<p>pass ```</p>
<ul>
<li><b>Specific Implementation (<code>LiteLLMModel</code>):</b> Inherits from <code>Model</code> and implements <code>__call__</code> using the <code>litellm</code> library. ```python </li>
</ul>
<h1><a class="anchor" id="autotoc_md3048"></a>
&mdash; File: models.py (Simplified LiteLLMModel <b>call</b>) &mdash;</h1>
<p>import litellm # The library that talks to many LLMs</p>
<p>class LiteLLMModel(Model): def <b>init</b>(self, model_id: str, **kwargs): super().__init__(**kwargs) self.model_id = model_id </p>
<h1><a class="anchor" id="autotoc_md3049"></a>
LiteLLM typically uses environment variables for API keys</h1>
<p>def <b>call</b>( self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, tools_to_call_from: Optional[List[Tool]] = None, **kwargs, ) -&gt; ChatMessage: </p>
<h1><a class="anchor" id="autotoc_md3050"></a>
1. Prepare arguments using the helper</h1>
<p>completion_kwargs = self._prepare_completion_kwargs( messages=messages, stop_sequences=stop_sequences, tools_to_call_from=tools_to_call_from, model=self.model_id, # Tell litellm which model </p>
<h1><a class="anchor" id="autotoc_md3051"></a>
... other parameters ...</h1>
<p>**kwargs, )</p>
<h1><a class="anchor" id="autotoc_md3052"></a>
2. Call the actual backend via litellm</h1>
<h1><a class="anchor" id="autotoc_md3053"></a>
This hides the complexity of different API calls!</h1>
<p>response = litellm.completion(**completion_kwargs)</p>
<h1><a class="anchor" id="autotoc_md3054"></a>
3. Parse the response into our standard ChatMessage</h1>
<h1><a class="anchor" id="autotoc_md3055"></a>
(Simplified - actual parsing involves more details)</h1>
<p>raw_message = response.choices[0].message chat_message = ChatMessage( role=raw_message.role, content=raw_message.content, tool_calls=raw_message.tool_calls # If the LLM requested a tool ) </p>
<h1><a class="anchor" id="autotoc_md3056"></a>
... store token counts, raw response etc. ...</h1>
<p>return chat_message ```</p>
<p><b>Explanation:</b></p><ul>
<li>The <code>Model</code> class defines the contract (the <code>__call__</code> method).</li>
<li><code>LiteLLMModel</code> fulfills this contract. Its <code>__call__</code> method uses <code>_prepare_completion_kwargs</code> to format the request suitable for <code>litellm</code>.</li>
<li>The core work happens in <code>litellm.completion(...)</code>, which connects to the actual LLM service (like OpenAI).</li>
<li>The result is then parsed back into the standard <code>ChatMessage</code> format.</li>
</ul>
<p>The beauty is that the <code>MultiStepAgent</code> only ever interacts with the <code>__call__</code> method, regardless of whether it's using <code>LiteLLMModel</code>, <code>TransformersModel</code> (for local models), or another interface.</p>
<h2><a class="anchor" id="autotoc_md3057"></a>
Conclusion</h2>
<p>The Model Interface is a vital piece of the <code>SmolaAgents</code> puzzle. It acts as a universal translator or remote control, allowing your <code>MultiStepAgent</code> to seamlessly communicate with a wide variety of Large Language Models without getting bogged down in the specific details of each one.</p>
<p>You've learned:</p>
<ul>
<li>Why a Model Interface is needed to handle diverse LLMs.</li>
<li>The "universal remote" analogy.</li>
<li>How the standard <code>__call__</code> method provides a consistent way for the agent to interact with the model.</li>
<li>How to choose, initialize, and provide a Model Interface (<code>LiteLLMModel</code> example) to your <code>MultiStepAgent</code>.</li>
<li>A glimpse into the internal process: translating requests, calling the backend LLM, and parsing responses.</li>
</ul>
<p>Now that our agent has a brain (<code>MultiStepAgent</code>) and a way to talk to it (<code>Model Interface</code>), how does it actually <em>do</em> things based on the LLM's suggestions? How does it search the web, run code, or perform other actions? That's where our next component comes in!</p>
<p><b>Next Chapter:</b> <a class="el" href="../../d4/d89/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2SmolaAgents_203__tool.html">Chapter 3: Tool</a> - Giving Your Agent Capabilities.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
