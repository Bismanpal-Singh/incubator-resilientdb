#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 01_llm</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('db/dc5/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_201__llm.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">01_llm</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md2587"></a>
autotoc_md2587</h2>
<p>layout: default title: "LLM" parent: "OpenManus" </p>
<h2><a class="anchor" id="autotoc_md2588"></a>
nav_order: 1</h2>
<h1><a class="anchor" id="autotoc_md2589"></a>
Chapter 1: The LLM - Your Agent's Brainpower</h1>
<p>Welcome to the OpenManus tutorial! We're thrilled to have you on board. Let's start with the absolute core of any intelligent agent: the "brain" that does the thinking and understanding. In OpenManus, this brainpower comes from something called a <b>Large Language Model (LLM)</b>, and we interact with it using our <code>LLM</code> class.</p>
<h2><a class="anchor" id="autotoc_md2590"></a>
What's the Big Deal with LLMs?</h2>
<p>Imagine you have access to an incredibly smart expert who understands language, can reason, write, summarize, and even generate creative ideas. That's kind of what an LLM (like GPT-4, Claude, or Llama) is! These are massive AI models trained on vast amounts of text and data, making them capable of understanding and generating human-like text.</p>
<p>They are the engine that drives the "intelligence" in AI applications like chatbots, writing assistants, and, of course, the agents you'll build with OpenManus.</p>
<h2><a class="anchor" id="autotoc_md2591"></a>
Why Do We Need an <code>LLM</code> Class?</h2>
<p>Okay, so LLMs are powerful. Can't our agent just talk directly to them?</p>
<p>Well, it's a bit more complicated than a casual chat. Talking to these big AI models usually involves:</p>
<ol type="1">
<li><b>Complex APIs:</b> Each LLM provider (like OpenAI, Anthropic, Google, AWS) has its own specific way (an API or Application Programming Interface) to send requests and get responses. It's like needing different phone numbers and dialing procedures for different experts.</li>
<li><b>API Keys:</b> You need secret keys to prove you're allowed to use the service (and get billed for it!). Managing these securely is important.</li>
<li><b>Formatting:</b> You need to structure your questions (prompts) and conversation history in a very specific format the LLM understands.</li>
<li><b>Errors &amp; Retries:</b> Sometimes network connections hiccup, or the LLM service is busy. You need a way to handle these errors gracefully, maybe by trying again.</li>
<li><b>Tracking Usage (Tokens):</b> Using these powerful models costs money, often based on how much text you send and receive (measured in "tokens"). You need to keep track of this.</li>
</ol>
<p>Doing all this <em>every time</em> an agent needs to think would be repetitive and messy!</p>
<p><b>This is where the <code>LLM</code> class comes in.</b> Think of it as a super-helpful <b>translator and network manager</b> rolled into one.</p>
<ul>
<li>It knows how to talk to different LLM APIs.</li>
<li>It securely handles your API keys (using settings from the <a class="el" href="../../d5/d05/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_207__configuration____config__.html">Configuration</a>).</li>
<li>It formats your messages correctly.</li>
<li>It automatically retries if there's a temporary glitch.</li>
<li>It helps count the "tokens" used.</li>
</ul>
<p>It hides all that complexity, giving your agent a simple way to "ask" the LLM something.</p>
<p><b>Use Case:</b> Let's say we want our agent to simply answer the question: "What is the capital of France?" The <code>LLM</code> class will handle all the background work to get that answer from the actual AI model.</p>
<h2><a class="anchor" id="autotoc_md2592"></a>
How Do Agents Use the <code>LLM</code> Class?</h2>
<p>In OpenManus, agents (which we'll learn more about in <a class="el" href="../../d0/d71/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_203__baseagent.html">Chapter 3: BaseAgent</a>) have an <code>llm</code> component built-in. Usually, you don't even need to create it manually; the agent does it for you when it starts up, using settings from your configuration file (<code>config/config.toml</code>).</p>
<p>The primary way an agent uses the <code>LLM</code> class is through its <code>ask</code> method.</p>
<p>Let's look at a simplified example of how you might use the <code>LLM</code> class directly (though usually, your agent handles this):</p>
<div class="fragment"><div class="line"><span class="comment"># Import necessary classes</span></div>
<div class="line"><span class="keyword">from</span> app.llm <span class="keyword">import</span> LLM</div>
<div class="line"><span class="keyword">from</span> app.schema <span class="keyword">import</span> Message</div>
<div class="line"><span class="keyword">import</span> asyncio <span class="comment"># Needed to run asynchronous code</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Assume configuration is already loaded (API keys, model name, etc.)</span></div>
<div class="line"><span class="comment"># Create an instance of the LLM class (using default settings)</span></div>
<div class="line">llm_interface = LLM()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Prepare the question as a list of messages</span></div>
<div class="line"><span class="comment"># (We&#39;ll learn more about Messages in Chapter 2)</span></div>
<div class="line">conversation = [</div>
<div class="line">    Message.user_message(<span class="stringliteral">&quot;What is the capital of France?&quot;</span>)</div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Define an async function to ask the question</span></div>
<div class="line"><span class="keyword">async def </span>ask_question():</div>
<div class="line">    print(<span class="stringliteral">&quot;Asking the LLM...&quot;</span>)</div>
<div class="line">    <span class="comment"># Use the &#39;ask&#39; method to send the conversation</span></div>
<div class="line">    response = await llm_interface.ask(messages=conversation)</div>
<div class="line">    print(f<span class="stringliteral">&quot;LLM Response: {response}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Run the async function</span></div>
<div class="line">asyncio.run(ask_question())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li>We import the <code>LLM</code> class and the <code>Message</code> class (more on <code>Message</code> in the <a class="el" href="../../dc/d8e/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_202__message______memory.html">next chapter</a>).</li>
<li>We create <code>llm_interface = LLM()</code>. This sets up our connection to the LLM using settings found in the configuration.</li>
<li>We create a <code>conversation</code> list containing our question, formatted as a <code>Message</code> object. The <code>LLM</code> class needs the input in this list-of-messages format.</li>
<li>We call <code>await llm_interface.ask(messages=conversation)</code>. This is the core action! We send our message list to the LLM via our interface. The <code>await</code> keyword is used because communicating over the network takes time, so we wait for the response asynchronously.</li>
<li>The <code>ask</code> method returns the LLM's text response as a string.</li>
</ol>
<p><b>Example Output (might vary slightly):</b></p>
<div class="fragment"><div class="line">Asking the LLM...</div>
<div class="line">LLM Response: The capital of France is Paris.</div>
</div><!-- fragment --><p>See? We just asked a question and got an answer, without worrying about API keys, JSON formatting, or network errors! The <code>LLM</code> class handled it all.</p>
<p>There's also a more advanced method called <code>ask_tool</code>, which allows the LLM to use specific <a class="el" href="../../de/db0/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_204__tool______toolcollection.html">Tools</a>, but we'll cover that later. For now, <code>ask</code> is the main way to get text responses.</p>
<h2><a class="anchor" id="autotoc_md2593"></a>
Under the Hood: What Happens When You <code>ask</code>?</h2>
<p>Let's peek behind the curtain. When your agent calls <code>llm.ask(...)</code>, several things happen in sequence:</p>
<ol type="1">
<li><b>Format Messages:</b> The <code>LLM</code> class takes your list of <code>Message</code> objects and converts them into the exact dictionary format the specific LLM API (like OpenAI's or AWS Bedrock's) expects. This might involve adding special tags or structuring image data if needed (<code>llm.py: format_messages</code>).</li>
<li><b>Count Tokens:</b> It calculates roughly how many "tokens" your input messages will use (<code>llm.py: count_message_tokens</code>).</li>
<li><b>Check Limits:</b> It checks if sending this request would exceed any configured token limits (<code>llm.py: check_token_limit</code>). If it does, it raises a specific <code>TokenLimitExceeded</code> error <em>before</em> making the expensive API call.</li>
<li><b>Send Request:</b> It sends the formatted messages and other parameters (like the desired model, <code>max_tokens</code>) to the LLM's API endpoint over the internet (<code>llm.py: client.chat.completions.create</code> or similar for AWS Bedrock in <code>bedrock.py</code>).</li>
<li><b>Handle Glitches (Retry):</b> If the API call fails due to a temporary issue (like a network timeout or the service being momentarily busy), the <code>LLM</code> class automatically waits a bit and tries again, up to a few times (thanks to the <code>@retry</code> decorator in <code>llm.py</code>).</li>
<li><b>Receive Response:</b> Once successful, it receives the response from the LLM API.</li>
<li><b>Extract Answer:</b> It pulls out the actual text content from the API response.</li>
<li><b>Update Counts:</b> It records the number of input tokens used and the number of tokens in the received response (<code>llm.py: update_token_count</code>).</li>
<li><b>Return Result:</b> Finally, it returns the LLM's text answer back to your agent.</li>
</ol>
<p>Here's a simplified diagram showing the flow:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant Agent</div>
<div class="line">    participant LLMClass as LLM Class (app/llm.py)</div>
<div class="line">    participant TokenCounter as Token Counter (app/llm.py)</div>
<div class="line">    participant OpenAIClient as OpenAI/Bedrock Client (app/llm.py, app/bedrock.py)</div>
<div class="line">    participant LLM_API as Actual LLM API (e.g., OpenAI, AWS Bedrock)</div>
<div class="line"> </div>
<div class="line">    Agent-&gt;&gt;+LLMClass: ask(messages)</div>
<div class="line">    LLMClass-&gt;&gt;LLMClass: format_messages(messages)</div>
<div class="line">    LLMClass-&gt;&gt;+TokenCounter: count_message_tokens(formatted_messages)</div>
<div class="line">    TokenCounter--&gt;&gt;-LLMClass: input_token_count</div>
<div class="line">    LLMClass-&gt;&gt;LLMClass: check_token_limit(input_token_count)</div>
<div class="line">    Note over LLMClass: If limit exceeded, raise Error.</div>
<div class="line">    LLMClass-&gt;&gt;+OpenAIClient: create_completion(formatted_messages, model, ...)</div>
<div class="line">    Note right of OpenAIClient: Handles retries on network errors etc.</div>
<div class="line">    OpenAIClient-&gt;&gt;+LLM_API: Send HTTP Request</div>
<div class="line">    LLM_API--&gt;&gt;-OpenAIClient: Receive HTTP Response</div>
<div class="line">    OpenAIClient--&gt;&gt;-LLMClass: completion_response</div>
<div class="line">    LLMClass-&gt;&gt;LLMClass: extract_content(completion_response)</div>
<div class="line">    LLMClass-&gt;&gt;+TokenCounter: update_token_count(input_tokens, completion_tokens)</div>
<div class="line">    TokenCounter--&gt;&gt;-LLMClass: </div>
<div class="line">    LLMClass--&gt;&gt;-Agent: llm_answer (string)</div>
</div><!-- fragment --><p>Let's look at a tiny piece of the <code>ask</code> method in <code>app/llm.py</code> to see the retry mechanism:</p>
<div class="fragment"><div class="line"><span class="comment"># Simplified snippet from app/llm.py</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> tenacity <span class="keyword">import</span> retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type</div>
<div class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAIError</div>
<div class="line"> </div>
<div class="line"><span class="comment"># ... other imports ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>LLM:</div>
<div class="line">    <span class="comment"># ... other methods like __init__, format_messages ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@retry</span>( <span class="comment"># This decorator handles retries!</span></div>
<div class="line">        wait=wait_random_exponential(min=1, max=60), <span class="comment"># Wait 1-60s between tries</span></div>
<div class="line">        stop=stop_after_attempt(6), <span class="comment"># Give up after 6 tries</span></div>
<div class="line">        retry=retry_if_exception_type((OpenAIError, Exception)) <span class="comment"># Retry on these errors</span></div>
<div class="line">    )</div>
<div class="line">    <span class="keyword">async def </span>ask(</div>
<div class="line">        self,</div>
<div class="line">        messages: List[Union[dict, Message]],</div>
<div class="line">        <span class="comment"># ... other parameters ...</span></div>
<div class="line">    ) -&gt; str:</div>
<div class="line">        <span class="keywordflow">try</span>:</div>
<div class="line">            <span class="comment"># 1. Format messages (simplified)</span></div>
<div class="line">            formatted_msgs = self.format_messages(messages)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 2. Count tokens &amp; Check limits (simplified)</span></div>
<div class="line">            input_tokens = self.count_message_tokens(formatted_msgs)</div>
<div class="line">            <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.check_token_limit(input_tokens):</div>
<div class="line">                <span class="keywordflow">raise</span> TokenLimitExceeded(...) <span class="comment"># Special error, not retried</span></div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 3. Prepare API call parameters (simplified)</span></div>
<div class="line">            params = {<span class="stringliteral">&quot;model&quot;</span>: self.model, <span class="stringliteral">&quot;messages&quot;</span>: formatted_msgs, ...}</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 4. Make the actual API call (simplified)</span></div>
<div class="line">            response = await self.client.chat.completions.create(**params)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># 5. Process response &amp; update tokens (simplified)</span></div>
<div class="line">            answer = response.choices[0].message.content</div>
<div class="line">            self.update_token_count(response.usage.prompt_tokens, ...)</div>
<div class="line"> </div>
<div class="line">            <span class="keywordflow">return</span> answer</div>
<div class="line">        <span class="keywordflow">except</span> TokenLimitExceeded:</div>
<div class="line">             <span class="keywordflow">raise</span> <span class="comment"># Don&#39;t retry token limits</span></div>
<div class="line">        <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">             logger.error(f<span class="stringliteral">&quot;LLM ask failed: {e}&quot;</span>)</div>
<div class="line">             <span class="keywordflow">raise</span> <span class="comment"># Let the @retry decorator handle retrying other errors</span></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ul>
<li>The <code>@retry(...)</code> part <em>above</em> the <code>async def ask(...)</code> line is key. It tells Python: "If the code inside this `ask` function fails with certain errors (like `OpenAIError`), wait a bit and try running it again, up to 6 times."</li>
<li>Inside the <code>try...except</code> block, the code performs the steps we discussed: format, count, check, call the API (<code>self.client.chat.completions.create</code>), and process the result.</li>
<li>Crucially, it catches the <code>TokenLimitExceeded</code> error separately and <code>raise</code>s it again immediately – we <em>don't</em> want to retry if we know we've run out of tokens!</li>
<li>Other errors will be caught by the final <code>except Exception</code>, logged, and re-raised, allowing the <code>@retry</code> mechanism to decide whether to try again.</li>
</ul>
<p>This shows how the <code>LLM</code> class uses libraries like <code>tenacity</code> to add resilience without cluttering the main logic of your agent.</p>
<h2><a class="anchor" id="autotoc_md2594"></a>
Wrapping Up Chapter 1</h2>
<p>You've learned about the core "brain" – the Large Language Model (LLM) – and why we need the <code>LLM</code> class in OpenManus to interact with it smoothly. This class acts as a vital interface, handling API complexities, errors, and token counting, providing your agents with simple <code>ask</code> (and <code>ask_tool</code>) methods.</p>
<p>Now that we understand how to communicate with the LLM, we need a way to structure the conversation – keeping track of who said what. That's where Messages and Memory come in.</p>
<p>Let's move on to <a class="el" href="../../dc/d8e/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2OpenManus_202__message______memory.html">Chapter 2: Message / Memory</a> to explore how we represent and store conversations for our agents.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
