#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 03_crawlerrunconfig</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">03_crawlerrunconfig</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1401"></a>
autotoc_md1401</h2>
<p>layout: default title: "CrawlerRunConfig" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1402"></a>
nav_order: 3</h2>
<h1><a class="anchor" id="autotoc_md1403"></a>
Chapter 3: Giving Instructions - CrawlerRunConfig</h1>
<p>In <a class="el" href="../../d8/dc9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_202__asyncwebcrawler.html">Chapter 2: Meet the General Manager - AsyncWebCrawler</a>, we met the <code>AsyncWebCrawler</code>, the central coordinator for our web crawling tasks. We saw how to tell it <em>what</em> URL to crawl using the <code>arun</code> method.</p>
<p>But what if we want to tell the crawler <em>how</em> to crawl that URL? Maybe we want it to take a picture (screenshot) of the page? Or perhaps we only care about a specific section of the page? Or maybe we want to ignore the cache and get the very latest version?</p>
<p>Passing all these different instructions individually every time we call <code>arun</code> could get complicated and messy.</p>
<div class="fragment"><div class="line"><span class="comment"># Imagine doing this every time - it gets long!</span></div>
<div class="line"><span class="comment"># result = await crawler.arun(</span></div>
<div class="line"><span class="comment">#     url=&quot;https://example.com&quot;,</span></div>
<div class="line"><span class="comment">#     take_screenshot=True,</span></div>
<div class="line"><span class="comment">#     ignore_cache=True,</span></div>
<div class="line"><span class="comment">#     only_look_at_this_part=&quot;#main-content&quot;,</span></div>
<div class="line"><span class="comment">#     wait_for_this_element=&quot;#data-table&quot;,</span></div>
<div class="line"><span class="comment">#     # ... maybe many more settings ...</span></div>
<div class="line"><span class="comment"># )</span></div>
</div><!-- fragment --><p>That's where <code>CrawlerRunConfig</code> comes in!</p>
<h2><a class="anchor" id="autotoc_md1404"></a>
What Problem Does <code>CrawlerRunConfig</code> Solve?</h2>
<p>Think of <code>CrawlerRunConfig</code> as the <b>Instruction Manual</b> for a <em>specific</em> crawl job. Instead of giving the <code>AsyncWebCrawler</code> manager lots of separate instructions each time, you bundle them all neatly into a single <code>CrawlerRunConfig</code> object.</p>
<p>This object tells the <code>AsyncWebCrawler</code> exactly <em>how</em> to handle a particular URL or set of URLs for that specific run. It makes your code cleaner and easier to manage.</p>
<h2><a class="anchor" id="autotoc_md1405"></a>
What is <code>CrawlerRunConfig</code>?</h2>
<p><code>CrawlerRunConfig</code> is a configuration class that holds all the settings for a single crawl operation initiated by <code>AsyncWebCrawler.arun()</code> or <code>arun_many()</code>.</p>
<p>It allows you to customize various aspects of the crawl, such as:</p>
<ul>
<li><b>Taking Screenshots:</b> Should the crawler capture an image of the page? (<code>screenshot</code>)</li>
<li><b>Waiting:</b> How long should the crawler wait for the page or specific elements to load? (<code>page_timeout</code>, <code>wait_for</code>)</li>
<li><b>Focusing Content:</b> Should the crawler only process a specific part of the page? (<code>css_selector</code>)</li>
<li><b>Extracting Data:</b> Should the crawler use a specific method to pull out structured data? (<a class="el" href="../../d1/d07/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_206__extractionstrategy.html">ExtractionStrategy</a>)</li>
<li><b>Caching:</b> How should the crawler interact with previously saved results? (<a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">CacheMode</a>)</li>
<li><b>And much more!</b> (like handling JavaScript, filtering links, etc.)</li>
</ul>
<h2><a class="anchor" id="autotoc_md1406"></a>
Using <code>CrawlerRunConfig</code></h2>
<p>Let's see how to use it. Remember our basic crawl from Chapter 2?</p>
<div class="fragment"><div class="line"><span class="comment"># chapter3_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {url_to_crawl} with default settings...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># This uses the default behavior (no specific config)</span></div>
<div class="line">        result = await crawler.arun(url=url_to_crawl)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;Success! Got the content.&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;Screenshot taken? {&#39;Yes&#39; if result.screenshot else &#39;No&#39;}&quot;</span>) <span class="comment"># Likely No</span></div>
<div class="line">            <span class="comment"># We&#39;ll learn about CacheMode later, but it defaults to using the cache</span></div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;Failed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p>Now, let's say for this <em>specific</em> crawl, we want to bypass the cache (fetch fresh) and also take a screenshot.</p>
<p>We create a <code>CrawlerRunConfig</code> instance and pass it to <code>arun</code>:</p>
<div class="fragment"><div class="line"><span class="comment"># chapter3_example_2.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> CrawlerRunConfig <span class="comment"># 1. Import the config class</span></div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> CacheMode        <span class="comment"># Import cache options</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {url_to_crawl} with custom settings...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 2. Create an instance of CrawlerRunConfig with our desired settings</span></div>
<div class="line">        my_instructions = CrawlerRunConfig(</div>
<div class="line">            cache_mode=CacheMode.BYPASS, <span class="comment"># Don&#39;t use the cache, fetch fresh</span></div>
<div class="line">            screenshot=<span class="keyword">True</span>              <span class="comment"># Take a screenshot</span></div>
<div class="line">        )</div>
<div class="line">        print(<span class="stringliteral">&quot;Instructions: Bypass cache, take screenshot.&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Pass the config object to arun()</span></div>
<div class="line">        result = await crawler.arun(</div>
<div class="line">            url=url_to_crawl,</div>
<div class="line">            config=my_instructions <span class="comment"># Pass our instruction manual</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Got the content with custom config.&quot;</span>)</div>
<div class="line">            print(f<span class="stringliteral">&quot;Screenshot taken? {&#39;Yes&#39; if result.screenshot else &#39;No&#39;}&quot;</span>) <span class="comment"># Should be Yes</span></div>
<div class="line">            <span class="comment"># Check if the screenshot file path exists in result.screenshot</span></div>
<div class="line">            <span class="keywordflow">if</span> result.screenshot:</div>
<div class="line">                print(f<span class="stringliteral">&quot;Screenshot saved to: {result.screenshot}&quot;</span>)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Import:</b> We import <code>CrawlerRunConfig</code> and <code>CacheMode</code>.</li>
<li><b>Create Config:</b> We create an instance: <code>my_instructions = CrawlerRunConfig(...)</code>. We set <code>cache_mode</code> to <code>CacheMode.BYPASS</code> and <code>screenshot</code> to <code>True</code>. All other settings remain at their defaults.</li>
<li><b>Pass Config:</b> We pass this <code>my_instructions</code> object to <code>crawler.arun</code> using the <code>config=</code> parameter.</li>
</ol>
<p>Now, when <code>AsyncWebCrawler</code> runs this job, it will look inside <code>my_instructions</code> and follow those specific settings for <em>this run only</em>.</p>
<h2><a class="anchor" id="autotoc_md1407"></a>
Some Common <code>CrawlerRunConfig</code> Parameters</h2>
<p><code>CrawlerRunConfig</code> has many options, but here are a few common ones you might use:</p>
<ul>
<li><b><code>cache_mode</code></b>: Controls caching behavior.<ul>
<li><code>CacheMode.ENABLED</code> (Default): Use the cache if available, otherwise fetch and save.</li>
<li><code>CacheMode.BYPASS</code>: Always fetch fresh, ignoring any cached version (but still save the new result).</li>
<li><code>CacheMode.DISABLED</code>: Never read from or write to the cache.</li>
<li><em>(More details in <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>)</em></li>
</ul>
</li>
<li><b><code>screenshot</code> (bool)</b>: If <code>True</code>, takes a screenshot of the fully rendered page. The path to the screenshot file will be in <code>CrawlResult.screenshot</code>. Default: <code>False</code>.</li>
<li><b><code>pdf</code> (bool)</b>: If <code>True</code>, generates a PDF of the page. The path to the PDF file will be in <code>CrawlResult.pdf</code>. Default: <code>False</code>.</li>
<li><b><code>css_selector</code> (str)</b>: If provided (e.g., <code>"#main-content"</code> or <code>.article-body</code>), the crawler will try to extract <em>only</em> the HTML content within the element(s) matching this CSS selector. This is great for focusing on the important part of a page. Default: <code>None</code> (process the whole page).</li>
<li><b><code>wait_for</code> (str)</b>: A CSS selector (e.g., <code>"#data-loaded-indicator"</code>). The crawler will wait until an element matching this selector appears on the page before proceeding. Useful for pages that load content dynamically with JavaScript. Default: <code>None</code>.</li>
<li><b><code>page_timeout</code> (int)</b>: Maximum time in milliseconds to wait for page navigation or certain operations. Default: <code>60000</code> (60 seconds).</li>
<li><b><code>extraction_strategy</code></b>: An object that defines how to extract specific, structured data (like product names and prices) from the page. Default: <code>None</code>. <em>(See <a class="el" href="../../d1/d07/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_206__extractionstrategy.html">Chapter 6: Getting Specific Data - ExtractionStrategy</a>)</em></li>
<li><b><code>scraping_strategy</code></b>: An object defining how the raw HTML is cleaned and basic content (like text and links) is extracted. Default: <code>WebScrapingStrategy()</code>. <em>(See <a class="el" href="../../df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html">Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy</a>)</em></li>
</ul>
<p>Let's try combining a few: focus on a specific part of the page and wait for something to appear.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter3_example_3.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler, CrawlerRunConfig</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># This example site has a heading &#39;H1&#39; inside a &#39;body&#39; tag.</span></div>
<div class="line">    url_to_crawl = <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {url_to_crawl}, focusing on the H1 tag...&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Instructions: Only get the H1 tag, wait max 10s for it</span></div>
<div class="line">        specific_config = CrawlerRunConfig(</div>
<div class="line">            css_selector=<span class="stringliteral">&quot;h1&quot;</span>, <span class="comment"># Only grab content inside &lt;h1&gt; tags</span></div>
<div class="line">            page_timeout=10000 <span class="comment"># Set page timeout to 10 seconds</span></div>
<div class="line">            <span class="comment"># We could also add wait_for=&quot;h1&quot; if needed for dynamic loading</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        result = await crawler.arun(url=url_to_crawl, config=specific_config)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> result.success:</div>
<div class="line">            print(<span class="stringliteral">&quot;\nSuccess! Focused crawl completed.&quot;</span>)</div>
<div class="line">            <span class="comment"># The markdown should now ONLY contain the H1 content</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;Markdown content:\n---\n{result.markdown.raw_markdown.strip()}\n---&quot;</span>)</div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            print(f<span class="stringliteral">&quot;\nFailed: {result.error_message}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p>This time, the <code>result.markdown</code> should only contain the text from the <code>&lt;h1&gt;</code> tag on that page, because we used <code>css_selector="h1"</code> in our <code>CrawlerRunConfig</code>.</p>
<h2><a class="anchor" id="autotoc_md1408"></a>
How <code>AsyncWebCrawler</code> Uses the Config (Under the Hood)</h2>
<p>You don't need to know the exact internal code, but it helps to understand the flow. When you call <code>crawler.arun(url, config=my_config)</code>, the <code>AsyncWebCrawler</code> essentially does this:</p>
<ol type="1">
<li>Receives the <code>url</code> and the <code>my_config</code> object.</li>
<li>Before fetching, it checks <code>my_config.cache_mode</code> to see if it should look in the cache first.</li>
<li>If fetching is needed, it passes <code>my_config</code> to the underlying <a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a>.</li>
<li>The strategy uses settings from <code>my_config</code> like <code>page_timeout</code>, <code>wait_for</code>, and whether to take a <code>screenshot</code>.</li>
<li>After getting the raw HTML, <code>AsyncWebCrawler</code> uses the <code>my_config.scraping_strategy</code> and <code>my_config.css_selector</code> to process the content.</li>
<li>If <code>my_config.extraction_strategy</code> is set, it uses that to extract structured data.</li>
<li>Finally, it bundles everything into a <code>CrawlResult</code> and returns it.</li>
</ol>
<p>Here's a simplified view:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant AWC as AsyncWebCrawler</div>
<div class="line">    participant Config as CrawlerRunConfig</div>
<div class="line">    participant Fetcher as AsyncCrawlerStrategy</div>
<div class="line">    participant Processor as Scraping/Extraction</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;AWC: arun(url, config=my_config)</div>
<div class="line">    AWC-&gt;&gt;Config: Check my_config.cache_mode</div>
<div class="line">    alt Need to Fetch</div>
<div class="line">        AWC-&gt;&gt;Fetcher: crawl(url, config=my_config)</div>
<div class="line">        Note over Fetcher: Uses my_config settings (timeout, wait_for, screenshot...)</div>
<div class="line">        Fetcher--&gt;&gt;AWC: Raw Response (HTML, screenshot?)</div>
<div class="line">        AWC-&gt;&gt;Processor: Process HTML (using my_config.css_selector, my_config.extraction_strategy...)</div>
<div class="line">        Processor--&gt;&gt;AWC: Processed Data</div>
<div class="line">    else Use Cache</div>
<div class="line">        AWC-&gt;&gt;AWC: Retrieve from Cache</div>
<div class="line">    end</div>
<div class="line">    AWC--&gt;&gt;User: Return CrawlResult</div>
</div><!-- fragment --><p>The <code>CrawlerRunConfig</code> acts as a messenger carrying your specific instructions throughout the crawling process.</p>
<p>Inside the <code>crawl4ai</code> library, in the file <code>async_configs.py</code>, you'll find the definition of the <code>CrawlerRunConfig</code> class. It looks something like this (simplified):</p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/async_configs.py</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> .cache_context <span class="keyword">import</span> CacheMode</div>
<div class="line"><span class="keyword">from</span> .extraction_strategy <span class="keyword">import</span> ExtractionStrategy</div>
<div class="line"><span class="keyword">from</span> .content_scraping_strategy <span class="keyword">import</span> ContentScrapingStrategy, WebScrapingStrategy</div>
<div class="line"><span class="comment"># ... other imports ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>CrawlerRunConfig():</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral">    Configuration class for controlling how the crawler runs each crawl operation.</span></div>
<div class="line"><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line">    <span class="keyword">def </span>__init__(</div>
<div class="line">        self,</div>
<div class="line">        <span class="comment"># Caching</span></div>
<div class="line">        cache_mode: CacheMode = CacheMode.BYPASS, <span class="comment"># Default behavior if not specified</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Content Selection / Waiting</span></div>
<div class="line">        css_selector: str = <span class="keywordtype">None</span>,</div>
<div class="line">        wait_for: str = <span class="keywordtype">None</span>,</div>
<div class="line">        page_timeout: int = 60000, <span class="comment"># 60 seconds</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Media</span></div>
<div class="line">        screenshot: bool = <span class="keyword">False</span>,</div>
<div class="line">        pdf: bool = <span class="keyword">False</span>,</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Processing Strategies</span></div>
<div class="line">        scraping_strategy: ContentScrapingStrategy = <span class="keywordtype">None</span>, <span class="comment"># Defaults internally if None</span></div>
<div class="line">        extraction_strategy: ExtractionStrategy = <span class="keywordtype">None</span>,</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># ... many other parameters omitted for clarity ...</span></div>
<div class="line">        **kwargs <span class="comment"># Allows for flexibility</span></div>
<div class="line">    ):</div>
<div class="line">        self.cache_mode = cache_mode</div>
<div class="line">        self.css_selector = css_selector</div>
<div class="line">        self.wait_for = wait_for</div>
<div class="line">        self.page_timeout = page_timeout</div>
<div class="line">        self.screenshot = screenshot</div>
<div class="line">        self.pdf = pdf</div>
<div class="line">        <span class="comment"># Assign scraping strategy, ensuring a default if None is provided</span></div>
<div class="line">        self.scraping_strategy = scraping_strategy <span class="keywordflow">or</span> WebScrapingStrategy()</div>
<div class="line">        self.extraction_strategy = extraction_strategy</div>
<div class="line">        <span class="comment"># ... initialize other attributes ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Helper methods like &#39;clone&#39;, &#39;to_dict&#39;, &#39;from_kwargs&#39; might exist too</span></div>
<div class="line">    <span class="comment"># ...</span></div>
</div><!-- fragment --><p>The key idea is that it's a class designed to hold various settings together. When you create an instance <code>CrawlerRunConfig(...)</code>, you're essentially creating an object that stores your choices for these parameters.</p>
<h2><a class="anchor" id="autotoc_md1409"></a>
Conclusion</h2>
<p>You've learned about <code>CrawlerRunConfig</code>, the "Instruction Manual" for individual crawl jobs in Crawl4AI!</p>
<ul>
<li>It solves the problem of passing many settings individually to <code>AsyncWebCrawler</code>.</li>
<li>You create an instance of <code>CrawlerRunConfig</code> and set the parameters you want to customize (like <code>cache_mode</code>, <code>screenshot</code>, <code>css_selector</code>, <code>wait_for</code>).</li>
<li>You pass this config object to <code>crawler.arun(url, config=your_config)</code>.</li>
<li>This makes your code cleaner and gives you fine-grained control over <em>how</em> each crawl is performed.</li>
</ul>
<p>Now that we know how to fetch content (<a class="el" href="../../dc/d53/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_201__asynccrawlerstrategy.html">AsyncCrawlerStrategy</a>), manage the overall process (<a class="el" href="../../d8/dc9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_202__asyncwebcrawler.html">AsyncWebCrawler</a>), and give specific instructions (<a class="el" href="../../db/d01/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_203__crawlerrunconfig.html">CrawlerRunConfig</a>), let's look at how the raw, messy HTML fetched from the web is initially cleaned up and processed.</p>
<p><b>Next:</b> Let's explore <a class="el" href="../../df/d80/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_204__contentscrapingstrategy.html">Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy</a>.</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
