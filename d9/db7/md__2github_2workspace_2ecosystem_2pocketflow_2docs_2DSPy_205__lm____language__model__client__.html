#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 05_lm__language_model_client_</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d9/db7/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_205__lm____language__model__client__.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">05_lm__language_model_client_</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1653"></a>
autotoc_md1653</h2>
<p>layout: default title: "LM (Language Model Client)" parent: "DSPy" </p>
<h2><a class="anchor" id="autotoc_md1654"></a>
nav_order: 5</h2>
<h1><a class="anchor" id="autotoc_md1655"></a>
Chapter 5: LM (Language Model Client) - The Engine Room</h1>
<p>In <a class="el" href="../../d5/d3c/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_204__predict.html">Chapter 4: Predict</a>, we saw how <code>dspy.Predict</code> takes a <a class="el" href="../../d4/dfe/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_202__signature.html">Signature</a> and input data to magically generate an output. We used our <code>translator</code> example:</p>
<div class="fragment"><div class="line"><span class="comment"># translator = dspy.Predict(TranslateToFrench)</span></div>
<div class="line"><span class="comment"># result = translator(english_sentence=&quot;Hello, how are you?&quot;)</span></div>
<div class="line"><span class="comment"># print(result.french_sentence) # --&gt; Bonjour, comment ça va?</span></div>
</div><!-- fragment --><p>But wait... how did <code>dspy.Predict</code> <em>actually</em> produce that French sentence? It didn't just invent it! It needed to talk to a powerful Language Model (LM) like GPT-3.5, GPT-4, Claude, Llama, or some other AI brain.</p>
<p>How does DSPy connect your program (<code>dspy.Predict</code> in this case) to these external AI brains? That's the job of the <b>LM (Language Model Client)</b> abstraction!</p>
<p>Think of the LM Client as:</p>
<ul>
<li><b>The Engine:</b> It's the core component that provides the "thinking" power to your DSPy modules.</li>
<li><b>The Translator:</b> It speaks the specific language (API calls, parameters) required by different LM providers (like OpenAI, Anthropic, Cohere, Hugging Face, or models running locally).</li>
<li><b>The Connection:</b> It bridges the gap between your abstract DSPy code and the concrete LM service.</li>
</ul>
<p>In this chapter, you'll learn:</p>
<ul>
<li>What the LM Client does and why it's crucial.</li>
<li>How to tell DSPy which Language Model to use.</li>
<li>How this setup lets you easily switch between different LMs.</li>
<li>A peek under the hood at how the connection works.</li>
</ul>
<p>Let's connect our program to an AI brain!</p>
<h2><a class="anchor" id="autotoc_md1656"></a>
What Does the LM Client Do?</h2>
<p>When a module like <code>dspy.Predict</code> needs an LM to generate text, it doesn't make the raw API call itself. Instead, it relies on the configured <b>LM Client</b>. The LM Client handles several important tasks:</p>
<ol type="1">
<li><b>API Interaction:</b> It knows how to format the request (the prompt, parameters like <code>temperature</code>, <code>max_tokens</code>) in the exact way the target LM provider expects. It then makes the actual network call to the provider's API (or interacts with a local model).</li>
<li><b>Parameter Management:</b> You can set standard parameters like <code>temperature</code> (controlling randomness) or <code>max_tokens</code> (limiting output length) when you configure the LM Client. It ensures these are sent correctly with each request.</li>
<li><b>Authentication:</b> It usually handles sending your API keys securely (often by reading them from environment variables).</li>
<li><b>Retries:</b> If an API call fails due to a temporary issue (like a network glitch or the LM service being busy), the LM Client often automatically retries the request a few times.</li>
<li><b>Standard Interface:</b> It provides a consistent way for DSPy modules (<code>Predict</code>, <code>ChainOfThought</code>, etc.) to interact with <em>any</em> supported LM. This means you can swap the underlying LM without changing your module code.</li>
<li><b>Caching:</b> To save time and money, the LM Client usually caches responses. If you make the exact same request again, it can return the saved result instantly instead of calling the LM API again.</li>
</ol>
<p>Essentially, the LM Client abstracts away all the messy details of talking to different AI models, giving your DSPy program a clean and consistent engine to rely on.</p>
<h2><a class="anchor" id="autotoc_md1657"></a>
Configuring Which LM to Use</h2>
<p>So, how do you tell DSPy <em>which</em> LM engine to use? You do this using <code>dspy.settings.configure</code>.</p>
<p>First, you need to import and create an instance of the specific client for your desired LM provider. DSPy integrates with many models primarily through the <code>litellm</code> library, but also provides direct wrappers for common ones like OpenAI.</p>
<p><b>Example: Configuring OpenAI's GPT-3.5 Turbo</b></p>
<p>Let's say you want to use OpenAI's <code>gpt-3.5-turbo</code> model.</p>
<ol type="1">
<li><b>Import the client:</b> <code>python import dspy </code> <em>(Note: For many common providers like OpenAI, Anthropic, Cohere, etc., you can use the general <code>dspy.LM</code> client which leverages <code>litellm</code>)</em></li>
<li><p class="startli"><b>Create an instance:</b> You specify the model name. API keys are typically picked up automatically from environment variables (e.g., <code>OPENAI_API_KEY</code>). You can also set default parameters here.</p>
<p class="startli">``&lsquo;python </p>
</li>
</ol>
<h1><a class="anchor" id="autotoc_md1658"></a>
Use the generic dspy.LM for LiteLLM integration</h1>
<h1><a class="anchor" id="autotoc_md1659"></a>
Model name follows 'provider/model_name&rsquo; format for many models</h1>
<p>turbo = dspy.LM(model='openai/gpt-3.5-turbo', max_tokens=100)</p>
<h1><a class="anchor" id="autotoc_md1660"></a>
Or, if you prefer the dedicated OpenAI client wrapper (functionally similar for basic use)</h1>
<h1><a class="anchor" id="autotoc_md1661"></a>
from dspy.models.openai import OpenAI</h1>
<h1><a class="anchor" id="autotoc_md1662"></a>
turbo = OpenAI(model='gpt-3.5-turbo', max_tokens=100)</h1>
<p>``<code> This creates an object</code>turbo<code>that knows how to talk to the</code>gpt-3.5-turbo<code>model via OpenAI's API (using</code>litellm`'s connection logic) and will limit responses to 100 tokens by default.</p>
<ol type="1">
<li><p class="startli"><b>Configure DSPy settings:</b> You tell DSPy globally that this is the LM engine to use for subsequent calls.</p>
<p class="startli"><code>python dspy.settings.configure(lm=turbo) </code> That's it! Now, any DSPy module (like <code>dspy.Predict</code>) that needs to call an LM will automatically use the <code>turbo</code> instance we just configured.</p>
</li>
</ol>
<p><b>Using Other Models (via <code>dspy.LM</code> and LiteLLM)</b></p>
<p>The <code>dspy.LM</code> client is very powerful because it uses <code>litellm</code> under the hood, which supports a vast numberk of models from providers like Anthropic, Cohere, Google, Hugging Face, Ollama (for local models), and more. You generally just need to change the <code>model</code> string.</p>
<div class="fragment"><div class="line"><span class="comment"># Example: Configure Anthropic&#39;s Claude 3 Haiku</span></div>
<div class="line"><span class="comment"># (Assumes ANTHROPIC_API_KEY environment variable is set)</span></div>
<div class="line"><span class="comment"># Note: Provider prefix &#39;anthropic/&#39; is often optional if model name is unique</span></div>
<div class="line">claude_haiku = dspy.LM(model=<span class="stringliteral">&#39;anthropic/claude-3-haiku-20240307&#39;</span>, max_tokens=200)</div>
<div class="line">dspy.settings.configure(lm=claude_haiku)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Now DSPy modules will use Claude 3 Haiku</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Example: Configure a local model served via Ollama</span></div>
<div class="line"><span class="comment"># (Assumes Ollama server is running and has the &#39;llama3&#39; model)</span></div>
<div class="line">local_llama = dspy.LM(model=<span class="stringliteral">&#39;ollama/llama3&#39;</span>, max_tokens=500, temperature=0.7)</div>
<div class="line">dspy.settings.configure(lm=local_llama)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Now DSPy modules will use the local Llama 3 model via Ollama</span></div>
</div><!-- fragment --><p>You only need to configure the LM <b>once</b> (usually at the start of your script).</p>
<h2><a class="anchor" id="autotoc_md1663"></a>
How Modules Use the Configured LM</h2>
<p>Remember our <code>translator</code> module from <a class="el" href="../../d5/d3c/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_204__predict.html">Chapter 4: Predict</a>?</p>
<div class="fragment"><div class="line"><span class="comment"># Define signature (same as before)</span></div>
<div class="line"><span class="keyword">class </span>TranslateToFrench(dspy.Signature):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Translates English text to French.&quot;&quot;&quot;</span></div>
<div class="line">    english_sentence = dspy.InputField()</div>
<div class="line">    french_sentence = dspy.OutputField()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Configure the LM (e.g., using OpenAI)</span></div>
<div class="line"><span class="comment"># turbo = dspy.LM(model=&#39;openai/gpt-3.5-turbo&#39;, max_tokens=100)</span></div>
<div class="line"><span class="comment"># dspy.settings.configure(lm=turbo)</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create the Predict module</span></div>
<div class="line">translator = dspy.Predict(TranslateToFrench)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Use the module - NO need to pass the LM here!</span></div>
<div class="line">result = translator(english_sentence=<span class="stringliteral">&quot;Hello, how are you?&quot;</span>)</div>
<div class="line">print(result.french_sentence)</div>
</div><!-- fragment --><p>Notice that we didn't pass <code>turbo</code> or <code>claude_haiku</code> or <code>local_llama</code> directly to <code>dspy.Predict</code>. When <code>translator(...)</code> is called, <code>dspy.Predict</code> internally asks <code>dspy.settings</code> for the currently configured <code>lm</code>. It then uses that client object to handle the actual LM interaction.</p>
<h2><a class="anchor" id="autotoc_md1664"></a>
The Power of Swapping LMs</h2>
<p>This setup makes it incredibly easy to experiment with different language models. Want to see if Claude does a better job at translation than GPT-3.5? Just change the configuration!</p>
<div class="fragment"><div class="line"><span class="comment"># --- Experiment 1: Using GPT-3.5 Turbo ---</span></div>
<div class="line">print(<span class="stringliteral">&quot;Testing with GPT-3.5 Turbo...&quot;</span>)</div>
<div class="line">turbo = dspy.LM(model=<span class="stringliteral">&#39;openai/gpt-3.5-turbo&#39;</span>, max_tokens=100)</div>
<div class="line">dspy.settings.configure(lm=turbo)</div>
<div class="line"> </div>
<div class="line">translator = dspy.Predict(TranslateToFrench)</div>
<div class="line">result_turbo = translator(english_sentence=<span class="stringliteral">&quot;Where is the library?&quot;</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;GPT-3.5: {result_turbo.french_sentence}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># --- Experiment 2: Using Claude 3 Haiku ---</span></div>
<div class="line">print(<span class="stringliteral">&quot;\nTesting with Claude 3 Haiku...&quot;</span>)</div>
<div class="line">claude_haiku = dspy.LM(model=<span class="stringliteral">&#39;anthropic/claude-3-haiku-20240307&#39;</span>, max_tokens=100)</div>
<div class="line">dspy.settings.configure(lm=claude_haiku)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># We can reuse the SAME translator object, or create a new one</span></div>
<div class="line"><span class="comment"># It will pick up the NEWLY configured LM from settings</span></div>
<div class="line">result_claude = translator(english_sentence=<span class="stringliteral">&quot;Where is the library?&quot;</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;Claude 3 Haiku: {result_claude.french_sentence}&quot;</span>)</div>
</div><!-- fragment --><p><b>Expected Output:</b></p>
<div class="fragment"><div class="line">Testing with GPT-3.5 Turbo...</div>
<div class="line">GPT-3.5: Où est la bibliothèque?</div>
<div class="line"> </div>
<div class="line">Testing with Claude 3 Haiku...</div>
<div class="line">Claude 3 Haiku: Où se trouve la bibliothèque ?</div>
</div><!-- fragment --><p>Look at that! We changed the underlying AI brain just by modifying the <code>dspy.settings.configure</code> call. The core logic of our <code>translator</code> module remained untouched. This flexibility is a key advantage of DSPy.</p>
<h2><a class="anchor" id="autotoc_md1665"></a>
How It Works Under the Hood (A Peek)</h2>
<p>Let's trace what happens when <code>translator(english_sentence=...)</code> runs:</p>
<ol type="1">
<li><b>Module Execution:</b> The <code>forward</code> method of the <code>dspy.Predict</code> module (<code>translator</code>) starts executing.</li>
<li><b>Get LM Client:</b> Inside its logic, <code>Predict</code> needs to call an LM. It accesses <code>dspy.settings.lm</code>. This returns the currently configured LM client object (e.g., the <code>claude_haiku</code> instance we set).</li>
<li><b>Format Prompt:</b> <code>Predict</code> uses the <a class="el" href="../../d4/dfe/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_202__signature.html">Signature</a> and the input (<code>english_sentence</code>) to prepare the text prompt.</li>
<li><b>LM Client Call:</b> <code>Predict</code> calls the LM client object, passing the formatted prompt and any necessary parameters (like <code>max_tokens</code> which might come from the client's defaults or be overridden). Let's say it calls <code>claude_haiku(prompt, max_tokens=100, ...)</code>.</li>
<li><b>API Interaction (Inside LM Client):</b><ul>
<li>The <code>claude_haiku</code> object (an instance of <code>dspy.LM</code>) checks its cache first. If the same request was made recently, it might return the cached response directly.</li>
<li>If not cached, it constructs the specific API request for Anthropic's Claude 3 Haiku model (using <code>litellm</code>). This includes setting headers, API keys, and formatting the prompt/parameters correctly for Anthropic.</li>
<li>It makes the HTTPS request to the Anthropic API endpoint.</li>
<li>It handles potential retries if the API returns specific errors.</li>
<li>It receives the raw response from the API.</li>
</ul>
</li>
<li><b>Parse Response (Inside LM Client):</b> The client extracts the generated text content from the API response structure.</li>
<li><b>Return to Module:</b> The LM client returns the generated text (e.g., <code>"Où se trouve la bibliothèque ?"</code>) back to the <code>dspy.Predict</code> module.</li>
<li><b>Module Finishes:</b> <code>Predict</code> takes this text, parses it according to the <code>OutputField</code> (<code>french_sentence</code>) in the signature, and returns the final <code>Prediction</code> object.</li>
</ol>
<p>Here's a simplified sequence diagram:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant PredictModule as translator (Predict)</div>
<div class="line">    participant Settings as dspy.settings</div>
<div class="line">    participant LMClient as LM Client (e.g., dspy.LM instance)</div>
<div class="line">    participant ActualAPI as Actual LM API (e.g., Anthropic)</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;PredictModule: Call translator(english_sentence=&quot;...&quot;)</div>
<div class="line">    PredictModule-&gt;&gt;Settings: Get configured lm</div>
<div class="line">    Settings--&gt;&gt;PredictModule: Return LMClient instance</div>
<div class="line">    PredictModule-&gt;&gt;PredictModule: Format prompt for LM</div>
<div class="line">    PredictModule-&gt;&gt;LMClient: __call__(prompt, **params)</div>
<div class="line">    LMClient-&gt;&gt;LMClient: Check Cache (Cache Miss)</div>
<div class="line">    LMClient-&gt;&gt;ActualAPI: Send formatted API request (prompt, key, params)</div>
<div class="line">    ActualAPI--&gt;&gt;LMClient: Return API response</div>
<div class="line">    LMClient-&gt;&gt;LMClient: Parse response, extract text</div>
<div class="line">    LMClient--&gt;&gt;PredictModule: Return generated text</div>
<div class="line">    PredictModule-&gt;&gt;PredictModule: Parse text into output fields</div>
<div class="line">    PredictModule--&gt;&gt;User: Return Prediction object</div>
</div><!-- fragment --><p><b>Relevant Code Files:</b></p>
<ul>
<li><code>dspy/clients/lm.py</code>: Defines the main <code>dspy.LM</code> class which uses <code>litellm</code> for broad compatibility. It handles caching (in-memory and disk via <code>litellm</code>), retries, parameter mapping, and calling the appropriate <code>litellm</code> functions.</li>
<li><code>dspy/clients/base_lm.py</code>: Defines the <code>BaseLM</code> abstract base class that all LM clients inherit from. It includes the basic <code>__call__</code> structure, history tracking, and requires subclasses to implement the core <code>forward</code> method for making the actual API call. It also defines <code>inspect_history</code>.</li>
<li><code>dspy/models/openai.py</code> (and others like <code>anthropic.py</code>, <code>cohere.py</code> - though <code>dspy.LM</code> is often preferred now): Specific client implementations (often inheriting from <code>BaseLM</code> or using <code>dspy.LM</code> internally).</li>
<li><code>dspy/dsp/utils/settings.py</code>: Defines the <code>Settings</code> singleton object where the configured <code>lm</code> (and other components like <code>rm</code>) are stored and accessed globally or via thread-local context.</li>
</ul>
<div class="fragment"><div class="line"><span class="comment"># Simplified structure from dspy/clients/base_lm.py</span></div>
<div class="line"><span class="keyword">class </span>BaseLM:</div>
<div class="line">    <span class="keyword">def </span>__init__(self, model, **kwargs):</div>
<div class="line">        self.model = model</div>
<div class="line">        self.kwargs = kwargs <span class="comment"># Default params like temp, max_tokens</span></div>
<div class="line">        self.history = [] <span class="comment"># Stores records of calls</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@with_callbacks</span> <span class="comment"># Handles logging, potential custom hooks</span></div>
<div class="line">    <span class="keyword">def </span>__call__(self, prompt=None, messages=None, **kwargs):</div>
<div class="line">        <span class="comment"># 1. Call the actual request logic (implemented by subclasses)</span></div>
<div class="line">        response = self.forward(prompt=prompt, messages=messages, **kwargs)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 2. Extract the output text(s)</span></div>
<div class="line">        outputs = [choice.message.content <span class="keywordflow">for</span> choice <span class="keywordflow">in</span> response.choices] <span class="comment"># Simplified</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Log the interaction (prompt, response, cost, etc.)</span></div>
<div class="line">        <span class="comment">#    (self.history.append(...))</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 4. Return the list of generated texts</span></div>
<div class="line">        <span class="keywordflow">return</span> outputs</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>forward(self, prompt=None, messages=None, **kwargs):</div>
<div class="line">        <span class="comment"># Subclasses MUST implement this method to make the actual API call</span></div>
<div class="line">        <span class="comment"># It should return an object similar to OpenAI&#39;s API response structure</span></div>
<div class="line">        <span class="keywordflow">raise</span> NotImplementedError</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Simplified structure from dspy/clients/lm.py</span></div>
<div class="line"><span class="keyword">import</span> litellm</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>LM(BaseLM): <span class="comment"># Inherits from BaseLM</span></div>
<div class="line">    <span class="keyword">def </span>__init__(self, model, model_type=&quot;chat&quot;, ..., num_retries=8, **kwargs):</div>
<div class="line">        super().__init__(model=model, **kwargs)</div>
<div class="line">        self.model_type = model_type</div>
<div class="line">        self.num_retries = num_retries</div>
<div class="line">        <span class="comment"># ... other setup ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>forward(self, prompt=None, messages=None, **kwargs):</div>
<div class="line">        <span class="comment"># Combine default and call-specific kwargs</span></div>
<div class="line">        request_kwargs = {**self.kwargs, **kwargs}</div>
<div class="line">        messages = messages <span class="keywordflow">or</span> [{<span class="stringliteral">&quot;role&quot;</span>: <span class="stringliteral">&quot;user&quot;</span>, <span class="stringliteral">&quot;content&quot;</span>: prompt}]</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Use litellm to make the call, handles different providers</span></div>
<div class="line">        <span class="comment"># Simplified - handles caching, retries, model types under the hood</span></div>
<div class="line">        <span class="keywordflow">if</span> self.model_type == <span class="stringliteral">&quot;chat&quot;</span>:</div>
<div class="line">            response = litellm.completion(</div>
<div class="line">                model=self.model,</div>
<div class="line">                messages=messages,</div>
<div class="line">                <span class="comment"># Pass combined parameters</span></div>
<div class="line">                **request_kwargs,</div>
<div class="line">                <span class="comment"># Configure retries and caching via litellm</span></div>
<div class="line">                num_retries=self.num_retries,</div>
<div class="line">                <span class="comment"># cache=...</span></div>
<div class="line">            )</div>
<div class="line">        <span class="keywordflow">else</span>: <span class="comment"># Text completion model type</span></div>
<div class="line">             response = litellm.text_completion(...) <span class="comment"># Simplified</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># LiteLLM returns an object compatible with BaseLM&#39;s expectations</span></div>
<div class="line">        <span class="keywordflow">return</span> response</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Simplified Usage in a Module (like Predict)</span></div>
<div class="line"><span class="comment"># from dspy.dsp.utils import settings</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Inside Predict&#39;s forward method:</span></div>
<div class="line"><span class="comment"># lm_client = settings.lm # Get the globally configured client</span></div>
<div class="line"><span class="comment"># prompt_text = self._generate_prompt(...) # Format the prompt</span></div>
<div class="line"><span class="comment"># parameters = self.config # Get parameters specific to this Predict instance</span></div>
<div class="line"><span class="comment"># generated_texts = lm_client(prompt_text, **parameters) # Call the LM Client!</span></div>
<div class="line"><span class="comment"># output_text = generated_texts[0]</span></div>
<div class="line"><span class="comment"># parsed_result = self._parse_output(output_text) # Parse based on signature</span></div>
<div class="line"><span class="comment"># return Prediction(**parsed_result)</span></div>
</div><!-- fragment --><p>The key is that modules interact with the standard <code>BaseLM</code> interface (primarily its <code>__call__</code> method), and the specific LM client implementation handles the rest.</p>
<h2><a class="anchor" id="autotoc_md1666"></a>
Conclusion</h2>
<p>You've now demystified the <b>LM (Language Model Client)</b>! It's the essential engine connecting your DSPy programs to the power of large language models.</p>
<ul>
<li>The LM Client acts as a <b>translator</b> and <b>engine</b>, handling API calls, parameters, retries, and caching.</li>
<li>You configure which LM to use <b>globally</b> via <code>dspy.settings.configure(lm=...)</code>, usually using <code>dspy.LM</code> for broad compatibility via <code>litellm</code>.</li>
<li>DSPy modules like <code>dspy.Predict</code> automatically <b>use the configured LM</b> without needing it passed explicitly.</li>
<li>This makes it easy to <b>swap out different LMs</b> (like GPT-4, Claude, Llama) with minimal code changes, facilitating experimentation.</li>
</ul>
<p>Now that we know how to connect to the "brain" (LM), what about connecting to external knowledge sources like databases or document collections? That's where the <b>RM (Retrieval Model Client)</b> comes in.</p>
<p><b>Next:</b> <a class="el" href="../../d9/d67/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2DSPy_206__rm____retrieval__model__client__.html">Chapter 6: RM (Retrieval Model Client)</a></p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
