#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 07_memory</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('da/dd4/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_207__memory.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">07_memory</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md765"></a>
autotoc_md765</h2>
<p>layout: default title: "Memory" parent: "AutoGen Core" </p>
<h2><a class="anchor" id="autotoc_md766"></a>
nav_order: 7</h2>
<h1><a class="anchor" id="autotoc_md767"></a>
Chapter 7: Memory - The Agent's Notebook</h1>
<p>In <a class="el" href="../../dd/dc3/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_206__chatcompletioncontext.html">Chapter 6: ChatCompletionContext</a>, we saw how agents manage the <em>short-term</em> history of a single conversation before talking to an LLM. It's like remembering what was just said in the last few minutes.</p>
<p>But what if an agent needs to remember things for much longer, across <em>multiple</em> conversations or tasks? For example, imagine an assistant agent that learns your preferences:</p><ul>
<li>You tell it: "Please always write emails in a formal style for me."</li>
<li>Weeks later, you ask it to draft a new email.</li>
</ul>
<p>How does it remember that preference? The short-term <code>ChatCompletionContext</code> might have forgotten the earlier instruction, especially if using a strategy like <code>BufferedChatCompletionContext</code>. The agent needs a <b>long-term memory</b>.</p>
<p>This is where the <b><code>Memory</code></b> abstraction comes in. Think of it as the agent's <b>long-term notebook or database</b>. While <code>ChatCompletionContext</code> is the scratchpad for the current chat, <code>Memory</code> holds persistent information the agent can add to or look up later.</p>
<h2><a class="anchor" id="autotoc_md768"></a>
Motivation: Remembering Across Conversations</h2>
<p>Our goal is to give an agent the ability to store a piece of information (like a user preference) and retrieve it later to influence its behavior, even in a completely new conversation. <code>Memory</code> provides the mechanism for this long-term storage and retrieval.</p>
<h2><a class="anchor" id="autotoc_md769"></a>
Key Concepts: How the Notebook Works</h2>
<ol type="1">
<li><p class="startli"><b>What it Stores (<code>MemoryContent</code>):</b> Agents can store various types of information in their memory. This could be:</p><ul>
<li>Plain text notes (<code>text/plain</code>)</li>
<li>Structured data like JSON (<code>application/json</code>)</li>
<li>Even images (<code>image/*</code>) Each piece of information is wrapped in a <code>MemoryContent</code> object, which includes the data itself, its type (<code>mime_type</code>), and optional descriptive <code>metadata</code>.</li>
</ul>
<p class="startli">```python </p>
</li>
</ol>
<h1><a class="anchor" id="autotoc_md770"></a>
From: memory/_base_memory.py (Simplified Concept)</h1>
<p>from pydantic import BaseModel from typing import Any, Dict, Union</p>
<h1><a class="anchor" id="autotoc_md771"></a>
Represents one entry in the memory notebook</h1>
<p>class MemoryContent(BaseModel): content: Union[str, bytes, Dict[str, Any]] # The actual data mime_type: str # What kind of data (e.g., "text/plain") metadata: Dict[str, Any] | None = None # Extra info (optional) ``` This standard format helps manage different kinds of memories.</p>
<ol type="1">
<li><b>Adding to Memory (<code>add</code>):</b> When an agent learns something important it wants to remember long-term (like the user's preferred style), it uses the <code>memory.add(content)</code> method. This is like writing a new entry in the notebook.</li>
<li><b>Querying Memory (<code>query</code>):</b> When an agent needs to recall information, it can use <code>memory.query(query_text)</code>. This is like searching the notebook for relevant entries. How the search works depends on the specific memory implementation (it could be a simple text match, or a sophisticated vector search in more advanced memories).</li>
<li><b>Updating Chat Context (<code>update_context</code>):</b> This is a crucial link! Before an agent talks to the LLM (using the <code>ChatCompletionClient</code> from <a class="el" href="../../d8/df2/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_205__chatcompletionclient.html">Chapter 5</a>), it can use <code>memory.update_context(chat_context)</code> method. This method:<ul>
<li>Looks at the current conversation (<code>chat_context</code>).</li>
<li>Queries the long-term memory (<code>Memory</code>) for relevant information.</li>
<li>Injects the retrieved memories <em>into</em> the <code>chat_context</code>, often as a <code>SystemMessage</code>. This way, the LLM gets the benefit of the long-term memory <em>in addition</em> to the short-term conversation history, right before generating its response.</li>
</ul>
</li>
<li><b>Different Memory Implementations:</b> Just like there are different <code>ChatCompletionContext</code> strategies, there can be different <code>Memory</code> implementations:<ul>
<li><code>ListMemory</code>: A very simple memory that stores everything in a Python list (like a simple chronological notebook).</li>
<li><em>Future Possibilities</em>: More advanced implementations could use databases or vector stores for more efficient storage and retrieval of vast amounts of information.</li>
</ul>
</li>
</ol>
<h2><a class="anchor" id="autotoc_md772"></a>
Use Case Example: Remembering User Preferences with <code>ListMemory</code></h2>
<p>Let's implement our user preference use case using the simple <code>ListMemory</code>.</p>
<p><b>Goal:</b></p><ol type="1">
<li>Create a <code>ListMemory</code>.</li>
<li>Add a user preference ("formal style") to it.</li>
<li>Start a <em>new</em> chat context.</li>
<li>Use <code>update_context</code> to inject the preference into the new chat context.</li>
<li>Show how the chat context looks <em>before</em> being sent to the LLM.</li>
</ol>
<p><b>Step 1: Create the Memory</b></p>
<p>We'll use <code>ListMemory</code>, the simplest implementation provided by AutoGen Core.</p>
<div class="fragment"><div class="line"><span class="comment"># File: create_list_memory.py</span></div>
<div class="line"><span class="keyword">from</span> autogen_core.memory <span class="keyword">import</span> ListMemory</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create a simple list-based memory instance</span></div>
<div class="line">user_prefs_memory = ListMemory(name=<span class="stringliteral">&quot;user_preferences&quot;</span>)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Created memory: {user_prefs_memory.name}&quot;</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;Initial content: {user_prefs_memory.content}&quot;</span>)</div>
<div class="line"><span class="comment"># Output:</span></div>
<div class="line"><span class="comment"># Created memory: user_preferences</span></div>
<div class="line"><span class="comment"># Initial content: []</span></div>
</div><!-- fragment --><p> We have an empty memory notebook named "user_preferences".</p>
<p><b>Step 2: Add the Preference</b></p>
<p>Let's add the user's preference as a piece of text memory.</p>
<div class="fragment"><div class="line"><span class="comment"># File: add_preference.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> autogen_core.memory <span class="keyword">import</span> MemoryContent</div>
<div class="line"><span class="comment"># Assume user_prefs_memory exists from the previous step</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Define the preference as MemoryContent</span></div>
<div class="line">preference = MemoryContent(</div>
<div class="line">    content=<span class="stringliteral">&quot;User prefers all communication to be written in a formal style.&quot;</span>,</div>
<div class="line">    mime_type=<span class="stringliteral">&quot;text/plain&quot;</span>, <span class="comment"># It&#39;s just text</span></div>
<div class="line">    metadata={<span class="stringliteral">&quot;source&quot;</span>: <span class="stringliteral">&quot;user_instruction_conversation_1&quot;</span>} <span class="comment"># Optional info</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span>add_to_memory():</div>
<div class="line">    <span class="comment"># Add the content to our memory instance</span></div>
<div class="line">    await user_prefs_memory.add(preference)</div>
<div class="line">    print(f<span class="stringliteral">&quot;Memory content after adding: {user_prefs_memory.content}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">asyncio.run(add_to_memory())</div>
<div class="line"><span class="comment"># Output (will show the MemoryContent object):</span></div>
<div class="line"><span class="comment"># Memory content after adding: [MemoryContent(content=&#39;User prefers...&#39;, mime_type=&#39;text/plain&#39;, metadata={&#39;source&#39;: &#39;...&#39;})]</span></div>
</div><!-- fragment --><p> We've successfully written the preference into our <code>ListMemory</code> notebook.</p>
<p><b>Step 3: Start a New Chat Context</b></p>
<p>Imagine time passes, and the user starts a new conversation asking for an email draft. We create a fresh <code>ChatCompletionContext</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># File: start_new_chat.py</span></div>
<div class="line"><span class="keyword">from</span> autogen_core.model_context <span class="keyword">import</span> UnboundedChatCompletionContext</div>
<div class="line"><span class="keyword">from</span> autogen_core.models <span class="keyword">import</span> UserMessage</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Start a new, empty chat context for a new task</span></div>
<div class="line">new_chat_context = UnboundedChatCompletionContext()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Add the user&#39;s new request</span></div>
<div class="line">new_request = UserMessage(content=<span class="stringliteral">&quot;Draft an email to the team about the Q3 results.&quot;</span>, source=<span class="stringliteral">&quot;User&quot;</span>)</div>
<div class="line"><span class="comment"># await new_chat_context.add_message(new_request) # In a real app, add the request</span></div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;Created a new, empty chat context.&quot;</span>)</div>
<div class="line"><span class="comment"># Output: Created a new, empty chat context.</span></div>
</div><!-- fragment --><p> This context currently <em>doesn't</em> know about the "formal style" preference stored in our long-term memory.</p>
<p><b>Step 4: Inject Memory into Chat Context</b></p>
<p>Before sending the <code>new_chat_context</code> to the LLM, we use <code>update_context</code> to bring in relevant long-term memories.</p>
<div class="fragment"><div class="line"><span class="comment"># File: update_chat_with_memory.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="comment"># Assume user_prefs_memory exists (with the preference added)</span></div>
<div class="line"><span class="comment"># Assume new_chat_context exists (empty or with just the new request)</span></div>
<div class="line"><span class="comment"># Assume new_request exists</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># --- This is where Memory connects to Chat Context ---</span></div>
<div class="line">    print(<span class="stringliteral">&quot;Updating chat context with memory...&quot;</span>)</div>
<div class="line">    update_result = await user_prefs_memory.update_context(new_chat_context)</div>
<div class="line">    print(f<span class="stringliteral">&quot;Memories injected: {len(update_result.memories.results)}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Now let&#39;s add the actual user request for this task</span></div>
<div class="line">    await new_chat_context.add_message(new_request)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># See what messages are now in the context</span></div>
<div class="line">    messages_for_llm = await new_chat_context.get_messages()</div>
<div class="line">    print(<span class="stringliteral">&quot;\nMessages to be sent to LLM:&quot;</span>)</div>
<div class="line">    <span class="keywordflow">for</span> msg <span class="keywordflow">in</span> messages_for_llm:</div>
<div class="line">        print(f<span class="stringliteral">&quot;- [{msg.type}]: {msg.content}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Expected Output:</b> </p><div class="fragment"><div class="line">Updating chat context with memory...</div>
<div class="line">Memories injected: 1</div>
<div class="line"> </div>
<div class="line">Messages to be sent to LLM:</div>
<div class="line">- [SystemMessage]:</div>
<div class="line">Relevant memory content (in chronological order):</div>
<div class="line">1. User prefers all communication to be written in a formal style.</div>
<div class="line"> </div>
<div class="line">- [UserMessage]: Draft an email to the team about the Q3 results.</div>
</div><!-- fragment --><p> Look! The <code>ListMemory.update_context</code> method automatically queried the memory (in this simple case, it just takes <em>all</em> entries) and added a <code>SystemMessage</code> to the <code>new_chat_context</code>. This message explicitly tells the LLM about the stored preference <em>before</em> it sees the user's request to draft the email.</p>
<p><b>Step 5: (Conceptual) Sending to LLM</b></p>
<p>Now, if we were to send <code>messages_for_llm</code> to the <code>ChatCompletionClient</code> (Chapter 5):</p>
<div class="fragment"><div class="line"><span class="comment"># Conceptual code - Requires a configured client</span></div>
<div class="line"><span class="comment"># response = await llm_client.create(messages=messages_for_llm)</span></div>
</div><!-- fragment --><p> The LLM would receive both the instruction about the formal style preference (from Memory) and the request to draft the email. It's much more likely to follow the preference now!</p>
<p><b>Step 6: Direct Query (Optional)</b></p>
<p>We can also directly query the memory if needed, without involving a chat context.</p>
<div class="fragment"><div class="line"><span class="comment"># File: query_memory.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="comment"># Assume user_prefs_memory exists</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    <span class="comment"># Query the memory (ListMemory returns all items regardless of query text)</span></div>
<div class="line">    query_result = await user_prefs_memory.query(<span class="stringliteral">&quot;style preference&quot;</span>)</div>
<div class="line">    print(<span class="stringliteral">&quot;\nDirect query result:&quot;</span>)</div>
<div class="line">    <span class="keywordflow">for</span> item <span class="keywordflow">in</span> query_result.results:</div>
<div class="line">        print(f<span class="stringliteral">&quot;- Content: {item.content}, Type: {item.mime_type}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="line"><span class="comment"># Output:</span></div>
<div class="line"><span class="comment"># Direct query result:</span></div>
<div class="line"><span class="comment"># - Content: User prefers all communication to be written in a formal style., Type: text/plain</span></div>
</div><!-- fragment --><p> This shows how an agent could specifically look things up in its notebook.</p>
<h2><a class="anchor" id="autotoc_md773"></a>
Under the Hood: How <code>ListMemory</code> Injects Context</h2>
<p>Let's trace the <code>update_context</code> call for <code>ListMemory</code>.</p>
<p><b>Conceptual Flow:</b></p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant AgentLogic as Agent Logic</div>
<div class="line">    participant ListMem as ListMemory</div>
<div class="line">    participant InternalList as Memory&#39;s Internal List</div>
<div class="line">    participant ChatCtx as ChatCompletionContext</div>
<div class="line"> </div>
<div class="line">    AgentLogic-&gt;&gt;+ListMem: update_context(chat_context)</div>
<div class="line">    ListMem-&gt;&gt;+InternalList: Get all stored MemoryContent items</div>
<div class="line">    InternalList--&gt;&gt;-ListMem: Return list of [pref_content]</div>
<div class="line">    alt Memory list is NOT empty</div>
<div class="line">        ListMem-&gt;&gt;ListMem: Format memories into a single string (e.g., &quot;1. pref_content&quot;)</div>
<div class="line">        ListMem-&gt;&gt;ListMem: Create SystemMessage with formatted string</div>
<div class="line">        ListMem-&gt;&gt;+ChatCtx: add_message(SystemMessage)</div>
<div class="line">        ChatCtx--&gt;&gt;-ListMem: Context updated</div>
<div class="line">    end</div>
<div class="line">    ListMem-&gt;&gt;ListMem: Create UpdateContextResult(memories=[pref_content])</div>
<div class="line">    ListMem--&gt;&gt;-AgentLogic: Return UpdateContextResult</div>
</div><!-- fragment --><ol type="1">
<li>The agent calls <code>user_prefs_memory.update_context(new_chat_context)</code>.</li>
<li>The <code>ListMemory</code> instance accesses its internal <code>_contents</code> list.</li>
<li>It checks if the list is empty. If not:</li>
<li>It iterates through the <code>MemoryContent</code> items in the list.</li>
<li>It formats them into a numbered string (like "Relevant memory content...\n1. Item 1\n2. Item 2...").</li>
<li>It creates a single <code>SystemMessage</code> containing this formatted string.</li>
<li>It calls <code>new_chat_context.add_message()</code> to add this <code>SystemMessage</code> to the chat history that will be sent to the LLM.</li>
<li>It returns an <code>UpdateContextResult</code> containing the list of memories it just processed.</li>
</ol>
<p><b>Code Glimpse:</b></p>
<ul>
<li><p class="startli"><b><code>Memory</code> Protocol (<code>memory/_base_memory.py</code>):</b> Defines the required methods for any memory implementation.</p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md774"></a>
From: memory/_base_memory.py (Simplified ABC)</h1>
<p>from abc import ABC, abstractmethod </p>
<h1><a class="anchor" id="autotoc_md775"></a>
... other imports: MemoryContent, MemoryQueryResult, UpdateContextResult, ChatCompletionContext</h1>
<p>class Memory(ABC): component_type = "memory"</p>
<p>@abstractmethod async def update_context(self, model_context: ChatCompletionContext) -&gt; UpdateContextResult: ...</p>
<p>@abstractmethod async def query(self, query: str | MemoryContent, ...) -&gt; MemoryQueryResult: ...</p>
<p>@abstractmethod async def add(self, content: MemoryContent, ...) -&gt; None: ...</p>
<p>@abstractmethod async def clear(self) -&gt; None: ...</p>
<p>@abstractmethod async def close(self) -&gt; None: ... ``` Any class wanting to act as Memory must provide these methods.</p>
<ul>
<li><p class="startli"><b><code>ListMemory</code> Implementation (<code>memory/_list_memory.py</code>):</b></p>
<p class="startli">```python </p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md776"></a>
From: memory/_list_memory.py (Simplified)</h1>
<p>from typing import List </p>
<h1><a class="anchor" id="autotoc_md777"></a>
... other imports: Memory, MemoryContent, ..., SystemMessage, ChatCompletionContext</h1>
<p>class ListMemory(Memory): def <b>init</b>(self, ..., memory_contents: List[MemoryContent] | None = None): </p>
<h1><a class="anchor" id="autotoc_md778"></a>
Stores memory items in a simple list</h1>
<p>self._contents: List[MemoryContent] = memory_contents or []</p>
<p>async def add(self, content: MemoryContent, ...) -&gt; None: """Add new content to the internal list.""" self._contents.append(content)</p>
<p>async def query(self, query: str | MemoryContent = "", ...) -&gt; MemoryQueryResult: """Return all memories, ignoring the query.""" </p>
<h1><a class="anchor" id="autotoc_md779"></a>
Simple implementation: just return everything</h1>
<p>return MemoryQueryResult(results=self._contents)</p>
<p>async def update_context(self, model_context: ChatCompletionContext) -&gt; UpdateContextResult: """Add all memories as a SystemMessage to the chat context.""" if not self._contents: # Do nothing if memory is empty return UpdateContextResult(memories=MemoryQueryResult(results=[]))</p>
<h1><a class="anchor" id="autotoc_md780"></a>
Format all memories into a numbered list string</h1>
<p>memory_strings = [f"{i}. {str(mem.content)}" for i, mem in enumerate(self._contents, 1)] memory_context_str = "Relevant memory content...\n" + "\n".join(memory_strings) + "\n"</p>
<h1><a class="anchor" id="autotoc_md781"></a>
Add this string as a SystemMessage to the provided chat context</h1>
<p>await model_context.add_message(SystemMessage(content=memory_context_str))</p>
<h1><a class="anchor" id="autotoc_md782"></a>
Return info about which memories were added</h1>
<p>return UpdateContextResult(memories=MemoryQueryResult(results=self._contents))</p>
<h1><a class="anchor" id="autotoc_md783"></a>
... clear(), close(), config methods ...</h1>
<p>``<code> This shows the straightforward logic of</code>ListMemory<code>: store in a list, retrieve the whole list, and inject the whole list as a single system message into the chat context. More complex memories might use smarter retrieval (e.g., based on the</code>query<code>in</code>query()<code>or the last message in</code>update_context`) and inject memories differently.</p>
<h2><a class="anchor" id="autotoc_md784"></a>
Next Steps</h2>
<p>You've learned about <code>Memory</code>, AutoGen Core's mechanism for giving agents long-term recall beyond the immediate conversation (<code>ChatCompletionContext</code>). We saw how <code>MemoryContent</code> holds information, <code>add</code> stores it, <code>query</code> retrieves it, and <code>update_context</code> injects relevant memories into the LLM's working context. We explored the simple <code>ListMemory</code> as a basic example.</p>
<p>Memory systems are crucial for agents that learn, adapt, or need to maintain state across interactions.</p>
<p>This concludes our deep dive into the core abstractions of AutoGen Core! We've covered Agents, Messaging, Runtime, Tools, LLM Clients, Chat Context, and now Memory. There's one final concept that ties many of these together from a configuration perspective:</p>
<ul>
<li><a class="el" href="../../dc/d8f/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2AutoGen_01Core_208__component.html">Chapter 8: Component</a>: Understand the general <code>Component</code> model in AutoGen Core, how it allows pieces like <code>Memory</code>, <code>ChatCompletionContext</code>, and <code>ChatCompletionClient</code> to be configured and managed consistently.</li>
</ul>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
