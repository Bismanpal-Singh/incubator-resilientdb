#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 06_llm</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('da/d0f/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_206__llm.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">06_llm</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1547"></a>
autotoc_md1547</h2>
<p>layout: default title: "LLM" parent: "CrewAI" </p>
<h2><a class="anchor" id="autotoc_md1548"></a>
nav_order: 6</h2>
<h1><a class="anchor" id="autotoc_md1549"></a>
Chapter 6: LLM - The Agent's Brain</h1>
<p>In the <a class="el" href="../../de/d7d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_205__process.html">previous chapter</a>, we explored the <code>Process</code> - how the <code>Crew</code> organizes the workflow for its <code>Agent</code>s, deciding whether they work sequentially or are managed hierarchically. We now have specialized agents (<a class="el" href="../../d2/d55/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_202__agent.html">Agent</a>), defined work (<a class="el" href="../../d1/dae/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_203__task.html">Task</a>), useful abilities (<a class="el" href="../../d0/dbf/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_204__tool.html">Tool</a>), and a workflow strategy (<a class="el" href="../../de/d7d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_205__process.html">Process</a>).</p>
<p>But what actually does the <em>thinking</em> inside an agent? When we give the 'Travel Researcher' agent the task "Find sunny European cities," what part of the agent understands this request, decides to use the search tool, interprets the results, and writes the final list?</p>
<p>This core thinking component is the <b>Large Language Model</b>, or <b>LLM</b>.</p>
<h2><a class="anchor" id="autotoc_md1550"></a>
Why Do Agents Need an LLM?</h2>
<p>Imagine our 'Travel Researcher' agent again. It has a <code>role</code>, <code>goal</code>, and <code>backstory</code>. It has a <code>Task</code> to complete and maybe a <code>Tool</code> to search the web. But it needs something to:</p>
<ol type="1">
<li><b>Understand:</b> Read the task description, its own role/goal, and any context from previous tasks.</li>
<li><b>Reason:</b> Figure out a plan. "Okay, I need sunny cities. My description says I'm an expert. The task asks for 3. I should use the search tool to get current info."</li>
<li><b>Act:</b> Decide <em>when</em> to use a tool and <em>what</em> input to give it (e.g., formulate the search query).</li>
<li><b>Generate:</b> Take the information (search results, its own knowledge) and write the final output in the expected format.</li>
</ol>
<p>The LLM is the engine that performs all these cognitive actions. It's the "brain" that drives the agent's behavior based on the instructions and tools provided.</p>
<p><b>Problem Solved:</b> The LLM provides the core intelligence for each <code>Agent</code>. It processes language, makes decisions (like which tool to use or what text to generate), and ultimately enables the agent to perform its assigned <code>Task</code> based on its defined profile.</p>
<h2><a class="anchor" id="autotoc_md1551"></a>
What is an LLM in CrewAI?</h2>
<p>Think of an LLM as a highly advanced, versatile AI assistant you can interact with using text. Models like OpenAI's GPT-4, Google's Gemini, Anthropic's Claude, or open-source models run locally via tools like Ollama are all examples of LLMs. They are trained on vast amounts of text data and can understand instructions, answer questions, write text, summarize information, and even make logical deductions.</p>
<p>In CrewAI, the <code>LLM</code> concept is an <b>abstraction</b>. CrewAI itself doesn't <em>include</em> these massive language models. Instead, it provides a standardized way to <b>connect to and interact with</b> various LLMs, whether they are hosted by companies like OpenAI or run on your own computer.</p>
<p><b>How CrewAI Handles LLMs:</b></p>
<ul>
<li><b><code>litellm</code> Integration:</b> CrewAI uses a fantastic library called <code>litellm</code> under the hood. <code>litellm</code> acts like a universal translator, allowing CrewAI to talk to over 100 different LLM providers (OpenAI, Azure OpenAI, Gemini, Anthropic, Ollama, Hugging Face, etc.) using a consistent interface. This means you can easily switch the "brain" of your agents without rewriting large parts of your code.</li>
<li><b>Standard Interface:</b> The CrewAI <code>LLM</code> abstraction (often represented by helper classes or configuration settings) simplifies how you specify which model to use and how it should behave. It handles common parameters like:<ul>
<li><code>model</code>: The specific name of the LLM you want to use (e.g., <code>"gpt-4o"</code>, <code>"ollama/llama3"</code>, <code>"gemini-pro"</code>).</li>
<li><code>temperature</code>: Controls the randomness (creativity) of the output. Lower values (e.g., 0.1) make the output more deterministic and focused, while higher values (e.g., 0.8) make it more creative but potentially less factual.</li>
<li><code>max_tokens</code>: The maximum number of words (tokens) the LLM should generate in its response.</li>
</ul>
</li>
<li><b>API Management:</b> It manages the technical details of sending requests to the chosen LLM provider and receiving the responses.</li>
</ul>
<p>Essentially, CrewAI lets you plug in the LLM brain of your choice for your agents.</p>
<h2><a class="anchor" id="autotoc_md1552"></a>
Configuring an LLM for Your Crew</h2>
<p>You need to tell CrewAI which LLM(s) your agents should use. There are several ways to do this, ranging from letting CrewAI detect settings automatically to explicitly configuring specific models.</p>
<p><b>1. Automatic Detection (Environment Variables)</b></p>
<p>Often the easiest way for common models like OpenAI's is to set environment variables. CrewAI (via <code>litellm</code>) can pick these up automatically.</p>
<p>If you set these in your system or a <code>.env</code> file:</p>
<div class="fragment"><div class="line"># Example .env file</div>
<div class="line">OPENAI_API_KEY=&quot;sk-your_openai_api_key_here&quot;</div>
<div class="line"># Optional: Specify the model, otherwise it uses a default like gpt-4o</div>
<div class="line">OPENAI_MODEL_NAME=&quot;gpt-4o&quot;</div>
</div><!-- fragment --><p>Then, often you don't need to specify the LLM explicitly in your code:</p>
<div class="fragment"><div class="line"><span class="comment"># agent.py (simplified)</span></div>
<div class="line"><span class="keyword">from</span> crewai <span class="keyword">import</span> Agent</div>
<div class="line"> </div>
<div class="line"><span class="comment"># If OPENAI_API_KEY and OPENAI_MODEL_NAME are set in the environment,</span></div>
<div class="line"><span class="comment"># CrewAI might automatically configure an OpenAI LLM for this agent.</span></div>
<div class="line">researcher = Agent(</div>
<div class="line">    role=<span class="stringliteral">&#39;Travel Researcher&#39;</span>,</div>
<div class="line">    goal=<span class="stringliteral">&#39;Find interesting cities in Europe&#39;</span>,</div>
<div class="line">    backstory=<span class="stringliteral">&#39;Expert researcher.&#39;</span>,</div>
<div class="line">    <span class="comment"># No &#39;llm=&#39; parameter needed here if env vars are set</span></div>
<div class="line">)</div>
</div><!-- fragment --><p><b>2. Explicit Configuration (Recommended for Clarity)</b></p>
<p>It's usually better to be explicit about which LLM you want to use. CrewAI integrates well with LangChain's LLM wrappers, which are commonly used.</p>
<p><b>Example: Using OpenAI (GPT-4o)</b></p>
<div class="fragment"><div class="line"><span class="comment"># Make sure you have langchain_openai installed: pip install langchain-openai</span></div>
<div class="line"><span class="keyword">import</span> os</div>
<div class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</div>
<div class="line"><span class="keyword">from</span> crewai <span class="keyword">import</span> Agent</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Set the API key (best practice: use environment variables)</span></div>
<div class="line"><span class="comment"># os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;sk-your_key_here&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Instantiate the OpenAI LLM wrapper</span></div>
<div class="line">openai_llm = ChatOpenAI(model=<span class="stringliteral">&quot;gpt-4o&quot;</span>, temperature=0.7)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Pass the configured LLM to the Agent</span></div>
<div class="line">researcher = Agent(</div>
<div class="line">    role=<span class="stringliteral">&#39;Travel Researcher&#39;</span>,</div>
<div class="line">    goal=<span class="stringliteral">&#39;Find interesting cities in Europe&#39;</span>,</div>
<div class="line">    backstory=<span class="stringliteral">&#39;Expert researcher.&#39;</span>,</div>
<div class="line">    llm=openai_llm <span class="comment"># Explicitly assign the LLM</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># You can also assign a default LLM to the Crew</span></div>
<div class="line"><span class="comment"># from crewai import Crew</span></div>
<div class="line"><span class="comment"># trip_crew = Crew(</span></div>
<div class="line"><span class="comment">#   agents=[researcher],</span></div>
<div class="line"><span class="comment">#   tasks=[...],</span></div>
<div class="line"><span class="comment">#   # Manager LLM for hierarchical process</span></div>
<div class="line"><span class="comment">#   manager_llm=openai_llm</span></div>
<div class="line"><span class="comment">#   # A function_calling_llm can also be set for tool use reasoning</span></div>
<div class="line"><span class="comment">#   # function_calling_llm=openai_llm</span></div>
<div class="line"><span class="comment"># )</span></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ul>
<li>We import <code>ChatOpenAI</code> from <code>langchain_openai</code>.</li>
<li>We create an instance, specifying the <code>model</code> name and optionally other parameters like <code>temperature</code>.</li>
<li>We pass this <code>openai_llm</code> object to the <code>llm</code> parameter when creating the <code>Agent</code>. This agent will now use GPT-4o for its thinking.</li>
<li>You can also assign LLMs at the <code>Crew</code> level, especially the <code>manager_llm</code> for hierarchical processes or a default <code>function_calling_llm</code> which helps agents decide <em>which</em> tool to use.</li>
</ul>
<p><b>Example: Using a Local Model via Ollama (Llama 3)</b></p>
<p>If you have Ollama running locally with a model like Llama 3 pulled (<code>ollama pull llama3</code>):</p>
<div class="fragment"><div class="line"><span class="comment"># Make sure you have langchain_community installed: pip install langchain-community</span></div>
<div class="line"><span class="keyword">from</span> langchain_community.llms <span class="keyword">import</span> Ollama</div>
<div class="line"><span class="keyword">from</span> crewai <span class="keyword">import</span> Agent</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Instantiate the Ollama LLM wrapper</span></div>
<div class="line"><span class="comment"># Make sure Ollama server is running!</span></div>
<div class="line">ollama_llm = Ollama(model=<span class="stringliteral">&quot;llama3&quot;</span>, base_url=<span class="stringliteral">&quot;http://localhost:11434&quot;</span>)</div>
<div class="line"><span class="comment"># temperature, etc. can also be set if supported by the model/wrapper</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Pass the configured LLM to the Agent</span></div>
<div class="line">local_researcher = Agent(</div>
<div class="line">    role=<span class="stringliteral">&#39;Travel Researcher&#39;</span>,</div>
<div class="line">    goal=<span class="stringliteral">&#39;Find interesting cities in Europe&#39;</span>,</div>
<div class="line">    backstory=<span class="stringliteral">&#39;Expert researcher.&#39;</span>,</div>
<div class="line">    llm=ollama_llm <span class="comment"># Use the local Llama 3 model</span></div>
<div class="line">)</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ul>
<li>We import <code>Ollama</code> from <code>langchain_community.llms</code>.</li>
<li>We create an instance, specifying the <code>model</code> name ("llama3" in this case, assuming it's available in your Ollama setup) and the <code>base_url</code> where your Ollama server is running.</li>
<li>We pass <code>ollama_llm</code> to the <code>Agent</code>. Now, this agent's "brain" runs entirely on your local machine!</li>
</ul>
<p><b>CrewAI's <code>LLM</code> Class (Advanced/Direct <code>litellm</code> Usage)</b></p>
<p>CrewAI also provides its own <code>LLM</code> class (<code>from crewai import LLM</code>) which allows more direct configuration using <code>litellm</code> parameters. This is less common for beginners than using the LangChain wrappers shown above, but offers fine-grained control.</p>
<p><b>Passing LLMs to the Crew</b></p>
<p>Besides assigning an LLM to each agent individually, you can set defaults or specific roles at the <code>Crew</code> level:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> crewai <span class="keyword">import</span> Crew, Process</div>
<div class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Assume agents &#39;researcher&#39;, &#39;planner&#39; and tasks &#39;task1&#39;, &#39;task2&#39; are defined</span></div>
<div class="line"> </div>
<div class="line">openai_llm = ChatOpenAI(model=<span class="stringliteral">&quot;gpt-4o&quot;</span>)</div>
<div class="line">fast_llm = ChatOpenAI(model=<span class="stringliteral">&quot;gpt-3.5-turbo&quot;</span>) <span class="comment"># Maybe a faster/cheaper model</span></div>
<div class="line"> </div>
<div class="line">trip_crew = Crew(</div>
<div class="line">    agents=[researcher, planner], <span class="comment"># Agents might have their own LLMs assigned too</span></div>
<div class="line">    tasks=[task1, task2],</div>
<div class="line">    process=Process.hierarchical,</div>
<div class="line">    <span class="comment"># The Manager agent will use gpt-4o</span></div>
<div class="line">    manager_llm=openai_llm,</div>
<div class="line">    <span class="comment"># Use gpt-3.5-turbo specifically for deciding which tool to use (can save costs)</span></div>
<div class="line">    function_calling_llm=fast_llm</div>
<div class="line">)</div>
</div><!-- fragment --><ul>
<li><code>manager_llm</code>: Specifies the brain for the manager agent in a hierarchical process.</li>
<li><code>function_calling_llm</code>: Specifies the LLM used by agents primarily to decide <em>which tool to call</em> and <em>with what arguments</em>. This can sometimes be a faster/cheaper model than the one used for generating the final detailed response. If not set, agents typically use their main <code>llm</code>.</li>
</ul>
<p>If an agent doesn't have an <code>llm</code> explicitly assigned, it might inherit the <code>function_calling_llm</code> or default to environment settings. It's usually clearest to assign LLMs explicitly where needed.</p>
<h2><a class="anchor" id="autotoc_md1553"></a>
How LLM Interaction Works Internally</h2>
<p>When an <a class="el" href="../../d2/d55/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_202__agent.html">Agent</a> needs to think (e.g., execute a <a class="el" href="../../d1/dae/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_203__task.html">Task</a>), the process looks like this:</p>
<ol type="1">
<li><b>Prompt Assembly:</b> The <code>Agent</code> gathers all relevant information: its <code>role</code>, <code>goal</code>, <code>backstory</code>, the <code>Task</code> description, <code>expected_output</code>, any <code>context</code> from previous tasks, and the descriptions of its available <code>Tool</code>s. It assembles this into a detailed prompt.</li>
<li><b>LLM Object Call:</b> The <code>Agent</code> passes this prompt to its configured <code>LLM</code> object (e.g., the <code>ChatOpenAI</code> instance or the <code>Ollama</code> instance we created).</li>
<li><b><code>litellm</code> Invocation:</b> The CrewAI/LangChain <code>LLM</code> object uses <code>litellm</code>'s <code>completion</code> function, passing the assembled prompt (formatted as messages), the target <code>model</code> name, and other parameters (<code>temperature</code>, <code>max_tokens</code>, <code>tools</code>, etc.).</li>
<li><b>API Request:</b> <code>litellm</code> handles the specifics of communicating with the target LLM's API (e.g., sending a request to OpenAI's API endpoint or the local Ollama server).</li>
<li><b>LLM Processing:</b> The actual LLM (GPT-4, Llama 3, etc.) processes the request.</li>
<li><b>API Response:</b> The LLM provider sends back the response (which could be generated text or a decision to use a specific tool with certain arguments).</li>
<li><b><code>litellm</code> Response Handling:</b> <code>litellm</code> receives the API response and standardizes it.</li>
<li><b>LLM Object Response:</b> The <code>LLM</code> object receives the standardized response from <code>litellm</code>.</li>
<li><b>Result to Agent:</b> The <code>LLM</code> object returns the result (text or tool call information) back to the <code>Agent</code>.</li>
<li><b>Agent Action:</b> The <code>Agent</code> then either uses the generated text as its output or, if the LLM decided to use a tool, it executes the specified tool.</li>
</ol>
<p>Let's visualize this:</p>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant Agent</div>
<div class="line">    participant LLM_Object as LLM Object (e.g., ChatOpenAI)</div>
<div class="line">    participant LiteLLM</div>
<div class="line">    participant ProviderAPI as Actual LLM API (e.g., OpenAI)</div>
<div class="line"> </div>
<div class="line">    Agent-&gt;&gt;Agent: Assemble Prompt (Role, Goal, Task, Tools...)</div>
<div class="line">    Agent-&gt;&gt;LLM_Object: call(prompt, tools_schema)</div>
<div class="line">    LLM_Object-&gt;&gt;LiteLLM: litellm.completion(model, messages, ...)</div>
<div class="line">    LiteLLM-&gt;&gt;ProviderAPI: Send API Request</div>
<div class="line">    ProviderAPI--&gt;&gt;LiteLLM: Receive API Response (text or tool_call)</div>
<div class="line">    LiteLLM--&gt;&gt;LLM_Object: Standardized Response</div>
<div class="line">    LLM_Object--&gt;&gt;Agent: Result (text or tool_call)</div>
<div class="line">    Agent-&gt;&gt;Agent: Process Result (Output text or Execute tool)</div>
</div><!-- fragment --><p><b>Diving into the Code (<code>llm.py</code>, <code>utilities/llm_utils.py</code>)</b></p>
<p>The primary logic resides in <code>crewai/llm.py</code> and the helper <code>crewai/utilities/llm_utils.py</code>.</p>
<ul>
<li><b><code>crewai/utilities/llm_utils.py</code>:</b> The <code>create_llm</code> function is key. It handles the logic of figuring out which LLM to instantiate based on environment variables, direct <code>LLM</code> object input, or string names. It tries to create an <code>LLM</code> instance.</li>
<li><b><code>crewai/llm.py</code>:</b><ul>
<li>The <code>LLM</code> class itself holds the configuration (<code>model</code>, <code>temperature</code>, etc.).</li>
<li>The <code>call</code> method is the main entry point. It takes the <code>messages</code> (the prompt) and optional <code>tools</code>.</li>
<li>It calls <code>_prepare_completion_params</code> to format the request parameters based on the LLM's requirements and the provided configuration.</li>
<li>Crucially, it then calls <code>litellm.completion(**params)</code>. This is where the magic happens â€“ <code>litellm</code> takes over communication with the actual LLM API.</li>
<li>It handles the response from <code>litellm</code>, checking for text content or tool calls (<code>_handle_non_streaming_response</code> or <code>_handle_streaming_response</code>).</li>
<li>It uses helper methods like <code>_format_messages_for_provider</code> to deal with quirks of different LLMs (like Anthropic needing a 'user' message first).</li>
</ul>
</li>
</ul>
<div class="fragment"><div class="line"><span class="comment"># Simplified view from crewai/llm.py</span></div>
<div class="line"> </div>
<div class="line"><span class="comment"># Import litellm and other necessary modules</span></div>
<div class="line"><span class="keyword">import</span> litellm</div>
<div class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Dict, Optional, Union, Any</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>LLM:</div>
<div class="line">    <span class="keyword">def </span>__init__(self, model: str, temperature: Optional[float] = 0.7, **kwargs):</div>
<div class="line">        self.model = model</div>
<div class="line">        self.temperature = temperature</div>
<div class="line">        <span class="comment"># ... store other parameters like max_tokens, api_key, base_url ...</span></div>
<div class="line">        self.additional_params = kwargs</div>
<div class="line">        self.stream = <span class="keyword">False</span> <span class="comment"># Default to non-streaming</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>_prepare_completion_params(self, messages, tools=None) -&gt; Dict[str, Any]:</div>
<div class="line">        <span class="comment"># Formats messages based on provider (e.g., Anthropic)</span></div>
<div class="line">        formatted_messages = self._format_messages_for_provider(messages)</div>
<div class="line"> </div>
<div class="line">        params = {</div>
<div class="line">            <span class="stringliteral">&quot;model&quot;</span>: self.model,</div>
<div class="line">            <span class="stringliteral">&quot;messages&quot;</span>: formatted_messages,</div>
<div class="line">            <span class="stringliteral">&quot;temperature&quot;</span>: self.temperature,</div>
<div class="line">            <span class="stringliteral">&quot;tools&quot;</span>: tools,</div>
<div class="line">            <span class="stringliteral">&quot;stream&quot;</span>: self.stream,</div>
<div class="line">            <span class="comment"># ... add other stored parameters (max_tokens, api_key etc.) ...</span></div>
<div class="line">            **self.additional_params,</div>
<div class="line">        }</div>
<div class="line">        <span class="comment"># Remove None values</span></div>
<div class="line">        <span class="keywordflow">return</span> {k: v <span class="keywordflow">for</span> k, v <span class="keywordflow">in</span> params.items() <span class="keywordflow">if</span> v <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>}</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>call(self, messages, tools=None, callbacks=None, available_functions=None) -&gt; Union[str, Any]:</div>
<div class="line">        <span class="comment"># ... (emit start event, validate params) ...</span></div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">try</span>:</div>
<div class="line">            <span class="comment"># Prepare the parameters for litellm</span></div>
<div class="line">            params = self._prepare_completion_params(messages, tools)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Decide whether to stream or not (simplified here)</span></div>
<div class="line">            <span class="keywordflow">if</span> self.stream:</div>
<div class="line">                 <span class="comment"># Handles chunk processing, tool calls from stream end</span></div>
<div class="line">                <span class="keywordflow">return</span> self._handle_streaming_response(params, callbacks, available_functions)</div>
<div class="line">            <span class="keywordflow">else</span>:</div>
<div class="line">                 <span class="comment"># Makes single call, handles tool calls from response</span></div>
<div class="line">                <span class="keywordflow">return</span> self._handle_non_streaming_response(params, callbacks, available_functions)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">            <span class="comment"># ... (emit failure event, handle exceptions like context window exceeded) ...</span></div>
<div class="line">            <span class="keywordflow">raise</span> e</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>_handle_non_streaming_response(self, params, callbacks, available_functions):</div>
<div class="line">         <span class="comment"># THE CORE CALL TO LITELLM</span></div>
<div class="line">        response = litellm.completion(**params)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Extract text content</span></div>
<div class="line">        text_response = response.choices[0].message.content <span class="keywordflow">or</span> <span class="stringliteral">&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Check for tool calls in the response</span></div>
<div class="line">        tool_calls = getattr(response.choices[0].message, <span class="stringliteral">&quot;tool_calls&quot;</span>, [])</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">if</span> <span class="keywordflow">not</span> tool_calls <span class="keywordflow">or</span> <span class="keywordflow">not</span> available_functions:</div>
<div class="line">            <span class="comment"># ... (emit success event) ...</span></div>
<div class="line">            <span class="keywordflow">return</span> text_response <span class="comment"># Return plain text</span></div>
<div class="line">        <span class="keywordflow">else</span>:</div>
<div class="line">            <span class="comment"># Handle the tool call (runs the actual function)</span></div>
<div class="line">            tool_result = self._handle_tool_call(tool_calls, available_functions)</div>
<div class="line">            <span class="keywordflow">if</span> tool_result <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line">                <span class="keywordflow">return</span> tool_result <span class="comment"># Return tool output</span></div>
<div class="line">            <span class="keywordflow">else</span>:</div>
<div class="line">                 <span class="comment"># ... (emit success event for text if tool failed?) ...</span></div>
<div class="line">                <span class="keywordflow">return</span> text_response <span class="comment"># Fallback to text if tool fails</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>_handle_tool_call(self, tool_calls, available_functions):</div>
<div class="line">        <span class="comment"># Extracts function name and args from tool_calls[0]</span></div>
<div class="line">        <span class="comment"># Looks up function in available_functions</span></div>
<div class="line">        <span class="comment"># Executes the function with args</span></div>
<div class="line">        <span class="comment"># Returns the result</span></div>
<div class="line">        <span class="comment"># ... (error handling) ...</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">def </span>_format_messages_for_provider(self, messages):</div>
<div class="line">        <span class="comment"># Handles provider-specific message formatting rules</span></div>
<div class="line">        <span class="comment"># (e.g., ensuring Anthropic starts with &#39;user&#39; role)</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># ... other methods like _handle_streaming_response ...</span></div>
</div><!-- fragment --><p>This simplified view shows how the <code>LLM</code> class acts as a wrapper around <code>litellm</code>, preparing requests and processing responses, shielding the rest of CrewAI from the complexities of different LLM APIs.</p>
<h2><a class="anchor" id="autotoc_md1554"></a>
Conclusion</h2>
<p>You've learned about the <b>LLM</b>, the essential "brain" powering your CrewAI <a class="el" href="../../d2/d55/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_202__agent.html">Agent</a>s. It's the component that understands language, reasons about tasks, decides on actions (like using <a class="el" href="../../d0/dbf/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_204__tool.html">Tool</a>s), and generates text.</p>
<p>We saw that CrewAI uses the <code>litellm</code> library to provide a flexible way to connect to a wide variety of LLM providers (like OpenAI, Google Gemini, Anthropic Claude, or local models via Ollama). You can configure which LLM your agents or crew use, either implicitly through environment variables or explicitly by passing configured LLM objects (often using LangChain wrappers) during <code>Agent</code> or <code>Crew</code> creation.</p>
<p>This abstraction makes CrewAI powerful, allowing you to experiment with different models to find the best fit for your specific needs and budget.</p>
<p>But sometimes, agents need to remember things from past interactions or previous tasks within the same run. How does CrewAI handle short-term and potentially long-term memory? Let's explore that in the next chapter!</p>
<p><b>Next:</b> <a class="el" href="../../db/de6/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2CrewAI_207__memory.html">Chapter 7: Memory - Giving Agents Recall</a></p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
