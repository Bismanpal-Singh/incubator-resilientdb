#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ResilientDB: 10_basedispatcher</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen_html_style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../logo.png"/></td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d6/d6d/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_210__basedispatcher.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">10_basedispatcher</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md1456"></a>
autotoc_md1456</h2>
<p>layout: default title: "BaseDispatcher" parent: "Crawl4AI" </p>
<h2><a class="anchor" id="autotoc_md1457"></a>
nav_order: 10</h2>
<h1><a class="anchor" id="autotoc_md1458"></a>
Chapter 10: Orchestrating the Crawl - BaseDispatcher</h1>
<p>In <a class="el" href="../../d1/dcd/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_209__cachecontext______cachemode.html">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>, we learned how Crawl4AI uses caching to cleverly avoid re-fetching the same webpage multiple times, which is especially helpful when crawling many URLs. We've also seen how methods like <code>arun_many()</code> (<a class="el" href="../../d8/dc9/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_202__asyncwebcrawler.html">Chapter 2: Meet the General Manager - AsyncWebCrawler</a>) or strategies like <a class="el" href="../../d3/da3/md__2github_2workspace_2ecosystem_2pocketflow_2docs_2Crawl4AI_208__deepcrawlstrategy.html">DeepCrawlStrategy</a> can lead to potentially hundreds or thousands of individual URLs needing to be crawled.</p>
<p>This raises a question: if we have 1000 URLs to crawl, does Crawl4AI try to crawl all 1000 simultaneously? That would likely overwhelm your computer's resources (like memory and CPU) and could also flood the target website with too many requests, potentially getting you blocked! How does Crawl4AI manage running many crawls efficiently and responsibly?</p>
<h2><a class="anchor" id="autotoc_md1459"></a>
What Problem Does <code>BaseDispatcher</code> Solve?</h2>
<p>Imagine you're managing a fleet of delivery drones (<code>AsyncWebCrawler</code> tasks) that need to pick up packages from many different addresses (URLs). If you launch all 1000 drones at the exact same moment:</p>
<ul>
<li>Your control station (your computer) might crash due to the processing load.</li>
<li>The central warehouse (the target website) might get overwhelmed by simultaneous arrivals.</li>
<li>Some drones might collide or interfere with each other.</li>
</ul>
<p>You need a <b>Traffic Controller</b> or a <b>Dispatch Center</b> to manage the fleet. This controller decides:</p>
<ol type="1">
<li>How many drones can be active in the air at any one time.</li>
<li>When to launch the next drone, maybe based on available airspace (system resources) or just a simple count limit.</li>
<li>How to handle potential delays or issues (like rate limiting from a specific website).</li>
</ol>
<p>In Crawl4AI, the <code>BaseDispatcher</code> acts as this <b>Traffic Controller</b> or <b>Task Scheduler</b> for concurrent crawling operations, primarily when using <code>arun_many()</code>. It manages <em>how</em> multiple crawl tasks are executed concurrently, ensuring the process is efficient without overwhelming your system or the target websites.</p>
<h2><a class="anchor" id="autotoc_md1460"></a>
What is <code>BaseDispatcher</code>?</h2>
<p><code>BaseDispatcher</code> is an abstract concept (a blueprint or job description) in Crawl4AI. It defines <em>that</em> we need a system for managing the execution of multiple, concurrent crawling tasks. It specifies the <em>interface</em> for how the main <code>AsyncWebCrawler</code> interacts with such a system, but the specific <em>logic</em> for managing concurrency can vary.</p>
<p>Think of it as the control panel for our drone fleet – the panel exists, but the specific rules programmed into it determine how drones are dispatched.</p>
<h2><a class="anchor" id="autotoc_md1461"></a>
The Different Controllers: Ways to Dispatch Tasks</h2>
<p>Crawl4AI provides concrete implementations (the actual traffic control systems) based on the <code>BaseDispatcher</code> blueprint:</p>
<ol type="1">
<li><b><code>SemaphoreDispatcher</code> (The Simple Counter):</b><ul>
<li><b>Analogy:</b> A parking garage with a fixed number of spots (e.g., 10). A gate (<code>asyncio.Semaphore</code>) only lets a new car in if one of the 10 spots is free.</li>
<li><b>How it works:</b> You tell it the maximum number of crawls that can run <em>at the same time</em> (e.g., <code>semaphore_count=10</code>). It uses a simple counter (a semaphore) to ensure that no more than this number of crawls are active simultaneously. When one crawl finishes, it allows another one from the queue to start.</li>
<li><b>Good for:</b> Simple, direct control over concurrency when you know a specific limit works well for your system and the target sites.</li>
</ul>
</li>
<li><b><code>MemoryAdaptiveDispatcher</code> (The Resource-Aware Controller - Default):</b><ul>
<li><b>Analogy:</b> A smart parking garage attendant who checks not just the number of cars, but also the <em>total space</em> they occupy (system memory). They might stop letting cars in if the garage is nearing its memory capacity, even if some numbered spots are technically free.</li>
<li><b>How it works:</b> This dispatcher monitors your system's available memory. It tries to run multiple crawls concurrently (up to a configurable maximum like <code>max_session_permit</code>), but it will pause launching new crawls if the system memory usage exceeds a certain threshold (e.g., <code>memory_threshold_percent=90.0</code>). It adapts the concurrency level based on available resources.</li>
<li><b>Good for:</b> Automatically adjusting concurrency to prevent out-of-memory errors, especially when crawl tasks vary significantly in resource usage. <b>This is the default dispatcher used by <code>arun_many</code> if you don't specify one.</b></li>
</ul>
</li>
</ol>
<p>These dispatchers can also optionally work with a <code>RateLimiter</code> component, which adds politeness rules for specific websites (e.g., slowing down requests to a domain if it returns "429 Too Many Requests").</p>
<h2><a class="anchor" id="autotoc_md1462"></a>
How <code>arun_many</code> Uses the Dispatcher</h2>
<p>When you call <code>crawler.arun_many(urls=...)</code>, here's the basic flow involving the dispatcher:</p>
<ol type="1">
<li><b>Get URLs:</b> <code>arun_many</code> receives the list of URLs you want to crawl.</li>
<li><b>Select Dispatcher:</b> It checks if you provided a specific <code>dispatcher</code> instance. If not, it creates an instance of the default <code>MemoryAdaptiveDispatcher</code>.</li>
<li><b>Delegate Execution:</b> It hands over the list of URLs and the <code>CrawlerRunConfig</code> to the chosen dispatcher's <code>run_urls</code> (or <code>run_urls_stream</code>) method.</li>
<li><b>Manage Tasks:</b> The dispatcher takes charge:<ul>
<li>It iterates through the URLs.</li>
<li>For each URL, it decides <em>when</em> to start the actual crawl based on its rules (semaphore count, memory usage, rate limits).</li>
<li>When ready, it typically calls the single-page <code>crawler.arun(url, config)</code> method internally for that specific URL, wrapped within its concurrency control mechanism.</li>
<li>It manages the running tasks (e.g., using <code>asyncio.create_task</code> and <code>asyncio.wait</code>).</li>
</ul>
</li>
<li><b>Collect Results:</b> As individual <code>arun</code> calls complete, the dispatcher collects their <code>CrawlResult</code> objects.</li>
<li><b>Return:</b> Once all URLs are processed, the dispatcher returns the list of results (or yields them if streaming).</li>
</ol>
<div class="fragment"><div class="line">sequenceDiagram</div>
<div class="line">    participant User</div>
<div class="line">    participant AWC as AsyncWebCrawler</div>
<div class="line">    participant Dispatcher as BaseDispatcher (e.g., MemoryAdaptive)</div>
<div class="line">    participant TaskPool as Concurrency Manager</div>
<div class="line"> </div>
<div class="line">    User-&gt;&gt;AWC: arun_many(urls, config, dispatcher?)</div>
<div class="line">    AWC-&gt;&gt;Dispatcher: run_urls(crawler=AWC, urls, config)</div>
<div class="line">    Dispatcher-&gt;&gt;TaskPool: Initialize (e.g., set max concurrency)</div>
<div class="line">    loop For each URL in urls</div>
<div class="line">        Dispatcher-&gt;&gt;TaskPool: Can I start a new task? (Checks limits)</div>
<div class="line">        alt Yes</div>
<div class="line">            TaskPool--&gt;&gt;Dispatcher: OK</div>
<div class="line">            Note over Dispatcher: Create task: call AWC.arun(url, config) internally</div>
<div class="line">            Dispatcher-&gt;&gt;TaskPool: Add new task</div>
<div class="line">        else No</div>
<div class="line">            TaskPool--&gt;&gt;Dispatcher: Wait</div>
<div class="line">            Note over Dispatcher: Waits for a running task to finish</div>
<div class="line">        end</div>
<div class="line">    end</div>
<div class="line">    Note over Dispatcher: Manages running tasks, collects results</div>
<div class="line">    Dispatcher--&gt;&gt;AWC: List of CrawlResults</div>
<div class="line">    AWC--&gt;&gt;User: List of CrawlResults</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md1463"></a>
Using the Dispatcher (Often Implicitly!)</h2>
<p>Most of the time, you don't need to think about the dispatcher explicitly. When you use <code>arun_many</code>, the default <code>MemoryAdaptiveDispatcher</code> handles things automatically.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter10_example_1.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> AsyncWebCrawler, CrawlerRunConfig</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    urls_to_crawl = [</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/html&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/links/5/0&quot;</span>, <span class="comment"># Page with 5 links</span></div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/robots.txt&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/status/200&quot;</span>,</div>
<div class="line">    ]</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># We DON&#39;T specify a dispatcher here.</span></div>
<div class="line">    <span class="comment"># arun_many will use the default MemoryAdaptiveDispatcher.</span></div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {len(urls_to_crawl)} URLs using the default dispatcher...&quot;</span>)</div>
<div class="line">        config = CrawlerRunConfig(stream=<span class="keyword">False</span>) <span class="comment"># Get results as a list at the end</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># The MemoryAdaptiveDispatcher manages concurrency behind the scenes.</span></div>
<div class="line">        results = await crawler.arun_many(urls=urls_to_crawl, config=config)</div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;\nFinished! Got {len(results)} results.&quot;</span>)</div>
<div class="line">        <span class="keywordflow">for</span> result <span class="keywordflow">in</span> results:</div>
<div class="line">            status = <span class="stringliteral">&quot;✅&quot;</span> <span class="keywordflow">if</span> result.success <span class="keywordflow">else</span> <span class="stringliteral">&quot;❌&quot;</span></div>
<div class="line">            url_short = result.url.split(<span class="stringliteral">&#39;/&#39;</span>)[-1]</div>
<div class="line">            print(f<span class="stringliteral">&quot;  {status} {url_short:&lt;15} | Title: {result.metadata.get(&#39;title&#39;, &#39;N/A&#39;)}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
<div class="ttc" id="anamespacemain_html"><div class="ttname"><a href="../../d2/dc1/namespacemain.html">main</a></div><div class="ttdef"><b>Definition</b> <a href="../../dc/dba/main_8py_source.html#l00001">main.py:1</a></div></div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ul>
<li>We call <code>crawler.arun_many</code> without passing a <code>dispatcher</code> argument.</li>
<li>Crawl4AI automatically creates and uses a <code>MemoryAdaptiveDispatcher</code>.</li>
<li>This dispatcher runs the crawls concurrently, adapting to your system's memory, and returns all the results once completed (because <code>stream=False</code>). You benefit from concurrency without explicit setup.</li>
</ul>
<h2><a class="anchor" id="autotoc_md1464"></a>
Explicitly Choosing a Dispatcher</h2>
<p>What if you want simpler, fixed concurrency? You can explicitly create and pass a <code>SemaphoreDispatcher</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># chapter10_example_2.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">from</span> crawl4ai <span class="keyword">import</span> (</div>
<div class="line">    AsyncWebCrawler,</div>
<div class="line">    CrawlerRunConfig,</div>
<div class="line">    SemaphoreDispatcher <span class="comment"># 1. Import the specific dispatcher</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">async def </span><a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>():</div>
<div class="line">    urls_to_crawl = [</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/delay/1&quot;</span>, <span class="comment"># Takes 1 second</span></div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/delay/1&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/delay/1&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/delay/1&quot;</span>,</div>
<div class="line">        <span class="stringliteral">&quot;https://httpbin.org/delay/1&quot;</span>,</div>
<div class="line">    ]</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># 2. Create an instance of the SemaphoreDispatcher</span></div>
<div class="line">    <span class="comment">#    Allow only 2 crawls to run at the same time.</span></div>
<div class="line">    semaphore_controller = SemaphoreDispatcher(semaphore_count=2)</div>
<div class="line">    print(f<span class="stringliteral">&quot;Using SemaphoreDispatcher with limit: {semaphore_controller.semaphore_count}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async</span> <span class="keyword">with</span> AsyncWebCrawler() <span class="keyword">as</span> crawler:</div>
<div class="line">        print(f<span class="stringliteral">&quot;Crawling {len(urls_to_crawl)} URLs with explicit dispatcher...&quot;</span>)</div>
<div class="line">        config = CrawlerRunConfig(stream=<span class="keyword">False</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># 3. Pass the dispatcher instance to arun_many</span></div>
<div class="line">        results = await crawler.arun_many(</div>
<div class="line">            urls=urls_to_crawl,</div>
<div class="line">            config=config,</div>
<div class="line">            dispatcher=semaphore_controller <span class="comment"># Pass our controller</span></div>
<div class="line">        )</div>
<div class="line"> </div>
<div class="line">        print(f<span class="stringliteral">&quot;\nFinished! Got {len(results)} results.&quot;</span>)</div>
<div class="line">        <span class="comment"># This crawl likely took around 3 seconds (5 tasks, 1s each, 2 concurrent = ceil(5/2)*1s)</span></div>
<div class="line">        <span class="keywordflow">for</span> result <span class="keywordflow">in</span> results:</div>
<div class="line">            status = <span class="stringliteral">&quot;✅&quot;</span> <span class="keywordflow">if</span> result.success <span class="keywordflow">else</span> <span class="stringliteral">&quot;❌&quot;</span></div>
<div class="line">            print(f<span class="stringliteral">&quot;  {status} {result.url}&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    asyncio.run(<a class="code hl_namespace" href="../../d2/dc1/namespacemain.html">main</a>())</div>
</div><!-- fragment --><p><b>Explanation:</b></p>
<ol type="1">
<li><b>Import:</b> We import <code>SemaphoreDispatcher</code>.</li>
<li><b>Instantiate:</b> We create <code>SemaphoreDispatcher(semaphore_count=2)</code>, limiting concurrency to 2 simultaneous crawls.</li>
<li><b>Pass Dispatcher:</b> We pass our <code>semaphore_controller</code> instance directly to the <code>dispatcher</code> parameter of <code>arun_many</code>.</li>
<li><b>Execution:</b> Now, <code>arun_many</code> uses our <code>SemaphoreDispatcher</code>. It will start the first two crawls. As one finishes, it will start the next one from the list, always ensuring no more than two are running concurrently.</li>
</ol>
<h2><a class="anchor" id="autotoc_md1465"></a>
A Glimpse Under the Hood</h2>
<p>Where are these dispatchers defined? In <code>crawl4ai/async_dispatcher.py</code>.</p>
<p><b>The Blueprint (<code>BaseDispatcher</code>):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/async_dispatcher.py</span></div>
<div class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</div>
<div class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List, Optional</div>
<div class="line"><span class="comment"># ... other imports like CrawlerRunConfig, CrawlerTaskResult, AsyncWebCrawler ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>BaseDispatcher(ABC):</div>
<div class="line">    <span class="keyword">def </span>__init__(</div>
<div class="line">        self,</div>
<div class="line">        rate_limiter: Optional[RateLimiter] = <span class="keywordtype">None</span>,</div>
<div class="line">        monitor: Optional[CrawlerMonitor] = <span class="keywordtype">None</span>,</div>
<div class="line">    ):</div>
<div class="line">        self.crawler = <span class="keywordtype">None</span> <span class="comment"># Will be set by arun_many</span></div>
<div class="line">        self.rate_limiter = rate_limiter</div>
<div class="line">        self.monitor = monitor</div>
<div class="line">        <span class="comment"># ... other common state ...</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>crawl_url(</div>
<div class="line">        self,</div>
<div class="line">        url: str,</div>
<div class="line">        config: CrawlerRunConfig,</div>
<div class="line">        task_id: str,</div>
<div class="line">        <span class="comment"># ... maybe other internal params ...</span></div>
<div class="line">    ) -&gt; CrawlerTaskResult:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;Crawls a single URL, potentially handling concurrency primitives.&quot;&quot;&quot;</span></div>
<div class="line">        <span class="comment"># This is often the core worker method called by run_urls</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="preprocessor">@abstractmethod</span></div>
<div class="line">    <span class="keyword">async def </span>run_urls(</div>
<div class="line">        self,</div>
<div class="line">        urls: List[str],</div>
<div class="line">        crawler: <span class="stringliteral">&quot;AsyncWebCrawler&quot;</span>,</div>
<div class="line">        config: CrawlerRunConfig,</div>
<div class="line">    ) -&gt; List[CrawlerTaskResult]:</div>
<div class="line">        <span class="stringliteral">&quot;&quot;&quot;Manages the concurrent execution of crawl_url for multiple URLs.&quot;&quot;&quot;</span></div>
<div class="line">        <span class="comment"># This is the main entry point called by arun_many</span></div>
<div class="line">        <span class="keywordflow">pass</span></div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>run_urls_stream(</div>
<div class="line">        self,</div>
<div class="line">        urls: List[str],</div>
<div class="line">        crawler: <span class="stringliteral">&quot;AsyncWebCrawler&quot;</span>,</div>
<div class="line">        config: CrawlerRunConfig,</div>
<div class="line">    ) -&gt; AsyncGenerator[CrawlerTaskResult, <span class="keywordtype">None</span>]:</div>
<div class="line">         <span class="stringliteral">&quot;&quot;&quot; Streaming version of run_urls (might be implemented in base or subclasses) &quot;&quot;&quot;</span></div>
<div class="line">         <span class="comment"># Example default implementation (subclasses might override)</span></div>
<div class="line">         results = await self.run_urls(urls, crawler, config)</div>
<div class="line">         <span class="keywordflow">for</span> res <span class="keywordflow">in</span> results: <span class="keywordflow">yield</span> res <span class="comment"># Naive stream, real one is more complex</span></div>
<div class="line"> </div>
<div class="line">    <span class="comment"># ... other potential helper methods ...</span></div>
</div><!-- fragment --><p><b>Example Implementation (<code>SemaphoreDispatcher</code>):</b></p>
<div class="fragment"><div class="line"><span class="comment"># Simplified from crawl4ai/async_dispatcher.py</span></div>
<div class="line"><span class="keyword">import</span> asyncio</div>
<div class="line"><span class="keyword">import</span> uuid</div>
<div class="line"><span class="keyword">import</span> psutil <span class="comment"># For memory tracking in crawl_url</span></div>
<div class="line"><span class="keyword">import</span> time   <span class="comment"># For timing in crawl_url</span></div>
<div class="line"><span class="comment"># ... other imports ...</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>SemaphoreDispatcher(BaseDispatcher):</div>
<div class="line">    <span class="keyword">def </span>__init__(</div>
<div class="line">        self,</div>
<div class="line">        semaphore_count: int = 5,</div>
<div class="line">        <span class="comment"># ... other params like rate_limiter, monitor ...</span></div>
<div class="line">    ):</div>
<div class="line">        super().__init__(...) <span class="comment"># Pass rate_limiter, monitor to base</span></div>
<div class="line">        self.semaphore_count = semaphore_count</div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>crawl_url(</div>
<div class="line">        self,</div>
<div class="line">        url: str,</div>
<div class="line">        config: CrawlerRunConfig,</div>
<div class="line">        task_id: str,</div>
<div class="line">        semaphore: asyncio.Semaphore = <span class="keywordtype">None</span>, <span class="comment"># Takes the semaphore</span></div>
<div class="line">    ) -&gt; CrawlerTaskResult:</div>
<div class="line">        <span class="comment"># ... (Code to track start time, memory usage - similar to MemoryAdaptiveDispatcher&#39;s version)</span></div>
<div class="line">        start_time = time.time()</div>
<div class="line">        error_message = <span class="stringliteral">&quot;&quot;</span></div>
<div class="line">        memory_usage = peak_memory = 0.0</div>
<div class="line">        result = <span class="keywordtype">None</span></div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">try</span>:</div>
<div class="line">            <span class="comment"># Update monitor state if used</span></div>
<div class="line">            <span class="keywordflow">if</span> self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.IN_PROGRESS)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Wait for rate limiter if used</span></div>
<div class="line">            <span class="keywordflow">if</span> self.rate_limiter: await self.rate_limiter.wait_if_needed(url)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># --- Core Semaphore Logic ---</span></div>
<div class="line">            <span class="keyword">async</span> <span class="keyword">with</span> semaphore: <span class="comment"># Acquire a spot from the semaphore</span></div>
<div class="line">                <span class="comment"># Now that we have a spot, run the actual crawl</span></div>
<div class="line">                process = psutil.Process()</div>
<div class="line">                start_memory = process.memory_info().rss / (1024 * 1024)</div>
<div class="line"> </div>
<div class="line">                <span class="comment"># Call the single-page crawl method of the main crawler</span></div>
<div class="line">                result = await self.crawler.arun(url, config=config, session_id=task_id)</div>
<div class="line"> </div>
<div class="line">                end_memory = process.memory_info().rss / (1024 * 1024)</div>
<div class="line">                memory_usage = peak_memory = end_memory - start_memory</div>
<div class="line">            <span class="comment"># --- Semaphore spot is released automatically on exiting &#39;async with&#39; ---</span></div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Update rate limiter based on result status if used</span></div>
<div class="line">            <span class="keywordflow">if</span> self.rate_limiter <span class="keywordflow">and</span> result.status_code:</div>
<div class="line">                 <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.rate_limiter.update_delay(url, result.status_code):</div>
<div class="line">                    <span class="comment"># Handle retry limit exceeded</span></div>
<div class="line">                    error_message = <span class="stringliteral">&quot;Rate limit retry count exceeded&quot;</span></div>
<div class="line">                    <span class="comment"># ... update monitor, prepare error result ...</span></div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Update monitor status (success/fail)</span></div>
<div class="line">            <span class="keywordflow">if</span> result <span class="keywordflow">and</span> <span class="keywordflow">not</span> result.success: error_message = result.error_message</div>
<div class="line">            <span class="keywordflow">if</span> self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.COMPLETED <span class="keywordflow">if</span> result.success <span class="keywordflow">else</span> CrawlStatus.FAILED)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">except</span> Exception <span class="keyword">as</span> e:</div>
<div class="line">            <span class="comment"># Handle unexpected errors during the crawl</span></div>
<div class="line">            error_message = str(e)</div>
<div class="line">            <span class="keywordflow">if</span> self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.FAILED)</div>
<div class="line">            <span class="comment"># Create a failed CrawlResult if needed</span></div>
<div class="line">            <span class="keywordflow">if</span> <span class="keywordflow">not</span> result: result = CrawlResult(url=url, html=<span class="stringliteral">&quot;&quot;</span>, success=<span class="keyword">False</span>, error_message=error_message)</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">finally</span>:</div>
<div class="line">            <span class="comment"># Final monitor update with timing, memory etc.</span></div>
<div class="line">             end_time = time.time()</div>
<div class="line">             <span class="keywordflow">if</span> self.monitor: self.monitor.update_task(...)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># Package everything into CrawlerTaskResult</span></div>
<div class="line">        <span class="keywordflow">return</span> CrawlerTaskResult(...)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">    <span class="keyword">async def </span>run_urls(</div>
<div class="line">        self,</div>
<div class="line">        crawler: <span class="stringliteral">&quot;AsyncWebCrawler&quot;</span>,</div>
<div class="line">        urls: List[str],</div>
<div class="line">        config: CrawlerRunConfig,</div>
<div class="line">    ) -&gt; List[CrawlerTaskResult]:</div>
<div class="line">        self.crawler = crawler <span class="comment"># Store the crawler instance</span></div>
<div class="line">        <span class="keywordflow">if</span> self.monitor: self.monitor.start()</div>
<div class="line"> </div>
<div class="line">        <span class="keywordflow">try</span>:</div>
<div class="line">            <span class="comment"># Create the semaphore with the specified count</span></div>
<div class="line">            semaphore = asyncio.Semaphore(self.semaphore_count)</div>
<div class="line">            tasks = []</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Create a crawl task for each URL, passing the semaphore</span></div>
<div class="line">            <span class="keywordflow">for</span> url <span class="keywordflow">in</span> urls:</div>
<div class="line">                task_id = str(uuid.uuid4())</div>
<div class="line">                <span class="keywordflow">if</span> self.monitor: self.monitor.add_task(task_id, url)</div>
<div class="line">                <span class="comment"># Create an asyncio task to run crawl_url</span></div>
<div class="line">                task = asyncio.create_task(</div>
<div class="line">                    self.crawl_url(url, config, task_id, semaphore=semaphore)</div>
<div class="line">                )</div>
<div class="line">                tasks.append(task)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Wait for all created tasks to complete</span></div>
<div class="line">            <span class="comment"># asyncio.gather runs them concurrently, respecting the semaphore limit</span></div>
<div class="line">            results = await asyncio.gather(*tasks, return_exceptions=<span class="keyword">True</span>)</div>
<div class="line"> </div>
<div class="line">            <span class="comment"># Process results (handle potential exceptions returned by gather)</span></div>
<div class="line">            final_results = []</div>
<div class="line">            <span class="keywordflow">for</span> res <span class="keywordflow">in</span> results:</div>
<div class="line">                <span class="keywordflow">if</span> isinstance(res, Exception):</div>
<div class="line">                    <span class="comment"># Handle case where gather caught an exception from a task</span></div>
<div class="line">                    <span class="comment"># You might create a failed CrawlerTaskResult here</span></div>
<div class="line">                    <span class="keywordflow">pass</span></div>
<div class="line">                <span class="keywordflow">elif</span> isinstance(res, CrawlerTaskResult):</div>
<div class="line">                    final_results.append(res)</div>
<div class="line">            <span class="keywordflow">return</span> final_results</div>
<div class="line">        <span class="keywordflow">finally</span>:</div>
<div class="line">            <span class="keywordflow">if</span> self.monitor: self.monitor.stop()</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># run_urls_stream would have similar logic but use asyncio.as_completed</span></div>
<div class="line">    <span class="comment"># or manage tasks manually to yield results as they finish.</span></div>
</div><!-- fragment --><p>The key takeaway is that the <code>Dispatcher</code> orchestrates calls to the single-page <code>crawler.arun</code> method, wrapping them with concurrency controls (like the <code>async with semaphore:</code> block) before running them using <code>asyncio</code>'s concurrency tools (<code>asyncio.create_task</code>, <code>asyncio.gather</code>, etc.).</p>
<h2><a class="anchor" id="autotoc_md1466"></a>
Conclusion</h2>
<p>You've learned about <code>BaseDispatcher</code>, the crucial "Traffic Controller" that manages concurrent crawls in Crawl4AI, especially for <code>arun_many</code>.</p>
<ul>
<li>It solves the problem of efficiently running many crawls without overloading systems or websites.</li>
<li>It acts as a <b>blueprint</b> for managing concurrency.</li>
<li>Key implementations:<ul>
<li><b><code>SemaphoreDispatcher</code></b>: Uses a simple count limit.</li>
<li><b><code>MemoryAdaptiveDispatcher</code></b>: Adjusts concurrency based on system memory (the default for <code>arun_many</code>).</li>
</ul>
</li>
<li>The dispatcher is used <b>automatically</b> by <code>arun_many</code>, but you can provide a specific instance if needed.</li>
<li>It orchestrates the execution of individual crawl tasks, respecting defined limits.</li>
</ul>
<p>Understanding the dispatcher helps appreciate how Crawl4AI handles large-scale crawling tasks responsibly and efficiently.</p>
<p>This concludes our tour of the core concepts in Crawl4AI! We've covered how pages are fetched, how the process is managed, how content is cleaned, filtered, and extracted, how deep crawls are performed, how caching optimizes fetches, and finally, how concurrency is managed. You now have a solid foundation to start building powerful web data extraction and processing applications with Crawl4AI. Happy crawling!</p>
<hr  />
<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a> </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
